{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "NumberGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kdb100/MadeWithML/blob/main/NumberGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJp-D51g0IDd"
      },
      "source": [
        "## **1) Importing Python Packages for GAN**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1k5mFBuzzl2a"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import Dense, Reshape, Flatten\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "import numpy as np\n",
        "!mkdir generated_images"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yr-eZOzg0X79"
      },
      "source": [
        "## **2) Variables for Neural Networks & Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RThZMDruz9cB"
      },
      "source": [
        "img_width = 28\n",
        "img_height = 28\n",
        "channels = 1\n",
        "img_shape = (img_width, img_height, channels)\n",
        "latent_dim = 100 \n",
        "#Adam is the name for stochastic gradient algorithm\n",
        "adam = Adam(learning_rate=0.0001) "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3bcJZZg0cqy"
      },
      "source": [
        "## **3) Building Generator**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdiqZpri0iQh",
        "outputId": "4dfbb549-4040-46a5-b32c-49badb558ce4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def build_generator():\n",
        "  model = Sequential()\n",
        "\n",
        "  model.add(Dense(256, input_dim=latent_dim))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(BatchNormalization(momentum=0.8))\n",
        "\n",
        "  model.add(Dense(256))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(BatchNormalization(momentum=0.8))\n",
        "\n",
        "  model.add(Dense(256))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(BatchNormalization(momentum=0.8))\n",
        "\n",
        "  model.add(Dense(np.prod(img_shape), activation='tanh'))\n",
        "  model.add(Reshape(img_shape))\n",
        "  \n",
        "  model.summary()\n",
        "  return model\n",
        "\n",
        "generator = build_generator()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 256)               25856     \n",
            "                                                                 \n",
            " leaky_re_lu (LeakyReLU)     (None, 256)               0         \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 256)              1024      \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 256)               65792     \n",
            "                                                                 \n",
            " leaky_re_lu_1 (LeakyReLU)   (None, 256)               0         \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 256)              1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 256)               65792     \n",
            "                                                                 \n",
            " leaky_re_lu_2 (LeakyReLU)   (None, 256)               0         \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 256)              1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 784)               201488    \n",
            "                                                                 \n",
            " reshape (Reshape)           (None, 28, 28, 1)         0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 362,000\n",
            "Trainable params: 360,464\n",
            "Non-trainable params: 1,536\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bt6QsJCW0mcI"
      },
      "source": [
        "## **4) Building Discriminator**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2JzEAPv0lKt",
        "outputId": "577d27e5-3a28-4470-df0f-8dafd935f6f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def build_discriminator():\n",
        "  model = Sequential()\n",
        "\n",
        "  model.add(Flatten(input_shape=img_shape))\n",
        "  model.add(Dense(512))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(Dense(256))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "  model.summary()\n",
        "  return model\n",
        "\n",
        "discriminator = build_discriminator()\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 784)               0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 512)               401920    \n",
            "                                                                 \n",
            " leaky_re_lu_3 (LeakyReLU)   (None, 512)               0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 256)               131328    \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 533,505\n",
            "Trainable params: 533,505\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbcKcKmA0q2S"
      },
      "source": [
        "## **5) Connecting Neural Networks to build GAN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0Ue3TEd0xLy",
        "outputId": "abd7a92d-2e4c-4733-c19e-3a821a00a174",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "GAN = Sequential()\n",
        "discriminator.trainable = False\n",
        "GAN.add(generator)\n",
        "GAN.add(discriminator)\n",
        "\n",
        "GAN.compile(loss='binary_crossentropy', optimizer=adam)\n",
        "GAN.summary()\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " sequential (Sequential)     (None, 28, 28, 1)         362000    \n",
            "                                                                 \n",
            " sequential_1 (Sequential)   (None, 1)                 533505    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 895,505\n",
            "Trainable params: 360,464\n",
            "Non-trainable params: 535,041\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WaNhBDwRwTG"
      },
      "source": [
        "## **6) Outputting Images**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQEJ0WbjRppy"
      },
      "source": [
        "#@title\n",
        "## **7) Outputting Images**\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import imageio\n",
        "import PIL\n",
        "\n",
        "save_name = 0.00000000\n",
        "\n",
        "def save_imgs(epoch):\n",
        "    #generate 25 images to fit on a 5 x 5 grid for our animation!\n",
        "    r, c = 5, 5\n",
        "    noise = np.random.normal(0, 1, (r * c, latent_dim))\n",
        "    gen_imgs = generator.predict(noise)\n",
        "    global save_name\n",
        "    save_name += 0.00000001\n",
        "    print(\"%.8f\" % save_name)\n",
        "\n",
        "    # Rescale images 0 - 1\n",
        "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "    fig, axs = plt.subplots(r, c)\n",
        "    cnt = 0\n",
        "    for i in range(r):\n",
        "        for j in range(c):\n",
        "            axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
        "            # axs[i,j].imshow(gen_imgs[cnt])\n",
        "            axs[i,j].axis('off')\n",
        "            cnt += 1\n",
        "    fig.savefig(\"generated_images/%.8f.png\" % save_name)\n",
        "    print('saved')\n",
        "    plt.close()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tE57Lk5V0xs2"
      },
      "source": [
        "## **7) Training GAN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egSJJvik00Iq",
        "outputId": "97108191-3d47-423c-9868-d06e780bff12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def train(epochs, batch_size=64, save_interval=200):\n",
        "  (X_train, _), (_, _) = mnist.load_data()\n",
        "\n",
        "  # print(X_train.shape)\n",
        "  #Rescale data between -1 and 1\n",
        "  X_train = X_train / 127.5 -1.\n",
        "  # X_train = np.expand_dims(X_train, axis=3)\n",
        "  # print(X_train.shape)\n",
        "\n",
        "  #Create our Y for our Neural Networks\n",
        "  valid = np.ones((batch_size, 1))\n",
        "  fakes = np.zeros((batch_size, 1))\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    #Get Random Batch\n",
        "    idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "    imgs = X_train[idx]\n",
        "\n",
        "    #Generate Fake Images\n",
        "    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "    gen_imgs = generator.predict(noise)\n",
        "\n",
        "    #Train discriminator\n",
        "    d_loss_real = discriminator.train_on_batch(imgs, valid)\n",
        "    d_loss_fake = discriminator.train_on_batch(gen_imgs, fakes)\n",
        "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "    \n",
        "    #inverse y label\n",
        "    g_loss = GAN.train_on_batch(noise, valid)\n",
        "\n",
        "    print(\"******* %d [D loss: %f, acc: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100* d_loss[1], g_loss))\n",
        "\n",
        "    if(epoch % save_interval) == 0:\n",
        "      save_imgs(epoch)\n",
        "\n",
        "  # print(valid)\n",
        "\n",
        "\n",
        "train(30000, batch_size=64, save_interval=200)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n",
            "******* 0 [D loss: 1.259146, acc: 4.69%] [G loss: 0.757126]\n",
            "0.00000001\n",
            "saved\n",
            "******* 1 [D loss: 0.585259, acc: 56.25%] [G loss: 0.772246]\n",
            "******* 2 [D loss: 0.416239, acc: 68.75%] [G loss: 0.766218]\n",
            "******* 3 [D loss: 0.378215, acc: 70.31%] [G loss: 0.771872]\n",
            "******* 4 [D loss: 0.351592, acc: 76.56%] [G loss: 0.822320]\n",
            "******* 5 [D loss: 0.333252, acc: 81.25%] [G loss: 0.893102]\n",
            "******* 6 [D loss: 0.297126, acc: 89.06%] [G loss: 0.908777]\n",
            "******* 7 [D loss: 0.267919, acc: 90.62%] [G loss: 0.966021]\n",
            "******* 8 [D loss: 0.283590, acc: 92.97%] [G loss: 1.004391]\n",
            "******* 9 [D loss: 0.231590, acc: 94.53%] [G loss: 1.125706]\n",
            "******* 10 [D loss: 0.247068, acc: 92.19%] [G loss: 1.254769]\n",
            "******* 11 [D loss: 0.192497, acc: 95.31%] [G loss: 1.344699]\n",
            "******* 12 [D loss: 0.182587, acc: 98.44%] [G loss: 1.462205]\n",
            "******* 13 [D loss: 0.158285, acc: 99.22%] [G loss: 1.549049]\n",
            "******* 14 [D loss: 0.125770, acc: 100.00%] [G loss: 1.642407]\n",
            "******* 15 [D loss: 0.110561, acc: 100.00%] [G loss: 1.786274]\n",
            "******* 16 [D loss: 0.096121, acc: 100.00%] [G loss: 1.915258]\n",
            "******* 17 [D loss: 0.080398, acc: 100.00%] [G loss: 2.053122]\n",
            "******* 18 [D loss: 0.080324, acc: 100.00%] [G loss: 2.151125]\n",
            "******* 19 [D loss: 0.071502, acc: 100.00%] [G loss: 2.261055]\n",
            "******* 20 [D loss: 0.066829, acc: 100.00%] [G loss: 2.398976]\n",
            "******* 21 [D loss: 0.056881, acc: 100.00%] [G loss: 2.450761]\n",
            "******* 22 [D loss: 0.048197, acc: 100.00%] [G loss: 2.458115]\n",
            "******* 23 [D loss: 0.048441, acc: 100.00%] [G loss: 2.617438]\n",
            "******* 24 [D loss: 0.044073, acc: 100.00%] [G loss: 2.646067]\n",
            "******* 25 [D loss: 0.043478, acc: 100.00%] [G loss: 2.770837]\n",
            "******* 26 [D loss: 0.038880, acc: 100.00%] [G loss: 2.824925]\n",
            "******* 27 [D loss: 0.036794, acc: 100.00%] [G loss: 2.856829]\n",
            "******* 28 [D loss: 0.033719, acc: 100.00%] [G loss: 2.942392]\n",
            "******* 29 [D loss: 0.040156, acc: 100.00%] [G loss: 2.964585]\n",
            "******* 30 [D loss: 0.031070, acc: 100.00%] [G loss: 2.965307]\n",
            "******* 31 [D loss: 0.031464, acc: 100.00%] [G loss: 3.069679]\n",
            "******* 32 [D loss: 0.029349, acc: 100.00%] [G loss: 3.021338]\n",
            "******* 33 [D loss: 0.029021, acc: 100.00%] [G loss: 3.093849]\n",
            "******* 34 [D loss: 0.026460, acc: 100.00%] [G loss: 3.150267]\n",
            "******* 35 [D loss: 0.024703, acc: 100.00%] [G loss: 3.124643]\n",
            "******* 36 [D loss: 0.024155, acc: 100.00%] [G loss: 3.129088]\n",
            "******* 37 [D loss: 0.028005, acc: 100.00%] [G loss: 3.154613]\n",
            "******* 38 [D loss: 0.023947, acc: 100.00%] [G loss: 3.149055]\n",
            "******* 39 [D loss: 0.024713, acc: 100.00%] [G loss: 3.189685]\n",
            "******* 40 [D loss: 0.026868, acc: 100.00%] [G loss: 3.212630]\n",
            "******* 41 [D loss: 0.024296, acc: 100.00%] [G loss: 3.182525]\n",
            "******* 42 [D loss: 0.026418, acc: 100.00%] [G loss: 3.277447]\n",
            "******* 43 [D loss: 0.026109, acc: 100.00%] [G loss: 3.243538]\n",
            "******* 44 [D loss: 0.021499, acc: 100.00%] [G loss: 3.264239]\n",
            "******* 45 [D loss: 0.022200, acc: 100.00%] [G loss: 3.194311]\n",
            "******* 46 [D loss: 0.022724, acc: 100.00%] [G loss: 3.338765]\n",
            "******* 47 [D loss: 0.022222, acc: 100.00%] [G loss: 3.332953]\n",
            "******* 48 [D loss: 0.025010, acc: 100.00%] [G loss: 3.263867]\n",
            "******* 49 [D loss: 0.022421, acc: 100.00%] [G loss: 3.299798]\n",
            "******* 50 [D loss: 0.025583, acc: 100.00%] [G loss: 3.385667]\n",
            "******* 51 [D loss: 0.024239, acc: 100.00%] [G loss: 3.408802]\n",
            "******* 52 [D loss: 0.018501, acc: 100.00%] [G loss: 3.197150]\n",
            "******* 53 [D loss: 0.019516, acc: 100.00%] [G loss: 3.334947]\n",
            "******* 54 [D loss: 0.022367, acc: 100.00%] [G loss: 3.391990]\n",
            "******* 55 [D loss: 0.018888, acc: 100.00%] [G loss: 3.309616]\n",
            "******* 56 [D loss: 0.021602, acc: 100.00%] [G loss: 3.369414]\n",
            "******* 57 [D loss: 0.019629, acc: 100.00%] [G loss: 3.379439]\n",
            "******* 58 [D loss: 0.019123, acc: 100.00%] [G loss: 3.393421]\n",
            "******* 59 [D loss: 0.019576, acc: 100.00%] [G loss: 3.398061]\n",
            "******* 60 [D loss: 0.020666, acc: 100.00%] [G loss: 3.335647]\n",
            "******* 61 [D loss: 0.019124, acc: 100.00%] [G loss: 3.378747]\n",
            "******* 62 [D loss: 0.020010, acc: 100.00%] [G loss: 3.403940]\n",
            "******* 63 [D loss: 0.019837, acc: 100.00%] [G loss: 3.435378]\n",
            "******* 64 [D loss: 0.020026, acc: 100.00%] [G loss: 3.347993]\n",
            "******* 65 [D loss: 0.020585, acc: 100.00%] [G loss: 3.440124]\n",
            "******* 66 [D loss: 0.021924, acc: 100.00%] [G loss: 3.470748]\n",
            "******* 67 [D loss: 0.020900, acc: 100.00%] [G loss: 3.478534]\n",
            "******* 68 [D loss: 0.021198, acc: 100.00%] [G loss: 3.485634]\n",
            "******* 69 [D loss: 0.021323, acc: 100.00%] [G loss: 3.444703]\n",
            "******* 70 [D loss: 0.018227, acc: 100.00%] [G loss: 3.495388]\n",
            "******* 71 [D loss: 0.016319, acc: 100.00%] [G loss: 3.497340]\n",
            "******* 72 [D loss: 0.017473, acc: 100.00%] [G loss: 3.502793]\n",
            "******* 73 [D loss: 0.015897, acc: 100.00%] [G loss: 3.570856]\n",
            "******* 74 [D loss: 0.017054, acc: 100.00%] [G loss: 3.532694]\n",
            "******* 75 [D loss: 0.019404, acc: 100.00%] [G loss: 3.570849]\n",
            "******* 76 [D loss: 0.017913, acc: 100.00%] [G loss: 3.540266]\n",
            "******* 77 [D loss: 0.017514, acc: 100.00%] [G loss: 3.567133]\n",
            "******* 78 [D loss: 0.018406, acc: 100.00%] [G loss: 3.527481]\n",
            "******* 79 [D loss: 0.017599, acc: 100.00%] [G loss: 3.597376]\n",
            "******* 80 [D loss: 0.017780, acc: 100.00%] [G loss: 3.613732]\n",
            "******* 81 [D loss: 0.015844, acc: 100.00%] [G loss: 3.616198]\n",
            "******* 82 [D loss: 0.017851, acc: 100.00%] [G loss: 3.616767]\n",
            "******* 83 [D loss: 0.018805, acc: 100.00%] [G loss: 3.631605]\n",
            "******* 84 [D loss: 0.014643, acc: 100.00%] [G loss: 3.633222]\n",
            "******* 85 [D loss: 0.014650, acc: 100.00%] [G loss: 3.618087]\n",
            "******* 86 [D loss: 0.017602, acc: 100.00%] [G loss: 3.633482]\n",
            "******* 87 [D loss: 0.017663, acc: 100.00%] [G loss: 3.707718]\n",
            "******* 88 [D loss: 0.012408, acc: 100.00%] [G loss: 3.667763]\n",
            "******* 89 [D loss: 0.014722, acc: 100.00%] [G loss: 3.682670]\n",
            "******* 90 [D loss: 0.015415, acc: 100.00%] [G loss: 3.661196]\n",
            "******* 91 [D loss: 0.016143, acc: 100.00%] [G loss: 3.671081]\n",
            "******* 92 [D loss: 0.015749, acc: 100.00%] [G loss: 3.725330]\n",
            "******* 93 [D loss: 0.016904, acc: 100.00%] [G loss: 3.701789]\n",
            "******* 94 [D loss: 0.015015, acc: 100.00%] [G loss: 3.725901]\n",
            "******* 95 [D loss: 0.013740, acc: 100.00%] [G loss: 3.718912]\n",
            "******* 96 [D loss: 0.012037, acc: 100.00%] [G loss: 3.718019]\n",
            "******* 97 [D loss: 0.015842, acc: 100.00%] [G loss: 3.749016]\n",
            "******* 98 [D loss: 0.015002, acc: 100.00%] [G loss: 3.703209]\n",
            "******* 99 [D loss: 0.017455, acc: 100.00%] [G loss: 3.678384]\n",
            "******* 100 [D loss: 0.014819, acc: 100.00%] [G loss: 3.765963]\n",
            "******* 101 [D loss: 0.018677, acc: 100.00%] [G loss: 3.772402]\n",
            "******* 102 [D loss: 0.013257, acc: 100.00%] [G loss: 3.700278]\n",
            "******* 103 [D loss: 0.014920, acc: 100.00%] [G loss: 3.800076]\n",
            "******* 104 [D loss: 0.014196, acc: 100.00%] [G loss: 3.738456]\n",
            "******* 105 [D loss: 0.014006, acc: 100.00%] [G loss: 3.916659]\n",
            "******* 106 [D loss: 0.013819, acc: 100.00%] [G loss: 3.816307]\n",
            "******* 107 [D loss: 0.015229, acc: 100.00%] [G loss: 3.827750]\n",
            "******* 108 [D loss: 0.015407, acc: 100.00%] [G loss: 3.757416]\n",
            "******* 109 [D loss: 0.013133, acc: 100.00%] [G loss: 3.897825]\n",
            "******* 110 [D loss: 0.012086, acc: 100.00%] [G loss: 3.887306]\n",
            "******* 111 [D loss: 0.013397, acc: 100.00%] [G loss: 3.919817]\n",
            "******* 112 [D loss: 0.014109, acc: 100.00%] [G loss: 3.934000]\n",
            "******* 113 [D loss: 0.012025, acc: 100.00%] [G loss: 3.961644]\n",
            "******* 114 [D loss: 0.013954, acc: 100.00%] [G loss: 3.939436]\n",
            "******* 115 [D loss: 0.014355, acc: 100.00%] [G loss: 3.922715]\n",
            "******* 116 [D loss: 0.015307, acc: 100.00%] [G loss: 3.894816]\n",
            "******* 117 [D loss: 0.011863, acc: 100.00%] [G loss: 3.880749]\n",
            "******* 118 [D loss: 0.011807, acc: 100.00%] [G loss: 3.914742]\n",
            "******* 119 [D loss: 0.014709, acc: 100.00%] [G loss: 3.964896]\n",
            "******* 120 [D loss: 0.012255, acc: 100.00%] [G loss: 3.964234]\n",
            "******* 121 [D loss: 0.012923, acc: 100.00%] [G loss: 3.883373]\n",
            "******* 122 [D loss: 0.010983, acc: 100.00%] [G loss: 3.979346]\n",
            "******* 123 [D loss: 0.011377, acc: 100.00%] [G loss: 3.914021]\n",
            "******* 124 [D loss: 0.012097, acc: 100.00%] [G loss: 3.966129]\n",
            "******* 125 [D loss: 0.012670, acc: 100.00%] [G loss: 4.018851]\n",
            "******* 126 [D loss: 0.011958, acc: 100.00%] [G loss: 3.976587]\n",
            "******* 127 [D loss: 0.011502, acc: 100.00%] [G loss: 3.960050]\n",
            "******* 128 [D loss: 0.012051, acc: 100.00%] [G loss: 3.996547]\n",
            "******* 129 [D loss: 0.016194, acc: 100.00%] [G loss: 3.971766]\n",
            "******* 130 [D loss: 0.012572, acc: 100.00%] [G loss: 4.057119]\n",
            "******* 131 [D loss: 0.011495, acc: 100.00%] [G loss: 3.982444]\n",
            "******* 132 [D loss: 0.011283, acc: 100.00%] [G loss: 4.043156]\n",
            "******* 133 [D loss: 0.012829, acc: 100.00%] [G loss: 3.977422]\n",
            "******* 134 [D loss: 0.013776, acc: 100.00%] [G loss: 3.953300]\n",
            "******* 135 [D loss: 0.012689, acc: 100.00%] [G loss: 3.973820]\n",
            "******* 136 [D loss: 0.013002, acc: 100.00%] [G loss: 3.983373]\n",
            "******* 137 [D loss: 0.013982, acc: 100.00%] [G loss: 3.971982]\n",
            "******* 138 [D loss: 0.011466, acc: 100.00%] [G loss: 3.974352]\n",
            "******* 139 [D loss: 0.011190, acc: 100.00%] [G loss: 4.095695]\n",
            "******* 140 [D loss: 0.011539, acc: 100.00%] [G loss: 4.023766]\n",
            "******* 141 [D loss: 0.014742, acc: 100.00%] [G loss: 3.978165]\n",
            "******* 142 [D loss: 0.012164, acc: 100.00%] [G loss: 4.025616]\n",
            "******* 143 [D loss: 0.011047, acc: 100.00%] [G loss: 4.003545]\n",
            "******* 144 [D loss: 0.012852, acc: 100.00%] [G loss: 4.053761]\n",
            "******* 145 [D loss: 0.011482, acc: 100.00%] [G loss: 4.116218]\n",
            "******* 146 [D loss: 0.011910, acc: 100.00%] [G loss: 4.100355]\n",
            "******* 147 [D loss: 0.007779, acc: 100.00%] [G loss: 4.088700]\n",
            "******* 148 [D loss: 0.010894, acc: 100.00%] [G loss: 4.058045]\n",
            "******* 149 [D loss: 0.011692, acc: 100.00%] [G loss: 4.186419]\n",
            "******* 150 [D loss: 0.012580, acc: 100.00%] [G loss: 4.147690]\n",
            "******* 151 [D loss: 0.011926, acc: 100.00%] [G loss: 4.138508]\n",
            "******* 152 [D loss: 0.009963, acc: 100.00%] [G loss: 4.089076]\n",
            "******* 153 [D loss: 0.009227, acc: 100.00%] [G loss: 4.088478]\n",
            "******* 154 [D loss: 0.010575, acc: 100.00%] [G loss: 4.145000]\n",
            "******* 155 [D loss: 0.010436, acc: 100.00%] [G loss: 4.104816]\n",
            "******* 156 [D loss: 0.014335, acc: 100.00%] [G loss: 4.119853]\n",
            "******* 157 [D loss: 0.010872, acc: 100.00%] [G loss: 4.163007]\n",
            "******* 158 [D loss: 0.012838, acc: 100.00%] [G loss: 4.123325]\n",
            "******* 159 [D loss: 0.010531, acc: 100.00%] [G loss: 4.181256]\n",
            "******* 160 [D loss: 0.013143, acc: 100.00%] [G loss: 4.156255]\n",
            "******* 161 [D loss: 0.012899, acc: 100.00%] [G loss: 4.186376]\n",
            "******* 162 [D loss: 0.010661, acc: 100.00%] [G loss: 4.136814]\n",
            "******* 163 [D loss: 0.010561, acc: 100.00%] [G loss: 4.226479]\n",
            "******* 164 [D loss: 0.011424, acc: 100.00%] [G loss: 4.171755]\n",
            "******* 165 [D loss: 0.010666, acc: 100.00%] [G loss: 4.199131]\n",
            "******* 166 [D loss: 0.011116, acc: 100.00%] [G loss: 4.191226]\n",
            "******* 167 [D loss: 0.013333, acc: 100.00%] [G loss: 4.239508]\n",
            "******* 168 [D loss: 0.012372, acc: 100.00%] [G loss: 4.162286]\n",
            "******* 169 [D loss: 0.010558, acc: 100.00%] [G loss: 4.203824]\n",
            "******* 170 [D loss: 0.009803, acc: 100.00%] [G loss: 4.187674]\n",
            "******* 171 [D loss: 0.010694, acc: 100.00%] [G loss: 4.183322]\n",
            "******* 172 [D loss: 0.009902, acc: 100.00%] [G loss: 4.210621]\n",
            "******* 173 [D loss: 0.011849, acc: 100.00%] [G loss: 4.245615]\n",
            "******* 174 [D loss: 0.011047, acc: 100.00%] [G loss: 4.162551]\n",
            "******* 175 [D loss: 0.010644, acc: 100.00%] [G loss: 4.183663]\n",
            "******* 176 [D loss: 0.013379, acc: 100.00%] [G loss: 4.185738]\n",
            "******* 177 [D loss: 0.010907, acc: 100.00%] [G loss: 4.214728]\n",
            "******* 178 [D loss: 0.011129, acc: 100.00%] [G loss: 4.225972]\n",
            "******* 179 [D loss: 0.012050, acc: 100.00%] [G loss: 4.272075]\n",
            "******* 180 [D loss: 0.010848, acc: 100.00%] [G loss: 4.236732]\n",
            "******* 181 [D loss: 0.012714, acc: 100.00%] [G loss: 4.265423]\n",
            "******* 182 [D loss: 0.009667, acc: 100.00%] [G loss: 4.237453]\n",
            "******* 183 [D loss: 0.009811, acc: 100.00%] [G loss: 4.276640]\n",
            "******* 184 [D loss: 0.011149, acc: 100.00%] [G loss: 4.206917]\n",
            "******* 185 [D loss: 0.012938, acc: 100.00%] [G loss: 4.267517]\n",
            "******* 186 [D loss: 0.011243, acc: 100.00%] [G loss: 4.345870]\n",
            "******* 187 [D loss: 0.010411, acc: 100.00%] [G loss: 4.329576]\n",
            "******* 188 [D loss: 0.010864, acc: 100.00%] [G loss: 4.392137]\n",
            "******* 189 [D loss: 0.009854, acc: 100.00%] [G loss: 4.328543]\n",
            "******* 190 [D loss: 0.010585, acc: 100.00%] [G loss: 4.361474]\n",
            "******* 191 [D loss: 0.011477, acc: 100.00%] [G loss: 4.363120]\n",
            "******* 192 [D loss: 0.010574, acc: 100.00%] [G loss: 4.349063]\n",
            "******* 193 [D loss: 0.010621, acc: 100.00%] [G loss: 4.327584]\n",
            "******* 194 [D loss: 0.009825, acc: 100.00%] [G loss: 4.271814]\n",
            "******* 195 [D loss: 0.009593, acc: 100.00%] [G loss: 4.278882]\n",
            "******* 196 [D loss: 0.009879, acc: 100.00%] [G loss: 4.329049]\n",
            "******* 197 [D loss: 0.013708, acc: 100.00%] [G loss: 4.286954]\n",
            "******* 198 [D loss: 0.010102, acc: 100.00%] [G loss: 4.400803]\n",
            "******* 199 [D loss: 0.011473, acc: 100.00%] [G loss: 4.345718]\n",
            "******* 200 [D loss: 0.012075, acc: 100.00%] [G loss: 4.286026]\n",
            "0.00000002\n",
            "saved\n",
            "******* 201 [D loss: 0.009632, acc: 100.00%] [G loss: 4.353082]\n",
            "******* 202 [D loss: 0.012103, acc: 100.00%] [G loss: 4.272809]\n",
            "******* 203 [D loss: 0.009957, acc: 100.00%] [G loss: 4.294311]\n",
            "******* 204 [D loss: 0.012621, acc: 100.00%] [G loss: 4.317394]\n",
            "******* 205 [D loss: 0.011533, acc: 100.00%] [G loss: 4.354378]\n",
            "******* 206 [D loss: 0.011599, acc: 100.00%] [G loss: 4.417022]\n",
            "******* 207 [D loss: 0.010441, acc: 100.00%] [G loss: 4.400993]\n",
            "******* 208 [D loss: 0.012073, acc: 100.00%] [G loss: 4.440799]\n",
            "******* 209 [D loss: 0.010329, acc: 100.00%] [G loss: 4.419337]\n",
            "******* 210 [D loss: 0.010392, acc: 100.00%] [G loss: 4.333756]\n",
            "******* 211 [D loss: 0.010826, acc: 100.00%] [G loss: 4.352354]\n",
            "******* 212 [D loss: 0.013227, acc: 100.00%] [G loss: 4.403137]\n",
            "******* 213 [D loss: 0.013554, acc: 100.00%] [G loss: 4.361920]\n",
            "******* 214 [D loss: 0.010770, acc: 100.00%] [G loss: 4.395922]\n",
            "******* 215 [D loss: 0.013174, acc: 100.00%] [G loss: 4.320681]\n",
            "******* 216 [D loss: 0.010897, acc: 100.00%] [G loss: 4.451324]\n",
            "******* 217 [D loss: 0.011961, acc: 100.00%] [G loss: 4.353343]\n",
            "******* 218 [D loss: 0.011049, acc: 100.00%] [G loss: 4.466198]\n",
            "******* 219 [D loss: 0.014515, acc: 100.00%] [G loss: 4.383353]\n",
            "******* 220 [D loss: 0.012387, acc: 100.00%] [G loss: 4.389385]\n",
            "******* 221 [D loss: 0.011703, acc: 100.00%] [G loss: 4.372257]\n",
            "******* 222 [D loss: 0.010638, acc: 100.00%] [G loss: 4.427025]\n",
            "******* 223 [D loss: 0.014886, acc: 100.00%] [G loss: 4.354573]\n",
            "******* 224 [D loss: 0.011010, acc: 100.00%] [G loss: 4.383588]\n",
            "******* 225 [D loss: 0.010773, acc: 100.00%] [G loss: 4.422935]\n",
            "******* 226 [D loss: 0.010730, acc: 100.00%] [G loss: 4.324615]\n",
            "******* 227 [D loss: 0.012466, acc: 100.00%] [G loss: 4.415746]\n",
            "******* 228 [D loss: 0.011696, acc: 100.00%] [G loss: 4.331886]\n",
            "******* 229 [D loss: 0.011940, acc: 100.00%] [G loss: 4.345984]\n",
            "******* 230 [D loss: 0.012744, acc: 100.00%] [G loss: 4.484286]\n",
            "******* 231 [D loss: 0.014038, acc: 100.00%] [G loss: 4.392208]\n",
            "******* 232 [D loss: 0.009934, acc: 100.00%] [G loss: 4.415565]\n",
            "******* 233 [D loss: 0.012027, acc: 100.00%] [G loss: 4.320645]\n",
            "******* 234 [D loss: 0.014176, acc: 100.00%] [G loss: 4.336018]\n",
            "******* 235 [D loss: 0.014286, acc: 100.00%] [G loss: 4.412439]\n",
            "******* 236 [D loss: 0.011609, acc: 100.00%] [G loss: 4.310916]\n",
            "******* 237 [D loss: 0.012495, acc: 100.00%] [G loss: 4.347276]\n",
            "******* 238 [D loss: 0.016076, acc: 100.00%] [G loss: 4.320924]\n",
            "******* 239 [D loss: 0.015090, acc: 100.00%] [G loss: 4.332123]\n",
            "******* 240 [D loss: 0.014407, acc: 100.00%] [G loss: 4.217914]\n",
            "******* 241 [D loss: 0.014730, acc: 100.00%] [G loss: 4.328410]\n",
            "******* 242 [D loss: 0.013858, acc: 100.00%] [G loss: 4.344730]\n",
            "******* 243 [D loss: 0.012889, acc: 100.00%] [G loss: 4.294591]\n",
            "******* 244 [D loss: 0.011696, acc: 100.00%] [G loss: 4.404684]\n",
            "******* 245 [D loss: 0.013735, acc: 100.00%] [G loss: 4.422232]\n",
            "******* 246 [D loss: 0.012650, acc: 100.00%] [G loss: 4.324985]\n",
            "******* 247 [D loss: 0.015336, acc: 100.00%] [G loss: 4.278890]\n",
            "******* 248 [D loss: 0.013210, acc: 100.00%] [G loss: 4.282798]\n",
            "******* 249 [D loss: 0.014350, acc: 100.00%] [G loss: 4.233422]\n",
            "******* 250 [D loss: 0.017111, acc: 100.00%] [G loss: 4.305356]\n",
            "******* 251 [D loss: 0.015118, acc: 100.00%] [G loss: 4.208001]\n",
            "******* 252 [D loss: 0.017208, acc: 100.00%] [G loss: 4.204294]\n",
            "******* 253 [D loss: 0.017773, acc: 100.00%] [G loss: 4.246494]\n",
            "******* 254 [D loss: 0.017656, acc: 100.00%] [G loss: 4.311588]\n",
            "******* 255 [D loss: 0.018002, acc: 100.00%] [G loss: 4.170702]\n",
            "******* 256 [D loss: 0.014794, acc: 100.00%] [G loss: 4.339211]\n",
            "******* 257 [D loss: 0.015531, acc: 100.00%] [G loss: 4.223423]\n",
            "******* 258 [D loss: 0.018516, acc: 100.00%] [G loss: 4.374252]\n",
            "******* 259 [D loss: 0.017731, acc: 100.00%] [G loss: 4.328239]\n",
            "******* 260 [D loss: 0.023123, acc: 100.00%] [G loss: 4.240039]\n",
            "******* 261 [D loss: 0.020348, acc: 100.00%] [G loss: 4.276284]\n",
            "******* 262 [D loss: 0.016572, acc: 100.00%] [G loss: 4.339936]\n",
            "******* 263 [D loss: 0.019538, acc: 100.00%] [G loss: 4.185007]\n",
            "******* 264 [D loss: 0.022141, acc: 100.00%] [G loss: 4.200314]\n",
            "******* 265 [D loss: 0.019026, acc: 100.00%] [G loss: 4.272797]\n",
            "******* 266 [D loss: 0.022159, acc: 100.00%] [G loss: 4.236254]\n",
            "******* 267 [D loss: 0.014306, acc: 100.00%] [G loss: 4.223872]\n",
            "******* 268 [D loss: 0.019487, acc: 100.00%] [G loss: 4.186904]\n",
            "******* 269 [D loss: 0.026223, acc: 100.00%] [G loss: 4.214071]\n",
            "******* 270 [D loss: 0.024832, acc: 100.00%] [G loss: 4.129817]\n",
            "******* 271 [D loss: 0.019046, acc: 100.00%] [G loss: 4.152232]\n",
            "******* 272 [D loss: 0.024713, acc: 100.00%] [G loss: 4.171772]\n",
            "******* 273 [D loss: 0.021009, acc: 100.00%] [G loss: 4.293684]\n",
            "******* 274 [D loss: 0.018598, acc: 100.00%] [G loss: 4.163955]\n",
            "******* 275 [D loss: 0.023200, acc: 100.00%] [G loss: 4.252538]\n",
            "******* 276 [D loss: 0.021810, acc: 100.00%] [G loss: 4.159318]\n",
            "******* 277 [D loss: 0.023701, acc: 100.00%] [G loss: 4.081226]\n",
            "******* 278 [D loss: 0.024251, acc: 100.00%] [G loss: 4.086313]\n",
            "******* 279 [D loss: 0.020214, acc: 100.00%] [G loss: 4.015887]\n",
            "******* 280 [D loss: 0.022627, acc: 100.00%] [G loss: 3.996496]\n",
            "******* 281 [D loss: 0.030914, acc: 99.22%] [G loss: 3.946717]\n",
            "******* 282 [D loss: 0.029018, acc: 100.00%] [G loss: 3.880348]\n",
            "******* 283 [D loss: 0.019391, acc: 100.00%] [G loss: 3.924658]\n",
            "******* 284 [D loss: 0.027436, acc: 100.00%] [G loss: 3.878205]\n",
            "******* 285 [D loss: 0.026868, acc: 100.00%] [G loss: 4.013833]\n",
            "******* 286 [D loss: 0.041149, acc: 99.22%] [G loss: 3.838436]\n",
            "******* 287 [D loss: 0.035273, acc: 100.00%] [G loss: 3.827974]\n",
            "******* 288 [D loss: 0.026773, acc: 100.00%] [G loss: 3.835342]\n",
            "******* 289 [D loss: 0.038020, acc: 100.00%] [G loss: 3.842368]\n",
            "******* 290 [D loss: 0.032362, acc: 100.00%] [G loss: 3.947084]\n",
            "******* 291 [D loss: 0.027883, acc: 100.00%] [G loss: 3.889825]\n",
            "******* 292 [D loss: 0.023611, acc: 100.00%] [G loss: 4.052633]\n",
            "******* 293 [D loss: 0.026606, acc: 100.00%] [G loss: 3.978327]\n",
            "******* 294 [D loss: 0.037246, acc: 100.00%] [G loss: 4.058054]\n",
            "******* 295 [D loss: 0.033725, acc: 100.00%] [G loss: 3.925148]\n",
            "******* 296 [D loss: 0.021934, acc: 100.00%] [G loss: 3.993293]\n",
            "******* 297 [D loss: 0.039532, acc: 100.00%] [G loss: 3.953525]\n",
            "******* 298 [D loss: 0.037219, acc: 99.22%] [G loss: 3.829571]\n",
            "******* 299 [D loss: 0.026221, acc: 100.00%] [G loss: 3.864639]\n",
            "******* 300 [D loss: 0.031733, acc: 100.00%] [G loss: 3.933057]\n",
            "******* 301 [D loss: 0.037502, acc: 100.00%] [G loss: 3.927267]\n",
            "******* 302 [D loss: 0.035235, acc: 100.00%] [G loss: 3.916459]\n",
            "******* 303 [D loss: 0.028524, acc: 100.00%] [G loss: 3.963127]\n",
            "******* 304 [D loss: 0.028008, acc: 100.00%] [G loss: 3.933797]\n",
            "******* 305 [D loss: 0.039003, acc: 100.00%] [G loss: 3.923979]\n",
            "******* 306 [D loss: 0.040447, acc: 99.22%] [G loss: 3.868951]\n",
            "******* 307 [D loss: 0.050628, acc: 98.44%] [G loss: 3.823356]\n",
            "******* 308 [D loss: 0.041688, acc: 100.00%] [G loss: 3.814112]\n",
            "******* 309 [D loss: 0.030408, acc: 100.00%] [G loss: 3.876191]\n",
            "******* 310 [D loss: 0.031414, acc: 100.00%] [G loss: 3.913128]\n",
            "******* 311 [D loss: 0.044902, acc: 99.22%] [G loss: 3.852710]\n",
            "******* 312 [D loss: 0.029178, acc: 100.00%] [G loss: 3.826735]\n",
            "******* 313 [D loss: 0.030510, acc: 100.00%] [G loss: 3.756067]\n",
            "******* 314 [D loss: 0.041677, acc: 100.00%] [G loss: 3.777743]\n",
            "******* 315 [D loss: 0.041683, acc: 100.00%] [G loss: 3.707634]\n",
            "******* 316 [D loss: 0.038261, acc: 100.00%] [G loss: 4.010366]\n",
            "******* 317 [D loss: 0.036769, acc: 100.00%] [G loss: 3.989127]\n",
            "******* 318 [D loss: 0.046327, acc: 100.00%] [G loss: 3.833690]\n",
            "******* 319 [D loss: 0.053744, acc: 100.00%] [G loss: 3.693644]\n",
            "******* 320 [D loss: 0.044244, acc: 100.00%] [G loss: 3.789034]\n",
            "******* 321 [D loss: 0.046009, acc: 99.22%] [G loss: 3.728583]\n",
            "******* 322 [D loss: 0.044834, acc: 100.00%] [G loss: 3.747887]\n",
            "******* 323 [D loss: 0.044858, acc: 100.00%] [G loss: 3.704331]\n",
            "******* 324 [D loss: 0.036445, acc: 100.00%] [G loss: 3.832938]\n",
            "******* 325 [D loss: 0.062353, acc: 98.44%] [G loss: 3.826673]\n",
            "******* 326 [D loss: 0.032455, acc: 100.00%] [G loss: 3.823815]\n",
            "******* 327 [D loss: 0.052134, acc: 99.22%] [G loss: 3.860846]\n",
            "******* 328 [D loss: 0.051512, acc: 99.22%] [G loss: 3.811655]\n",
            "******* 329 [D loss: 0.035355, acc: 100.00%] [G loss: 3.853931]\n",
            "******* 330 [D loss: 0.030720, acc: 100.00%] [G loss: 3.976213]\n",
            "******* 331 [D loss: 0.033253, acc: 100.00%] [G loss: 3.874887]\n",
            "******* 332 [D loss: 0.035730, acc: 100.00%] [G loss: 3.899900]\n",
            "******* 333 [D loss: 0.038113, acc: 99.22%] [G loss: 3.730451]\n",
            "******* 334 [D loss: 0.036392, acc: 100.00%] [G loss: 3.493798]\n",
            "******* 335 [D loss: 0.055010, acc: 99.22%] [G loss: 3.409742]\n",
            "******* 336 [D loss: 0.046088, acc: 100.00%] [G loss: 3.316736]\n",
            "******* 337 [D loss: 0.037205, acc: 100.00%] [G loss: 3.549691]\n",
            "******* 338 [D loss: 0.036920, acc: 100.00%] [G loss: 3.624953]\n",
            "******* 339 [D loss: 0.043469, acc: 100.00%] [G loss: 3.655697]\n",
            "******* 340 [D loss: 0.052265, acc: 100.00%] [G loss: 3.667277]\n",
            "******* 341 [D loss: 0.040608, acc: 100.00%] [G loss: 3.680434]\n",
            "******* 342 [D loss: 0.040741, acc: 100.00%] [G loss: 3.671467]\n",
            "******* 343 [D loss: 0.075391, acc: 99.22%] [G loss: 3.510478]\n",
            "******* 344 [D loss: 0.099524, acc: 96.88%] [G loss: 3.172535]\n",
            "******* 345 [D loss: 0.062759, acc: 98.44%] [G loss: 3.308179]\n",
            "******* 346 [D loss: 0.054618, acc: 99.22%] [G loss: 3.506596]\n",
            "******* 347 [D loss: 0.050620, acc: 100.00%] [G loss: 3.879175]\n",
            "******* 348 [D loss: 0.056472, acc: 100.00%] [G loss: 3.993836]\n",
            "******* 349 [D loss: 0.060439, acc: 100.00%] [G loss: 3.489868]\n",
            "******* 350 [D loss: 0.063437, acc: 100.00%] [G loss: 3.421105]\n",
            "******* 351 [D loss: 0.049528, acc: 100.00%] [G loss: 3.297743]\n",
            "******* 352 [D loss: 0.048439, acc: 99.22%] [G loss: 3.423967]\n",
            "******* 353 [D loss: 0.040073, acc: 100.00%] [G loss: 3.695418]\n",
            "******* 354 [D loss: 0.043563, acc: 100.00%] [G loss: 3.728446]\n",
            "******* 355 [D loss: 0.039075, acc: 100.00%] [G loss: 3.738081]\n",
            "******* 356 [D loss: 0.045086, acc: 99.22%] [G loss: 3.425990]\n",
            "******* 357 [D loss: 0.043002, acc: 100.00%] [G loss: 3.340065]\n",
            "******* 358 [D loss: 0.048867, acc: 100.00%] [G loss: 3.371922]\n",
            "******* 359 [D loss: 0.042542, acc: 99.22%] [G loss: 3.469625]\n",
            "******* 360 [D loss: 0.073099, acc: 99.22%] [G loss: 3.597032]\n",
            "******* 361 [D loss: 0.037733, acc: 100.00%] [G loss: 3.570014]\n",
            "******* 362 [D loss: 0.039514, acc: 100.00%] [G loss: 3.823133]\n",
            "******* 363 [D loss: 0.054188, acc: 100.00%] [G loss: 3.645914]\n",
            "******* 364 [D loss: 0.057789, acc: 98.44%] [G loss: 3.487713]\n",
            "******* 365 [D loss: 0.050080, acc: 100.00%] [G loss: 3.285966]\n",
            "******* 366 [D loss: 0.067685, acc: 100.00%] [G loss: 3.411177]\n",
            "******* 367 [D loss: 0.058009, acc: 99.22%] [G loss: 3.317048]\n",
            "******* 368 [D loss: 0.042054, acc: 100.00%] [G loss: 3.560546]\n",
            "******* 369 [D loss: 0.062823, acc: 98.44%] [G loss: 3.612590]\n",
            "******* 370 [D loss: 0.058894, acc: 100.00%] [G loss: 3.598997]\n",
            "******* 371 [D loss: 0.052953, acc: 100.00%] [G loss: 3.640214]\n",
            "******* 372 [D loss: 0.055169, acc: 99.22%] [G loss: 3.416286]\n",
            "******* 373 [D loss: 0.055205, acc: 100.00%] [G loss: 3.344724]\n",
            "******* 374 [D loss: 0.061793, acc: 99.22%] [G loss: 3.616972]\n",
            "******* 375 [D loss: 0.050850, acc: 100.00%] [G loss: 3.622934]\n",
            "******* 376 [D loss: 0.057012, acc: 100.00%] [G loss: 3.626131]\n",
            "******* 377 [D loss: 0.047090, acc: 99.22%] [G loss: 3.535329]\n",
            "******* 378 [D loss: 0.049835, acc: 100.00%] [G loss: 3.619707]\n",
            "******* 379 [D loss: 0.054071, acc: 99.22%] [G loss: 3.379895]\n",
            "******* 380 [D loss: 0.074384, acc: 98.44%] [G loss: 3.496505]\n",
            "******* 381 [D loss: 0.056097, acc: 100.00%] [G loss: 3.557979]\n",
            "******* 382 [D loss: 0.055200, acc: 100.00%] [G loss: 3.892313]\n",
            "******* 383 [D loss: 0.064968, acc: 98.44%] [G loss: 3.761883]\n",
            "******* 384 [D loss: 0.054389, acc: 98.44%] [G loss: 3.558938]\n",
            "******* 385 [D loss: 0.045833, acc: 99.22%] [G loss: 3.384677]\n",
            "******* 386 [D loss: 0.073595, acc: 99.22%] [G loss: 3.483393]\n",
            "******* 387 [D loss: 0.038211, acc: 100.00%] [G loss: 3.620439]\n",
            "******* 388 [D loss: 0.039259, acc: 100.00%] [G loss: 3.493302]\n",
            "******* 389 [D loss: 0.074278, acc: 99.22%] [G loss: 3.338090]\n",
            "******* 390 [D loss: 0.070277, acc: 99.22%] [G loss: 3.293897]\n",
            "******* 391 [D loss: 0.041382, acc: 100.00%] [G loss: 3.082744]\n",
            "******* 392 [D loss: 0.096638, acc: 98.44%] [G loss: 3.114703]\n",
            "******* 393 [D loss: 0.043412, acc: 100.00%] [G loss: 3.250982]\n",
            "******* 394 [D loss: 0.073302, acc: 98.44%] [G loss: 3.422078]\n",
            "******* 395 [D loss: 0.052061, acc: 100.00%] [G loss: 3.750928]\n",
            "******* 396 [D loss: 0.077840, acc: 99.22%] [G loss: 3.672110]\n",
            "******* 397 [D loss: 0.052157, acc: 100.00%] [G loss: 3.486587]\n",
            "******* 398 [D loss: 0.040501, acc: 100.00%] [G loss: 3.540493]\n",
            "******* 399 [D loss: 0.034189, acc: 100.00%] [G loss: 3.649640]\n",
            "******* 400 [D loss: 0.043220, acc: 100.00%] [G loss: 3.854336]\n",
            "0.00000003\n",
            "saved\n",
            "******* 401 [D loss: 0.049203, acc: 100.00%] [G loss: 3.573831]\n",
            "******* 402 [D loss: 0.059154, acc: 99.22%] [G loss: 3.472974]\n",
            "******* 403 [D loss: 0.051038, acc: 99.22%] [G loss: 3.461746]\n",
            "******* 404 [D loss: 0.058152, acc: 100.00%] [G loss: 3.368850]\n",
            "******* 405 [D loss: 0.063353, acc: 99.22%] [G loss: 3.695536]\n",
            "******* 406 [D loss: 0.042641, acc: 100.00%] [G loss: 3.997291]\n",
            "******* 407 [D loss: 0.067850, acc: 98.44%] [G loss: 3.390143]\n",
            "******* 408 [D loss: 0.085018, acc: 98.44%] [G loss: 3.178313]\n",
            "******* 409 [D loss: 0.073960, acc: 97.66%] [G loss: 3.260266]\n",
            "******* 410 [D loss: 0.037008, acc: 100.00%] [G loss: 3.502032]\n",
            "******* 411 [D loss: 0.057937, acc: 100.00%] [G loss: 3.612240]\n",
            "******* 412 [D loss: 0.068897, acc: 99.22%] [G loss: 3.668476]\n",
            "******* 413 [D loss: 0.045568, acc: 100.00%] [G loss: 3.593278]\n",
            "******* 414 [D loss: 0.043064, acc: 100.00%] [G loss: 3.662469]\n",
            "******* 415 [D loss: 0.053756, acc: 100.00%] [G loss: 3.676821]\n",
            "******* 416 [D loss: 0.037604, acc: 100.00%] [G loss: 3.633748]\n",
            "******* 417 [D loss: 0.069593, acc: 99.22%] [G loss: 3.578003]\n",
            "******* 418 [D loss: 0.036344, acc: 100.00%] [G loss: 3.569733]\n",
            "******* 419 [D loss: 0.061530, acc: 98.44%] [G loss: 3.469076]\n",
            "******* 420 [D loss: 0.055776, acc: 99.22%] [G loss: 3.413592]\n",
            "******* 421 [D loss: 0.035107, acc: 100.00%] [G loss: 3.743647]\n",
            "******* 422 [D loss: 0.061550, acc: 99.22%] [G loss: 3.681903]\n",
            "******* 423 [D loss: 0.042980, acc: 100.00%] [G loss: 3.875374]\n",
            "******* 424 [D loss: 0.061005, acc: 98.44%] [G loss: 3.675337]\n",
            "******* 425 [D loss: 0.046753, acc: 100.00%] [G loss: 3.783807]\n",
            "******* 426 [D loss: 0.094024, acc: 97.66%] [G loss: 3.544645]\n",
            "******* 427 [D loss: 0.087643, acc: 96.88%] [G loss: 3.139683]\n",
            "******* 428 [D loss: 0.063027, acc: 98.44%] [G loss: 3.530715]\n",
            "******* 429 [D loss: 0.042249, acc: 100.00%] [G loss: 3.935143]\n",
            "******* 430 [D loss: 0.023396, acc: 100.00%] [G loss: 4.347106]\n",
            "******* 431 [D loss: 0.081489, acc: 97.66%] [G loss: 3.725052]\n",
            "******* 432 [D loss: 0.059977, acc: 100.00%] [G loss: 3.381384]\n",
            "******* 433 [D loss: 0.081087, acc: 98.44%] [G loss: 3.434904]\n",
            "******* 434 [D loss: 0.048439, acc: 99.22%] [G loss: 3.717343]\n",
            "******* 435 [D loss: 0.019290, acc: 100.00%] [G loss: 4.459780]\n",
            "******* 436 [D loss: 0.082908, acc: 98.44%] [G loss: 3.734604]\n",
            "******* 437 [D loss: 0.048985, acc: 100.00%] [G loss: 3.564134]\n",
            "******* 438 [D loss: 0.045931, acc: 100.00%] [G loss: 3.537026]\n",
            "******* 439 [D loss: 0.040219, acc: 100.00%] [G loss: 3.789515]\n",
            "******* 440 [D loss: 0.052548, acc: 99.22%] [G loss: 3.916949]\n",
            "******* 441 [D loss: 0.043871, acc: 100.00%] [G loss: 3.993778]\n",
            "******* 442 [D loss: 0.051571, acc: 99.22%] [G loss: 3.923379]\n",
            "******* 443 [D loss: 0.041289, acc: 100.00%] [G loss: 3.924175]\n",
            "******* 444 [D loss: 0.056325, acc: 99.22%] [G loss: 3.898202]\n",
            "******* 445 [D loss: 0.039444, acc: 99.22%] [G loss: 4.153681]\n",
            "******* 446 [D loss: 0.054193, acc: 98.44%] [G loss: 3.861538]\n",
            "******* 447 [D loss: 0.039015, acc: 99.22%] [G loss: 3.809305]\n",
            "******* 448 [D loss: 0.028538, acc: 100.00%] [G loss: 3.735808]\n",
            "******* 449 [D loss: 0.041412, acc: 100.00%] [G loss: 3.999835]\n",
            "******* 450 [D loss: 0.051580, acc: 100.00%] [G loss: 4.050940]\n",
            "******* 451 [D loss: 0.096468, acc: 99.22%] [G loss: 3.371803]\n",
            "******* 452 [D loss: 0.090015, acc: 97.66%] [G loss: 3.469430]\n",
            "******* 453 [D loss: 0.043953, acc: 100.00%] [G loss: 3.945647]\n",
            "******* 454 [D loss: 0.023690, acc: 100.00%] [G loss: 4.402864]\n",
            "******* 455 [D loss: 0.029281, acc: 100.00%] [G loss: 4.628282]\n",
            "******* 456 [D loss: 0.074511, acc: 98.44%] [G loss: 4.079707]\n",
            "******* 457 [D loss: 0.074742, acc: 98.44%] [G loss: 3.606928]\n",
            "******* 458 [D loss: 0.052401, acc: 99.22%] [G loss: 3.957874]\n",
            "******* 459 [D loss: 0.030493, acc: 100.00%] [G loss: 4.061655]\n",
            "******* 460 [D loss: 0.048686, acc: 99.22%] [G loss: 4.420245]\n",
            "******* 461 [D loss: 0.058064, acc: 98.44%] [G loss: 4.244843]\n",
            "******* 462 [D loss: 0.057514, acc: 99.22%] [G loss: 4.037530]\n",
            "******* 463 [D loss: 0.047696, acc: 99.22%] [G loss: 4.197433]\n",
            "******* 464 [D loss: 0.030019, acc: 100.00%] [G loss: 4.254813]\n",
            "******* 465 [D loss: 0.029095, acc: 100.00%] [G loss: 4.249551]\n",
            "******* 466 [D loss: 0.062612, acc: 98.44%] [G loss: 3.984973]\n",
            "******* 467 [D loss: 0.081879, acc: 98.44%] [G loss: 3.444596]\n",
            "******* 468 [D loss: 0.055858, acc: 99.22%] [G loss: 3.489581]\n",
            "******* 469 [D loss: 0.046079, acc: 100.00%] [G loss: 4.184968]\n",
            "******* 470 [D loss: 0.019808, acc: 100.00%] [G loss: 4.463723]\n",
            "******* 471 [D loss: 0.038894, acc: 100.00%] [G loss: 4.660014]\n",
            "******* 472 [D loss: 0.076601, acc: 97.66%] [G loss: 3.931674]\n",
            "******* 473 [D loss: 0.043769, acc: 100.00%] [G loss: 3.764288]\n",
            "******* 474 [D loss: 0.045454, acc: 100.00%] [G loss: 4.044827]\n",
            "******* 475 [D loss: 0.024818, acc: 100.00%] [G loss: 4.359695]\n",
            "******* 476 [D loss: 0.038004, acc: 100.00%] [G loss: 4.224230]\n",
            "******* 477 [D loss: 0.066432, acc: 98.44%] [G loss: 3.884837]\n",
            "******* 478 [D loss: 0.046683, acc: 99.22%] [G loss: 3.378211]\n",
            "******* 479 [D loss: 0.045264, acc: 100.00%] [G loss: 3.539934]\n",
            "******* 480 [D loss: 0.029091, acc: 100.00%] [G loss: 4.192215]\n",
            "******* 481 [D loss: 0.040878, acc: 100.00%] [G loss: 4.339808]\n",
            "******* 482 [D loss: 0.042153, acc: 99.22%] [G loss: 4.608124]\n",
            "******* 483 [D loss: 0.032423, acc: 100.00%] [G loss: 4.177842]\n",
            "******* 484 [D loss: 0.052875, acc: 100.00%] [G loss: 4.099197]\n",
            "******* 485 [D loss: 0.040295, acc: 100.00%] [G loss: 4.126048]\n",
            "******* 486 [D loss: 0.040061, acc: 99.22%] [G loss: 4.742354]\n",
            "******* 487 [D loss: 0.044040, acc: 98.44%] [G loss: 4.850488]\n",
            "******* 488 [D loss: 0.036648, acc: 100.00%] [G loss: 4.260927]\n",
            "******* 489 [D loss: 0.053607, acc: 99.22%] [G loss: 3.811087]\n",
            "******* 490 [D loss: 0.044077, acc: 100.00%] [G loss: 3.922465]\n",
            "******* 491 [D loss: 0.031693, acc: 99.22%] [G loss: 4.273571]\n",
            "******* 492 [D loss: 0.027552, acc: 100.00%] [G loss: 4.493749]\n",
            "******* 493 [D loss: 0.022027, acc: 100.00%] [G loss: 4.962738]\n",
            "******* 494 [D loss: 0.025088, acc: 100.00%] [G loss: 4.769579]\n",
            "******* 495 [D loss: 0.030170, acc: 99.22%] [G loss: 4.440445]\n",
            "******* 496 [D loss: 0.053258, acc: 99.22%] [G loss: 4.259196]\n",
            "******* 497 [D loss: 0.035675, acc: 99.22%] [G loss: 4.420621]\n",
            "******* 498 [D loss: 0.043303, acc: 97.66%] [G loss: 4.401463]\n",
            "******* 499 [D loss: 0.039869, acc: 99.22%] [G loss: 4.203744]\n",
            "******* 500 [D loss: 0.027891, acc: 100.00%] [G loss: 4.626346]\n",
            "******* 501 [D loss: 0.040528, acc: 100.00%] [G loss: 4.779170]\n",
            "******* 502 [D loss: 0.055252, acc: 99.22%] [G loss: 4.376072]\n",
            "******* 503 [D loss: 0.035304, acc: 100.00%] [G loss: 4.683032]\n",
            "******* 504 [D loss: 0.022289, acc: 100.00%] [G loss: 5.045601]\n",
            "******* 505 [D loss: 0.047788, acc: 99.22%] [G loss: 4.696654]\n",
            "******* 506 [D loss: 0.020162, acc: 100.00%] [G loss: 4.400968]\n",
            "******* 507 [D loss: 0.024925, acc: 100.00%] [G loss: 4.504508]\n",
            "******* 508 [D loss: 0.032257, acc: 100.00%] [G loss: 4.946567]\n",
            "******* 509 [D loss: 0.024092, acc: 100.00%] [G loss: 5.102789]\n",
            "******* 510 [D loss: 0.019142, acc: 100.00%] [G loss: 5.038964]\n",
            "******* 511 [D loss: 0.047377, acc: 100.00%] [G loss: 4.272418]\n",
            "******* 512 [D loss: 0.042506, acc: 100.00%] [G loss: 3.993861]\n",
            "******* 513 [D loss: 0.044960, acc: 99.22%] [G loss: 4.141799]\n",
            "******* 514 [D loss: 0.032299, acc: 100.00%] [G loss: 4.273446]\n",
            "******* 515 [D loss: 0.020747, acc: 100.00%] [G loss: 5.234305]\n",
            "******* 516 [D loss: 0.026355, acc: 99.22%] [G loss: 5.148090]\n",
            "******* 517 [D loss: 0.033339, acc: 100.00%] [G loss: 4.626273]\n",
            "******* 518 [D loss: 0.029458, acc: 100.00%] [G loss: 4.184879]\n",
            "******* 519 [D loss: 0.039168, acc: 100.00%] [G loss: 4.060826]\n",
            "******* 520 [D loss: 0.032311, acc: 100.00%] [G loss: 4.548548]\n",
            "******* 521 [D loss: 0.016287, acc: 100.00%] [G loss: 5.052336]\n",
            "******* 522 [D loss: 0.038077, acc: 99.22%] [G loss: 5.144054]\n",
            "******* 523 [D loss: 0.046300, acc: 99.22%] [G loss: 4.333774]\n",
            "******* 524 [D loss: 0.054757, acc: 99.22%] [G loss: 4.203672]\n",
            "******* 525 [D loss: 0.033354, acc: 100.00%] [G loss: 4.057889]\n",
            "******* 526 [D loss: 0.021505, acc: 100.00%] [G loss: 4.619494]\n",
            "******* 527 [D loss: 0.018303, acc: 100.00%] [G loss: 4.783595]\n",
            "******* 528 [D loss: 0.022451, acc: 100.00%] [G loss: 4.958591]\n",
            "******* 529 [D loss: 0.027309, acc: 100.00%] [G loss: 4.701765]\n",
            "******* 530 [D loss: 0.019477, acc: 100.00%] [G loss: 4.449280]\n",
            "******* 531 [D loss: 0.028820, acc: 100.00%] [G loss: 4.693878]\n",
            "******* 532 [D loss: 0.019808, acc: 100.00%] [G loss: 5.007617]\n",
            "******* 533 [D loss: 0.016444, acc: 100.00%] [G loss: 4.918575]\n",
            "******* 534 [D loss: 0.069369, acc: 99.22%] [G loss: 4.664674]\n",
            "******* 535 [D loss: 0.038628, acc: 100.00%] [G loss: 4.126701]\n",
            "******* 536 [D loss: 0.034514, acc: 100.00%] [G loss: 4.144361]\n",
            "******* 537 [D loss: 0.029408, acc: 100.00%] [G loss: 4.575517]\n",
            "******* 538 [D loss: 0.072579, acc: 98.44%] [G loss: 4.434137]\n",
            "******* 539 [D loss: 0.033415, acc: 100.00%] [G loss: 4.146142]\n",
            "******* 540 [D loss: 0.028386, acc: 100.00%] [G loss: 4.577576]\n",
            "******* 541 [D loss: 0.032036, acc: 100.00%] [G loss: 4.836453]\n",
            "******* 542 [D loss: 0.017870, acc: 100.00%] [G loss: 5.176247]\n",
            "******* 543 [D loss: 0.039974, acc: 98.44%] [G loss: 4.924011]\n",
            "******* 544 [D loss: 0.034641, acc: 100.00%] [G loss: 4.450042]\n",
            "******* 545 [D loss: 0.028011, acc: 100.00%] [G loss: 4.318913]\n",
            "******* 546 [D loss: 0.025969, acc: 100.00%] [G loss: 4.792892]\n",
            "******* 547 [D loss: 0.033363, acc: 99.22%] [G loss: 4.716269]\n",
            "******* 548 [D loss: 0.019659, acc: 100.00%] [G loss: 4.592896]\n",
            "******* 549 [D loss: 0.016497, acc: 100.00%] [G loss: 4.752325]\n",
            "******* 550 [D loss: 0.026565, acc: 100.00%] [G loss: 4.576893]\n",
            "******* 551 [D loss: 0.025962, acc: 100.00%] [G loss: 4.636617]\n",
            "******* 552 [D loss: 0.033383, acc: 99.22%] [G loss: 4.526978]\n",
            "******* 553 [D loss: 0.019599, acc: 100.00%] [G loss: 4.762952]\n",
            "******* 554 [D loss: 0.018661, acc: 100.00%] [G loss: 5.029912]\n",
            "******* 555 [D loss: 0.027316, acc: 100.00%] [G loss: 4.968607]\n",
            "******* 556 [D loss: 0.029981, acc: 100.00%] [G loss: 4.727730]\n",
            "******* 557 [D loss: 0.037844, acc: 100.00%] [G loss: 4.498280]\n",
            "******* 558 [D loss: 0.027514, acc: 99.22%] [G loss: 4.465091]\n",
            "******* 559 [D loss: 0.027190, acc: 100.00%] [G loss: 4.683622]\n",
            "******* 560 [D loss: 0.029898, acc: 100.00%] [G loss: 4.891689]\n",
            "******* 561 [D loss: 0.037780, acc: 99.22%] [G loss: 4.635472]\n",
            "******* 562 [D loss: 0.032609, acc: 99.22%] [G loss: 4.458632]\n",
            "******* 563 [D loss: 0.020130, acc: 100.00%] [G loss: 4.398494]\n",
            "******* 564 [D loss: 0.021878, acc: 100.00%] [G loss: 4.769916]\n",
            "******* 565 [D loss: 0.023356, acc: 100.00%] [G loss: 4.795057]\n",
            "******* 566 [D loss: 0.031306, acc: 99.22%] [G loss: 4.390064]\n",
            "******* 567 [D loss: 0.022303, acc: 100.00%] [G loss: 4.385533]\n",
            "******* 568 [D loss: 0.020741, acc: 100.00%] [G loss: 4.627354]\n",
            "******* 569 [D loss: 0.028597, acc: 100.00%] [G loss: 4.634258]\n",
            "******* 570 [D loss: 0.042162, acc: 100.00%] [G loss: 4.525040]\n",
            "******* 571 [D loss: 0.035939, acc: 99.22%] [G loss: 4.204682]\n",
            "******* 572 [D loss: 0.039043, acc: 100.00%] [G loss: 4.079404]\n",
            "******* 573 [D loss: 0.026269, acc: 100.00%] [G loss: 4.827646]\n",
            "******* 574 [D loss: 0.027859, acc: 100.00%] [G loss: 4.730546]\n",
            "******* 575 [D loss: 0.022411, acc: 100.00%] [G loss: 5.127073]\n",
            "******* 576 [D loss: 0.047347, acc: 99.22%] [G loss: 4.217976]\n",
            "******* 577 [D loss: 0.028716, acc: 100.00%] [G loss: 4.488687]\n",
            "******* 578 [D loss: 0.032835, acc: 100.00%] [G loss: 4.716378]\n",
            "******* 579 [D loss: 0.030505, acc: 99.22%] [G loss: 4.778368]\n",
            "******* 580 [D loss: 0.023512, acc: 100.00%] [G loss: 4.935681]\n",
            "******* 581 [D loss: 0.023353, acc: 100.00%] [G loss: 4.477145]\n",
            "******* 582 [D loss: 0.036731, acc: 98.44%] [G loss: 4.582604]\n",
            "******* 583 [D loss: 0.040663, acc: 99.22%] [G loss: 4.158389]\n",
            "******* 584 [D loss: 0.030672, acc: 100.00%] [G loss: 4.248609]\n",
            "******* 585 [D loss: 0.029806, acc: 100.00%] [G loss: 4.675049]\n",
            "******* 586 [D loss: 0.016848, acc: 100.00%] [G loss: 5.182116]\n",
            "******* 587 [D loss: 0.073742, acc: 97.66%] [G loss: 4.206107]\n",
            "******* 588 [D loss: 0.041202, acc: 100.00%] [G loss: 4.055174]\n",
            "******* 589 [D loss: 0.024143, acc: 100.00%] [G loss: 4.469007]\n",
            "******* 590 [D loss: 0.019740, acc: 100.00%] [G loss: 5.140706]\n",
            "******* 591 [D loss: 0.024242, acc: 100.00%] [G loss: 5.183584]\n",
            "******* 592 [D loss: 0.029392, acc: 99.22%] [G loss: 4.564991]\n",
            "******* 593 [D loss: 0.030219, acc: 100.00%] [G loss: 4.027762]\n",
            "******* 594 [D loss: 0.040782, acc: 100.00%] [G loss: 4.790417]\n",
            "******* 595 [D loss: 0.022210, acc: 100.00%] [G loss: 4.894086]\n",
            "******* 596 [D loss: 0.021266, acc: 100.00%] [G loss: 5.237662]\n",
            "******* 597 [D loss: 0.029871, acc: 100.00%] [G loss: 4.785766]\n",
            "******* 598 [D loss: 0.026128, acc: 100.00%] [G loss: 4.207448]\n",
            "******* 599 [D loss: 0.033358, acc: 100.00%] [G loss: 4.457972]\n",
            "******* 600 [D loss: 0.032814, acc: 100.00%] [G loss: 4.676749]\n",
            "0.00000004\n",
            "saved\n",
            "******* 601 [D loss: 0.041805, acc: 99.22%] [G loss: 4.641292]\n",
            "******* 602 [D loss: 0.073911, acc: 98.44%] [G loss: 4.378494]\n",
            "******* 603 [D loss: 0.033833, acc: 100.00%] [G loss: 4.492355]\n",
            "******* 604 [D loss: 0.034258, acc: 100.00%] [G loss: 4.564976]\n",
            "******* 605 [D loss: 0.030513, acc: 100.00%] [G loss: 4.710240]\n",
            "******* 606 [D loss: 0.039860, acc: 99.22%] [G loss: 4.689404]\n",
            "******* 607 [D loss: 0.047814, acc: 98.44%] [G loss: 4.220061]\n",
            "******* 608 [D loss: 0.034834, acc: 100.00%] [G loss: 4.111519]\n",
            "******* 609 [D loss: 0.028935, acc: 99.22%] [G loss: 4.901449]\n",
            "******* 610 [D loss: 0.033477, acc: 100.00%] [G loss: 4.769326]\n",
            "******* 611 [D loss: 0.025455, acc: 100.00%] [G loss: 4.384972]\n",
            "******* 612 [D loss: 0.033525, acc: 100.00%] [G loss: 4.184165]\n",
            "******* 613 [D loss: 0.036172, acc: 100.00%] [G loss: 4.426873]\n",
            "******* 614 [D loss: 0.031089, acc: 100.00%] [G loss: 4.839089]\n",
            "******* 615 [D loss: 0.072873, acc: 99.22%] [G loss: 4.195943]\n",
            "******* 616 [D loss: 0.031234, acc: 100.00%] [G loss: 4.316346]\n",
            "******* 617 [D loss: 0.038699, acc: 100.00%] [G loss: 4.299416]\n",
            "******* 618 [D loss: 0.027504, acc: 100.00%] [G loss: 4.901617]\n",
            "******* 619 [D loss: 0.025020, acc: 100.00%] [G loss: 5.406486]\n",
            "******* 620 [D loss: 0.050975, acc: 99.22%] [G loss: 4.662879]\n",
            "******* 621 [D loss: 0.032700, acc: 100.00%] [G loss: 3.955397]\n",
            "******* 622 [D loss: 0.035000, acc: 100.00%] [G loss: 4.078009]\n",
            "******* 623 [D loss: 0.044170, acc: 98.44%] [G loss: 4.414219]\n",
            "******* 624 [D loss: 0.026521, acc: 100.00%] [G loss: 4.862875]\n",
            "******* 625 [D loss: 0.023523, acc: 100.00%] [G loss: 4.964815]\n",
            "******* 626 [D loss: 0.049312, acc: 99.22%] [G loss: 4.814074]\n",
            "******* 627 [D loss: 0.045503, acc: 100.00%] [G loss: 4.217618]\n",
            "******* 628 [D loss: 0.032594, acc: 100.00%] [G loss: 4.480240]\n",
            "******* 629 [D loss: 0.044354, acc: 99.22%] [G loss: 4.316709]\n",
            "******* 630 [D loss: 0.036109, acc: 100.00%] [G loss: 4.099227]\n",
            "******* 631 [D loss: 0.074256, acc: 97.66%] [G loss: 3.961918]\n",
            "******* 632 [D loss: 0.056127, acc: 99.22%] [G loss: 4.134660]\n",
            "******* 633 [D loss: 0.028296, acc: 100.00%] [G loss: 4.939019]\n",
            "******* 634 [D loss: 0.044029, acc: 99.22%] [G loss: 4.572003]\n",
            "******* 635 [D loss: 0.073818, acc: 97.66%] [G loss: 3.920657]\n",
            "******* 636 [D loss: 0.083654, acc: 98.44%] [G loss: 4.279243]\n",
            "******* 637 [D loss: 0.041721, acc: 100.00%] [G loss: 5.205833]\n",
            "******* 638 [D loss: 0.048117, acc: 98.44%] [G loss: 5.135139]\n",
            "******* 639 [D loss: 0.035804, acc: 100.00%] [G loss: 4.291598]\n",
            "******* 640 [D loss: 0.052764, acc: 99.22%] [G loss: 4.079858]\n",
            "******* 641 [D loss: 0.051439, acc: 100.00%] [G loss: 4.093591]\n",
            "******* 642 [D loss: 0.025636, acc: 100.00%] [G loss: 4.659931]\n",
            "******* 643 [D loss: 0.029147, acc: 99.22%] [G loss: 5.152394]\n",
            "******* 644 [D loss: 0.116659, acc: 97.66%] [G loss: 3.779171]\n",
            "******* 645 [D loss: 0.124523, acc: 94.53%] [G loss: 3.648833]\n",
            "******* 646 [D loss: 0.024059, acc: 100.00%] [G loss: 5.456959]\n",
            "******* 647 [D loss: 0.042607, acc: 98.44%] [G loss: 5.739372]\n",
            "******* 648 [D loss: 0.099434, acc: 95.31%] [G loss: 3.437268]\n",
            "******* 649 [D loss: 0.073913, acc: 100.00%] [G loss: 3.200685]\n",
            "******* 650 [D loss: 0.047584, acc: 100.00%] [G loss: 4.420447]\n",
            "******* 651 [D loss: 0.017249, acc: 100.00%] [G loss: 5.829629]\n",
            "******* 652 [D loss: 0.039743, acc: 99.22%] [G loss: 5.874634]\n",
            "******* 653 [D loss: 0.071491, acc: 97.66%] [G loss: 4.323745]\n",
            "******* 654 [D loss: 0.042766, acc: 100.00%] [G loss: 4.028131]\n",
            "******* 655 [D loss: 0.054006, acc: 99.22%] [G loss: 4.427302]\n",
            "******* 656 [D loss: 0.023617, acc: 100.00%] [G loss: 5.325922]\n",
            "******* 657 [D loss: 0.053593, acc: 99.22%] [G loss: 4.575109]\n",
            "******* 658 [D loss: 0.039487, acc: 99.22%] [G loss: 4.564995]\n",
            "******* 659 [D loss: 0.045890, acc: 99.22%] [G loss: 4.570867]\n",
            "******* 660 [D loss: 0.044353, acc: 99.22%] [G loss: 5.050771]\n",
            "******* 661 [D loss: 0.123664, acc: 97.66%] [G loss: 3.878549]\n",
            "******* 662 [D loss: 0.065920, acc: 100.00%] [G loss: 3.955412]\n",
            "******* 663 [D loss: 0.079626, acc: 97.66%] [G loss: 4.099507]\n",
            "******* 664 [D loss: 0.059961, acc: 98.44%] [G loss: 4.487229]\n",
            "******* 665 [D loss: 0.093039, acc: 96.88%] [G loss: 3.948168]\n",
            "******* 666 [D loss: 0.096664, acc: 97.66%] [G loss: 3.640829]\n",
            "******* 667 [D loss: 0.054604, acc: 99.22%] [G loss: 4.629906]\n",
            "******* 668 [D loss: 0.022302, acc: 100.00%] [G loss: 5.291731]\n",
            "******* 669 [D loss: 0.070283, acc: 99.22%] [G loss: 4.180741]\n",
            "******* 670 [D loss: 0.111660, acc: 96.09%] [G loss: 3.343433]\n",
            "******* 671 [D loss: 0.083775, acc: 96.09%] [G loss: 4.543999]\n",
            "******* 672 [D loss: 0.058722, acc: 99.22%] [G loss: 5.698540]\n",
            "******* 673 [D loss: 0.091461, acc: 96.88%] [G loss: 3.951686]\n",
            "******* 674 [D loss: 0.058737, acc: 99.22%] [G loss: 3.750789]\n",
            "******* 675 [D loss: 0.068520, acc: 100.00%] [G loss: 3.822068]\n",
            "******* 676 [D loss: 0.032382, acc: 100.00%] [G loss: 5.101560]\n",
            "******* 677 [D loss: 0.040542, acc: 98.44%] [G loss: 5.338375]\n",
            "******* 678 [D loss: 0.097964, acc: 97.66%] [G loss: 3.729855]\n",
            "******* 679 [D loss: 0.114863, acc: 98.44%] [G loss: 4.326760]\n",
            "******* 680 [D loss: 0.025136, acc: 100.00%] [G loss: 5.531093]\n",
            "******* 681 [D loss: 0.100371, acc: 95.31%] [G loss: 4.299173]\n",
            "******* 682 [D loss: 0.047490, acc: 100.00%] [G loss: 4.083668]\n",
            "******* 683 [D loss: 0.100254, acc: 96.88%] [G loss: 3.753351]\n",
            "******* 684 [D loss: 0.028618, acc: 100.00%] [G loss: 4.846890]\n",
            "******* 685 [D loss: 0.054225, acc: 99.22%] [G loss: 5.038671]\n",
            "******* 686 [D loss: 0.041942, acc: 100.00%] [G loss: 4.591249]\n",
            "******* 687 [D loss: 0.045784, acc: 100.00%] [G loss: 3.900014]\n",
            "******* 688 [D loss: 0.154064, acc: 93.75%] [G loss: 3.847511]\n",
            "******* 689 [D loss: 0.046433, acc: 99.22%] [G loss: 4.868147]\n",
            "******* 690 [D loss: 0.091202, acc: 98.44%] [G loss: 4.576719]\n",
            "******* 691 [D loss: 0.078482, acc: 99.22%] [G loss: 4.311102]\n",
            "******* 692 [D loss: 0.143848, acc: 96.88%] [G loss: 3.682069]\n",
            "******* 693 [D loss: 0.075513, acc: 98.44%] [G loss: 4.432054]\n",
            "******* 694 [D loss: 0.045507, acc: 99.22%] [G loss: 4.642807]\n",
            "******* 695 [D loss: 0.029898, acc: 100.00%] [G loss: 5.281262]\n",
            "******* 696 [D loss: 0.072988, acc: 98.44%] [G loss: 4.001150]\n",
            "******* 697 [D loss: 0.102198, acc: 97.66%] [G loss: 4.070593]\n",
            "******* 698 [D loss: 0.101405, acc: 98.44%] [G loss: 4.119695]\n",
            "******* 699 [D loss: 0.049685, acc: 99.22%] [G loss: 4.343393]\n",
            "******* 700 [D loss: 0.088482, acc: 96.09%] [G loss: 3.929012]\n",
            "******* 701 [D loss: 0.063131, acc: 99.22%] [G loss: 3.874645]\n",
            "******* 702 [D loss: 0.035383, acc: 100.00%] [G loss: 4.520118]\n",
            "******* 703 [D loss: 0.078809, acc: 99.22%] [G loss: 4.009397]\n",
            "******* 704 [D loss: 0.055777, acc: 100.00%] [G loss: 4.295992]\n",
            "******* 705 [D loss: 0.074571, acc: 97.66%] [G loss: 3.913724]\n",
            "******* 706 [D loss: 0.036977, acc: 100.00%] [G loss: 3.797566]\n",
            "******* 707 [D loss: 0.031375, acc: 100.00%] [G loss: 4.735102]\n",
            "******* 708 [D loss: 0.071844, acc: 98.44%] [G loss: 3.717204]\n",
            "******* 709 [D loss: 0.064502, acc: 100.00%] [G loss: 3.829091]\n",
            "******* 710 [D loss: 0.033567, acc: 100.00%] [G loss: 4.826436]\n",
            "******* 711 [D loss: 0.066671, acc: 97.66%] [G loss: 4.350151]\n",
            "******* 712 [D loss: 0.116086, acc: 97.66%] [G loss: 3.378714]\n",
            "******* 713 [D loss: 0.073080, acc: 99.22%] [G loss: 3.604057]\n",
            "******* 714 [D loss: 0.063559, acc: 98.44%] [G loss: 4.304995]\n",
            "******* 715 [D loss: 0.068196, acc: 99.22%] [G loss: 4.142700]\n",
            "******* 716 [D loss: 0.071343, acc: 98.44%] [G loss: 4.070461]\n",
            "******* 717 [D loss: 0.094812, acc: 98.44%] [G loss: 3.364373]\n",
            "******* 718 [D loss: 0.038120, acc: 99.22%] [G loss: 4.172138]\n",
            "******* 719 [D loss: 0.067718, acc: 97.66%] [G loss: 4.083411]\n",
            "******* 720 [D loss: 0.064944, acc: 99.22%] [G loss: 3.703547]\n",
            "******* 721 [D loss: 0.119714, acc: 98.44%] [G loss: 3.728717]\n",
            "******* 722 [D loss: 0.071946, acc: 97.66%] [G loss: 4.595442]\n",
            "******* 723 [D loss: 0.050389, acc: 97.66%] [G loss: 4.755315]\n",
            "******* 724 [D loss: 0.074625, acc: 99.22%] [G loss: 3.838801]\n",
            "******* 725 [D loss: 0.058475, acc: 100.00%] [G loss: 3.601349]\n",
            "******* 726 [D loss: 0.044118, acc: 98.44%] [G loss: 4.367364]\n",
            "******* 727 [D loss: 0.068344, acc: 98.44%] [G loss: 3.711842]\n",
            "******* 728 [D loss: 0.051235, acc: 100.00%] [G loss: 4.239558]\n",
            "******* 729 [D loss: 0.074500, acc: 98.44%] [G loss: 4.030569]\n",
            "******* 730 [D loss: 0.062900, acc: 99.22%] [G loss: 4.176919]\n",
            "******* 731 [D loss: 0.072769, acc: 99.22%] [G loss: 4.134695]\n",
            "******* 732 [D loss: 0.046461, acc: 99.22%] [G loss: 4.428450]\n",
            "******* 733 [D loss: 0.049945, acc: 100.00%] [G loss: 4.277335]\n",
            "******* 734 [D loss: 0.059061, acc: 98.44%] [G loss: 4.338488]\n",
            "******* 735 [D loss: 0.089593, acc: 97.66%] [G loss: 4.421343]\n",
            "******* 736 [D loss: 0.077336, acc: 98.44%] [G loss: 3.610501]\n",
            "******* 737 [D loss: 0.108448, acc: 99.22%] [G loss: 3.971361]\n",
            "******* 738 [D loss: 0.033901, acc: 100.00%] [G loss: 5.017318]\n",
            "******* 739 [D loss: 0.158332, acc: 95.31%] [G loss: 3.487464]\n",
            "******* 740 [D loss: 0.073799, acc: 99.22%] [G loss: 3.584009]\n",
            "******* 741 [D loss: 0.083793, acc: 98.44%] [G loss: 4.825761]\n",
            "******* 742 [D loss: 0.058224, acc: 98.44%] [G loss: 4.784527]\n",
            "******* 743 [D loss: 0.093128, acc: 97.66%] [G loss: 3.867197]\n",
            "******* 744 [D loss: 0.112607, acc: 97.66%] [G loss: 3.640755]\n",
            "******* 745 [D loss: 0.053441, acc: 99.22%] [G loss: 4.718205]\n",
            "******* 746 [D loss: 0.052271, acc: 99.22%] [G loss: 5.261663]\n",
            "******* 747 [D loss: 0.080983, acc: 98.44%] [G loss: 3.999121]\n",
            "******* 748 [D loss: 0.069743, acc: 99.22%] [G loss: 3.758354]\n",
            "******* 749 [D loss: 0.042525, acc: 100.00%] [G loss: 4.282620]\n",
            "******* 750 [D loss: 0.072333, acc: 98.44%] [G loss: 4.349862]\n",
            "******* 751 [D loss: 0.079310, acc: 97.66%] [G loss: 3.805495]\n",
            "******* 752 [D loss: 0.071799, acc: 98.44%] [G loss: 3.709714]\n",
            "******* 753 [D loss: 0.043557, acc: 99.22%] [G loss: 4.678675]\n",
            "******* 754 [D loss: 0.060828, acc: 99.22%] [G loss: 4.463944]\n",
            "******* 755 [D loss: 0.090153, acc: 97.66%] [G loss: 3.368298]\n",
            "******* 756 [D loss: 0.072525, acc: 100.00%] [G loss: 4.158538]\n",
            "******* 757 [D loss: 0.041395, acc: 100.00%] [G loss: 5.735186]\n",
            "******* 758 [D loss: 0.213996, acc: 94.53%] [G loss: 2.487171]\n",
            "******* 759 [D loss: 0.229379, acc: 89.06%] [G loss: 3.456382]\n",
            "******* 760 [D loss: 0.030553, acc: 99.22%] [G loss: 6.517160]\n",
            "******* 761 [D loss: 0.096089, acc: 95.31%] [G loss: 6.064176]\n",
            "******* 762 [D loss: 0.064700, acc: 98.44%] [G loss: 4.765442]\n",
            "******* 763 [D loss: 0.037019, acc: 100.00%] [G loss: 4.196888]\n",
            "******* 764 [D loss: 0.033566, acc: 99.22%] [G loss: 4.545382]\n",
            "******* 765 [D loss: 0.025703, acc: 100.00%] [G loss: 5.063390]\n",
            "******* 766 [D loss: 0.023224, acc: 100.00%] [G loss: 5.380795]\n",
            "******* 767 [D loss: 0.046086, acc: 97.66%] [G loss: 4.791473]\n",
            "******* 768 [D loss: 0.088552, acc: 97.66%] [G loss: 3.842950]\n",
            "******* 769 [D loss: 0.123247, acc: 96.88%] [G loss: 3.302511]\n",
            "******* 770 [D loss: 0.053845, acc: 99.22%] [G loss: 4.713982]\n",
            "******* 771 [D loss: 0.085901, acc: 98.44%] [G loss: 4.712140]\n",
            "******* 772 [D loss: 0.076897, acc: 98.44%] [G loss: 4.354253]\n",
            "******* 773 [D loss: 0.078095, acc: 99.22%] [G loss: 3.672472]\n",
            "******* 774 [D loss: 0.043392, acc: 100.00%] [G loss: 4.044025]\n",
            "******* 775 [D loss: 0.097772, acc: 97.66%] [G loss: 3.803469]\n",
            "******* 776 [D loss: 0.208460, acc: 93.75%] [G loss: 2.825508]\n",
            "******* 777 [D loss: 0.061811, acc: 100.00%] [G loss: 4.519554]\n",
            "******* 778 [D loss: 0.115806, acc: 98.44%] [G loss: 4.928638]\n",
            "******* 779 [D loss: 0.234777, acc: 92.19%] [G loss: 2.373296]\n",
            "******* 780 [D loss: 0.237170, acc: 89.84%] [G loss: 3.830877]\n",
            "******* 781 [D loss: 0.023269, acc: 100.00%] [G loss: 7.811692]\n",
            "******* 782 [D loss: 0.413190, acc: 86.72%] [G loss: 2.475736]\n",
            "******* 783 [D loss: 0.562092, acc: 71.88%] [G loss: 2.031306]\n",
            "******* 784 [D loss: 0.037415, acc: 100.00%] [G loss: 6.396229]\n",
            "******* 785 [D loss: 0.008275, acc: 100.00%] [G loss: 9.478876]\n",
            "******* 786 [D loss: 0.425905, acc: 85.16%] [G loss: 5.554422]\n",
            "******* 787 [D loss: 0.051730, acc: 100.00%] [G loss: 2.827277]\n",
            "******* 788 [D loss: 0.225610, acc: 87.50%] [G loss: 2.851154]\n",
            "******* 789 [D loss: 0.027373, acc: 100.00%] [G loss: 5.021918]\n",
            "******* 790 [D loss: 0.013427, acc: 100.00%] [G loss: 6.983609]\n",
            "******* 791 [D loss: 0.126605, acc: 96.88%] [G loss: 7.260466]\n",
            "******* 792 [D loss: 0.058048, acc: 96.88%] [G loss: 5.657223]\n",
            "******* 793 [D loss: 0.017710, acc: 100.00%] [G loss: 4.460309]\n",
            "******* 794 [D loss: 0.070429, acc: 98.44%] [G loss: 3.761110]\n",
            "******* 795 [D loss: 0.042847, acc: 100.00%] [G loss: 4.429986]\n",
            "******* 796 [D loss: 0.102563, acc: 96.88%] [G loss: 4.483636]\n",
            "******* 797 [D loss: 0.089536, acc: 98.44%] [G loss: 4.234754]\n",
            "******* 798 [D loss: 0.082927, acc: 98.44%] [G loss: 4.059705]\n",
            "******* 799 [D loss: 0.165450, acc: 96.09%] [G loss: 3.245781]\n",
            "******* 800 [D loss: 0.151493, acc: 96.09%] [G loss: 3.485219]\n",
            "0.00000005\n",
            "saved\n",
            "******* 801 [D loss: 0.086949, acc: 99.22%] [G loss: 4.445570]\n",
            "******* 802 [D loss: 0.213025, acc: 94.53%] [G loss: 3.510561]\n",
            "******* 803 [D loss: 0.279604, acc: 86.72%] [G loss: 3.121846]\n",
            "******* 804 [D loss: 0.128626, acc: 96.88%] [G loss: 3.573491]\n",
            "******* 805 [D loss: 0.196831, acc: 94.53%] [G loss: 3.388234]\n",
            "******* 806 [D loss: 0.094821, acc: 99.22%] [G loss: 3.520195]\n",
            "******* 807 [D loss: 0.068227, acc: 99.22%] [G loss: 4.359078]\n",
            "******* 808 [D loss: 0.201561, acc: 95.31%] [G loss: 3.280633]\n",
            "******* 809 [D loss: 0.164900, acc: 95.31%] [G loss: 3.281532]\n",
            "******* 810 [D loss: 0.072227, acc: 99.22%] [G loss: 4.261488]\n",
            "******* 811 [D loss: 0.158872, acc: 93.75%] [G loss: 3.573895]\n",
            "******* 812 [D loss: 0.138918, acc: 96.09%] [G loss: 2.817024]\n",
            "******* 813 [D loss: 0.094619, acc: 99.22%] [G loss: 3.451109]\n",
            "******* 814 [D loss: 0.049732, acc: 100.00%] [G loss: 4.485694]\n",
            "******* 815 [D loss: 0.089883, acc: 96.88%] [G loss: 4.061836]\n",
            "******* 816 [D loss: 0.215535, acc: 93.75%] [G loss: 2.866001]\n",
            "******* 817 [D loss: 0.198275, acc: 92.97%] [G loss: 3.907925]\n",
            "******* 818 [D loss: 0.132872, acc: 93.75%] [G loss: 4.744276]\n",
            "******* 819 [D loss: 0.261813, acc: 92.19%] [G loss: 2.371080]\n",
            "******* 820 [D loss: 0.260276, acc: 85.16%] [G loss: 3.513291]\n",
            "******* 821 [D loss: 0.101062, acc: 95.31%] [G loss: 5.584065]\n",
            "******* 822 [D loss: 0.241476, acc: 90.62%] [G loss: 3.031997]\n",
            "******* 823 [D loss: 0.145635, acc: 97.66%] [G loss: 3.062772]\n",
            "******* 824 [D loss: 0.122398, acc: 96.09%] [G loss: 4.159210]\n",
            "******* 825 [D loss: 0.045753, acc: 100.00%] [G loss: 5.092515]\n",
            "******* 826 [D loss: 0.153733, acc: 94.53%] [G loss: 3.221791]\n",
            "******* 827 [D loss: 0.112908, acc: 97.66%] [G loss: 2.819405]\n",
            "******* 828 [D loss: 0.081958, acc: 99.22%] [G loss: 4.440615]\n",
            "******* 829 [D loss: 0.097516, acc: 97.66%] [G loss: 4.742773]\n",
            "******* 830 [D loss: 0.283479, acc: 90.62%] [G loss: 2.591860]\n",
            "******* 831 [D loss: 0.287334, acc: 88.28%] [G loss: 3.343970]\n",
            "******* 832 [D loss: 0.072726, acc: 98.44%] [G loss: 5.565089]\n",
            "******* 833 [D loss: 0.318302, acc: 89.06%] [G loss: 2.835898]\n",
            "******* 834 [D loss: 0.251935, acc: 89.06%] [G loss: 2.746809]\n",
            "******* 835 [D loss: 0.111674, acc: 97.66%] [G loss: 3.946786]\n",
            "******* 836 [D loss: 0.144782, acc: 95.31%] [G loss: 4.293227]\n",
            "******* 837 [D loss: 0.233625, acc: 90.62%] [G loss: 2.756088]\n",
            "******* 838 [D loss: 0.218059, acc: 95.31%] [G loss: 3.241268]\n",
            "******* 839 [D loss: 0.052909, acc: 99.22%] [G loss: 4.723237]\n",
            "******* 840 [D loss: 0.187923, acc: 91.41%] [G loss: 3.679775]\n",
            "******* 841 [D loss: 0.151941, acc: 97.66%] [G loss: 2.468716]\n",
            "******* 842 [D loss: 0.196386, acc: 92.97%] [G loss: 3.638301]\n",
            "******* 843 [D loss: 0.066291, acc: 98.44%] [G loss: 5.460186]\n",
            "******* 844 [D loss: 0.159407, acc: 94.53%] [G loss: 4.535542]\n",
            "******* 845 [D loss: 0.138082, acc: 95.31%] [G loss: 2.830982]\n",
            "******* 846 [D loss: 0.193395, acc: 90.62%] [G loss: 2.983047]\n",
            "******* 847 [D loss: 0.063021, acc: 99.22%] [G loss: 4.742429]\n",
            "******* 848 [D loss: 0.237511, acc: 89.84%] [G loss: 3.162600]\n",
            "******* 849 [D loss: 0.190142, acc: 94.53%] [G loss: 3.081157]\n",
            "******* 850 [D loss: 0.101658, acc: 98.44%] [G loss: 4.053286]\n",
            "******* 851 [D loss: 0.147832, acc: 94.53%] [G loss: 4.016418]\n",
            "******* 852 [D loss: 0.197753, acc: 93.75%] [G loss: 2.811785]\n",
            "******* 853 [D loss: 0.164958, acc: 95.31%] [G loss: 3.348549]\n",
            "******* 854 [D loss: 0.137953, acc: 96.09%] [G loss: 3.983016]\n",
            "******* 855 [D loss: 0.172083, acc: 94.53%] [G loss: 3.800899]\n",
            "******* 856 [D loss: 0.114413, acc: 98.44%] [G loss: 3.486941]\n",
            "******* 857 [D loss: 0.085097, acc: 99.22%] [G loss: 4.069578]\n",
            "******* 858 [D loss: 0.141146, acc: 94.53%] [G loss: 3.593990]\n",
            "******* 859 [D loss: 0.178472, acc: 94.53%] [G loss: 3.046463]\n",
            "******* 860 [D loss: 0.121485, acc: 97.66%] [G loss: 3.657411]\n",
            "******* 861 [D loss: 0.074021, acc: 99.22%] [G loss: 4.408990]\n",
            "******* 862 [D loss: 0.169320, acc: 95.31%] [G loss: 3.310554]\n",
            "******* 863 [D loss: 0.169863, acc: 96.09%] [G loss: 3.116579]\n",
            "******* 864 [D loss: 0.137910, acc: 96.88%] [G loss: 3.726757]\n",
            "******* 865 [D loss: 0.113627, acc: 97.66%] [G loss: 3.810206]\n",
            "******* 866 [D loss: 0.271480, acc: 90.62%] [G loss: 2.532383]\n",
            "******* 867 [D loss: 0.200149, acc: 92.97%] [G loss: 3.480340]\n",
            "******* 868 [D loss: 0.097417, acc: 97.66%] [G loss: 4.694045]\n",
            "******* 869 [D loss: 0.208747, acc: 93.75%] [G loss: 3.149425]\n",
            "******* 870 [D loss: 0.230057, acc: 89.06%] [G loss: 3.259177]\n",
            "******* 871 [D loss: 0.122822, acc: 97.66%] [G loss: 4.656058]\n",
            "******* 872 [D loss: 0.386147, acc: 87.50%] [G loss: 2.453314]\n",
            "******* 873 [D loss: 0.341193, acc: 79.69%] [G loss: 2.791117]\n",
            "******* 874 [D loss: 0.160731, acc: 98.44%] [G loss: 4.706731]\n",
            "******* 875 [D loss: 0.224658, acc: 92.19%] [G loss: 4.428600]\n",
            "******* 876 [D loss: 0.244058, acc: 92.19%] [G loss: 2.452246]\n",
            "******* 877 [D loss: 0.206210, acc: 92.97%] [G loss: 3.267691]\n",
            "******* 878 [D loss: 0.078174, acc: 97.66%] [G loss: 4.908006]\n",
            "******* 879 [D loss: 0.277865, acc: 91.41%] [G loss: 3.749462]\n",
            "******* 880 [D loss: 0.297576, acc: 89.84%] [G loss: 2.899183]\n",
            "******* 881 [D loss: 0.162623, acc: 96.09%] [G loss: 3.403077]\n",
            "******* 882 [D loss: 0.171752, acc: 93.75%] [G loss: 3.710565]\n",
            "******* 883 [D loss: 0.274457, acc: 86.72%] [G loss: 3.180998]\n",
            "******* 884 [D loss: 0.184357, acc: 95.31%] [G loss: 2.953328]\n",
            "******* 885 [D loss: 0.205054, acc: 91.41%] [G loss: 3.288077]\n",
            "******* 886 [D loss: 0.153566, acc: 96.09%] [G loss: 3.581366]\n",
            "******* 887 [D loss: 0.299904, acc: 85.94%] [G loss: 3.142642]\n",
            "******* 888 [D loss: 0.135874, acc: 96.09%] [G loss: 4.019229]\n",
            "******* 889 [D loss: 0.128388, acc: 97.66%] [G loss: 4.154812]\n",
            "******* 890 [D loss: 0.254722, acc: 92.97%] [G loss: 3.009591]\n",
            "******* 891 [D loss: 0.231943, acc: 92.19%] [G loss: 3.008491]\n",
            "******* 892 [D loss: 0.107748, acc: 99.22%] [G loss: 4.270319]\n",
            "******* 893 [D loss: 0.067980, acc: 97.66%] [G loss: 4.626533]\n",
            "******* 894 [D loss: 0.317216, acc: 89.84%] [G loss: 2.967770]\n",
            "******* 895 [D loss: 0.440360, acc: 74.22%] [G loss: 2.860515]\n",
            "******* 896 [D loss: 0.076566, acc: 98.44%] [G loss: 4.713421]\n",
            "******* 897 [D loss: 0.329664, acc: 91.41%] [G loss: 3.734445]\n",
            "******* 898 [D loss: 0.303501, acc: 87.50%] [G loss: 2.390241]\n",
            "******* 899 [D loss: 0.185365, acc: 93.75%] [G loss: 3.531767]\n",
            "******* 900 [D loss: 0.052229, acc: 99.22%] [G loss: 5.363829]\n",
            "******* 901 [D loss: 0.189584, acc: 94.53%] [G loss: 3.958322]\n",
            "******* 902 [D loss: 0.199937, acc: 94.53%] [G loss: 2.803725]\n",
            "******* 903 [D loss: 0.225418, acc: 89.06%] [G loss: 3.464579]\n",
            "******* 904 [D loss: 0.078466, acc: 97.66%] [G loss: 5.132672]\n",
            "******* 905 [D loss: 0.270869, acc: 92.19%] [G loss: 3.840402]\n",
            "******* 906 [D loss: 0.150974, acc: 97.66%] [G loss: 2.733767]\n",
            "******* 907 [D loss: 0.145846, acc: 96.09%] [G loss: 3.209625]\n",
            "******* 908 [D loss: 0.137963, acc: 94.53%] [G loss: 3.908396]\n",
            "******* 909 [D loss: 0.112800, acc: 95.31%] [G loss: 3.897000]\n",
            "******* 910 [D loss: 0.315077, acc: 89.06%] [G loss: 2.675163]\n",
            "******* 911 [D loss: 0.163461, acc: 94.53%] [G loss: 3.082882]\n",
            "******* 912 [D loss: 0.202139, acc: 94.53%] [G loss: 4.276149]\n",
            "******* 913 [D loss: 0.271594, acc: 89.84%] [G loss: 3.220351]\n",
            "******* 914 [D loss: 0.340611, acc: 87.50%] [G loss: 2.497608]\n",
            "******* 915 [D loss: 0.155200, acc: 96.88%] [G loss: 3.878197]\n",
            "******* 916 [D loss: 0.220062, acc: 95.31%] [G loss: 4.235205]\n",
            "******* 917 [D loss: 0.175584, acc: 92.97%] [G loss: 2.973125]\n",
            "******* 918 [D loss: 0.249829, acc: 89.06%] [G loss: 2.654429]\n",
            "******* 919 [D loss: 0.219092, acc: 92.97%] [G loss: 3.768147]\n",
            "******* 920 [D loss: 0.190143, acc: 94.53%] [G loss: 4.545498]\n",
            "******* 921 [D loss: 0.269718, acc: 91.41%] [G loss: 3.276960]\n",
            "******* 922 [D loss: 0.254401, acc: 89.06%] [G loss: 3.296049]\n",
            "******* 923 [D loss: 0.130181, acc: 95.31%] [G loss: 4.178572]\n",
            "******* 924 [D loss: 0.114754, acc: 96.88%] [G loss: 3.948971]\n",
            "******* 925 [D loss: 0.118524, acc: 98.44%] [G loss: 3.042112]\n",
            "******* 926 [D loss: 0.258574, acc: 92.97%] [G loss: 2.929148]\n",
            "******* 927 [D loss: 0.124705, acc: 95.31%] [G loss: 4.078240]\n",
            "******* 928 [D loss: 0.147703, acc: 95.31%] [G loss: 3.947924]\n",
            "******* 929 [D loss: 0.251557, acc: 89.84%] [G loss: 2.589658]\n",
            "******* 930 [D loss: 0.409604, acc: 84.38%] [G loss: 2.686615]\n",
            "******* 931 [D loss: 0.128934, acc: 97.66%] [G loss: 4.631573]\n",
            "******* 932 [D loss: 0.214129, acc: 92.19%] [G loss: 4.513997]\n",
            "******* 933 [D loss: 0.429688, acc: 85.94%] [G loss: 2.148637]\n",
            "******* 934 [D loss: 0.330547, acc: 84.38%] [G loss: 2.884145]\n",
            "******* 935 [D loss: 0.122704, acc: 98.44%] [G loss: 5.403201]\n",
            "******* 936 [D loss: 0.332980, acc: 87.50%] [G loss: 3.666752]\n",
            "******* 937 [D loss: 0.271631, acc: 91.41%] [G loss: 2.308964]\n",
            "******* 938 [D loss: 0.223488, acc: 91.41%] [G loss: 3.529583]\n",
            "******* 939 [D loss: 0.092119, acc: 97.66%] [G loss: 5.020451]\n",
            "******* 940 [D loss: 0.209965, acc: 90.62%] [G loss: 3.719714]\n",
            "******* 941 [D loss: 0.162242, acc: 95.31%] [G loss: 2.747169]\n",
            "******* 942 [D loss: 0.228561, acc: 90.62%] [G loss: 3.188294]\n",
            "******* 943 [D loss: 0.096277, acc: 98.44%] [G loss: 4.163128]\n",
            "******* 944 [D loss: 0.235184, acc: 94.53%] [G loss: 3.876588]\n",
            "******* 945 [D loss: 0.194511, acc: 94.53%] [G loss: 3.096980]\n",
            "******* 946 [D loss: 0.250633, acc: 93.75%] [G loss: 2.684412]\n",
            "******* 947 [D loss: 0.125050, acc: 95.31%] [G loss: 3.790063]\n",
            "******* 948 [D loss: 0.222526, acc: 92.19%] [G loss: 3.474466]\n",
            "******* 949 [D loss: 0.213990, acc: 94.53%] [G loss: 3.021754]\n",
            "******* 950 [D loss: 0.228014, acc: 93.75%] [G loss: 3.185601]\n",
            "******* 951 [D loss: 0.096156, acc: 96.88%] [G loss: 4.108215]\n",
            "******* 952 [D loss: 0.134011, acc: 95.31%] [G loss: 4.337161]\n",
            "******* 953 [D loss: 0.159100, acc: 96.09%] [G loss: 3.474332]\n",
            "******* 954 [D loss: 0.145517, acc: 96.88%] [G loss: 3.042956]\n",
            "******* 955 [D loss: 0.128642, acc: 99.22%] [G loss: 3.690310]\n",
            "******* 956 [D loss: 0.131398, acc: 96.88%] [G loss: 3.730617]\n",
            "******* 957 [D loss: 0.176874, acc: 95.31%] [G loss: 3.023736]\n",
            "******* 958 [D loss: 0.203682, acc: 93.75%] [G loss: 2.667489]\n",
            "******* 959 [D loss: 0.210717, acc: 96.88%] [G loss: 3.559732]\n",
            "******* 960 [D loss: 0.158614, acc: 96.88%] [G loss: 3.656359]\n",
            "******* 961 [D loss: 0.164836, acc: 98.44%] [G loss: 3.308737]\n",
            "******* 962 [D loss: 0.193435, acc: 93.75%] [G loss: 3.020819]\n",
            "******* 963 [D loss: 0.239146, acc: 91.41%] [G loss: 3.184024]\n",
            "******* 964 [D loss: 0.182002, acc: 93.75%] [G loss: 3.263681]\n",
            "******* 965 [D loss: 0.307390, acc: 85.16%] [G loss: 2.922938]\n",
            "******* 966 [D loss: 0.135055, acc: 95.31%] [G loss: 3.319264]\n",
            "******* 967 [D loss: 0.248434, acc: 91.41%] [G loss: 3.100847]\n",
            "******* 968 [D loss: 0.221293, acc: 94.53%] [G loss: 3.165061]\n",
            "******* 969 [D loss: 0.147978, acc: 96.88%] [G loss: 3.610003]\n",
            "******* 970 [D loss: 0.325568, acc: 92.19%] [G loss: 3.326510]\n",
            "******* 971 [D loss: 0.263066, acc: 91.41%] [G loss: 2.627558]\n",
            "******* 972 [D loss: 0.204333, acc: 92.97%] [G loss: 4.010117]\n",
            "******* 973 [D loss: 0.204673, acc: 93.75%] [G loss: 4.078267]\n",
            "******* 974 [D loss: 0.257344, acc: 92.19%] [G loss: 2.577897]\n",
            "******* 975 [D loss: 0.173532, acc: 94.53%] [G loss: 3.184227]\n",
            "******* 976 [D loss: 0.146017, acc: 96.88%] [G loss: 4.403499]\n",
            "******* 977 [D loss: 0.302894, acc: 89.06%] [G loss: 3.098307]\n",
            "******* 978 [D loss: 0.155539, acc: 97.66%] [G loss: 3.027874]\n",
            "******* 979 [D loss: 0.140834, acc: 97.66%] [G loss: 3.314478]\n",
            "******* 980 [D loss: 0.131539, acc: 96.88%] [G loss: 3.679858]\n",
            "******* 981 [D loss: 0.284293, acc: 92.19%] [G loss: 2.837253]\n",
            "******* 982 [D loss: 0.178116, acc: 94.53%] [G loss: 2.674356]\n",
            "******* 983 [D loss: 0.324019, acc: 90.62%] [G loss: 3.490681]\n",
            "******* 984 [D loss: 0.132578, acc: 96.88%] [G loss: 3.849707]\n",
            "******* 985 [D loss: 0.200137, acc: 93.75%] [G loss: 3.725098]\n",
            "******* 986 [D loss: 0.164069, acc: 95.31%] [G loss: 2.958410]\n",
            "******* 987 [D loss: 0.219819, acc: 92.19%] [G loss: 3.718019]\n",
            "******* 988 [D loss: 0.116063, acc: 96.88%] [G loss: 4.784781]\n",
            "******* 989 [D loss: 0.203928, acc: 92.19%] [G loss: 3.500438]\n",
            "******* 990 [D loss: 0.179505, acc: 94.53%] [G loss: 2.724908]\n",
            "******* 991 [D loss: 0.136888, acc: 96.09%] [G loss: 3.526082]\n",
            "******* 992 [D loss: 0.183207, acc: 94.53%] [G loss: 4.198495]\n",
            "******* 993 [D loss: 0.161859, acc: 93.75%] [G loss: 3.495897]\n",
            "******* 994 [D loss: 0.180072, acc: 95.31%] [G loss: 3.231448]\n",
            "******* 995 [D loss: 0.166545, acc: 96.88%] [G loss: 3.431924]\n",
            "******* 996 [D loss: 0.215372, acc: 93.75%] [G loss: 3.532294]\n",
            "******* 997 [D loss: 0.137592, acc: 94.53%] [G loss: 3.488990]\n",
            "******* 998 [D loss: 0.147415, acc: 96.09%] [G loss: 3.636859]\n",
            "******* 999 [D loss: 0.111554, acc: 96.09%] [G loss: 3.856629]\n",
            "******* 1000 [D loss: 0.122772, acc: 96.88%] [G loss: 3.740327]\n",
            "0.00000006\n",
            "saved\n",
            "******* 1001 [D loss: 0.162109, acc: 94.53%] [G loss: 3.040455]\n",
            "******* 1002 [D loss: 0.178937, acc: 96.88%] [G loss: 2.950969]\n",
            "******* 1003 [D loss: 0.214790, acc: 93.75%] [G loss: 3.512635]\n",
            "******* 1004 [D loss: 0.112587, acc: 97.66%] [G loss: 3.379828]\n",
            "******* 1005 [D loss: 0.175585, acc: 96.09%] [G loss: 3.264096]\n",
            "******* 1006 [D loss: 0.172159, acc: 96.88%] [G loss: 3.339413]\n",
            "******* 1007 [D loss: 0.165126, acc: 96.88%] [G loss: 3.946454]\n",
            "******* 1008 [D loss: 0.190253, acc: 92.97%] [G loss: 3.694314]\n",
            "******* 1009 [D loss: 0.105459, acc: 96.88%] [G loss: 3.559630]\n",
            "******* 1010 [D loss: 0.226818, acc: 91.41%] [G loss: 3.076990]\n",
            "******* 1011 [D loss: 0.149299, acc: 95.31%] [G loss: 2.913362]\n",
            "******* 1012 [D loss: 0.142082, acc: 96.09%] [G loss: 3.469275]\n",
            "******* 1013 [D loss: 0.086391, acc: 98.44%] [G loss: 3.967070]\n",
            "******* 1014 [D loss: 0.090004, acc: 96.88%] [G loss: 3.683678]\n",
            "******* 1015 [D loss: 0.138153, acc: 97.66%] [G loss: 3.275958]\n",
            "******* 1016 [D loss: 0.191285, acc: 96.09%] [G loss: 2.944677]\n",
            "******* 1017 [D loss: 0.117646, acc: 98.44%] [G loss: 3.718337]\n",
            "******* 1018 [D loss: 0.160253, acc: 93.75%] [G loss: 3.850133]\n",
            "******* 1019 [D loss: 0.208675, acc: 94.53%] [G loss: 2.788526]\n",
            "******* 1020 [D loss: 0.226273, acc: 92.19%] [G loss: 3.603745]\n",
            "******* 1021 [D loss: 0.092502, acc: 96.88%] [G loss: 4.670565]\n",
            "******* 1022 [D loss: 0.204871, acc: 92.97%] [G loss: 3.521939]\n",
            "******* 1023 [D loss: 0.213913, acc: 92.97%] [G loss: 3.074602]\n",
            "******* 1024 [D loss: 0.099002, acc: 97.66%] [G loss: 3.353379]\n",
            "******* 1025 [D loss: 0.131370, acc: 96.88%] [G loss: 3.764740]\n",
            "******* 1026 [D loss: 0.081735, acc: 99.22%] [G loss: 3.600567]\n",
            "******* 1027 [D loss: 0.182509, acc: 93.75%] [G loss: 2.656253]\n",
            "******* 1028 [D loss: 0.133446, acc: 96.88%] [G loss: 3.388077]\n",
            "******* 1029 [D loss: 0.112974, acc: 96.88%] [G loss: 3.989313]\n",
            "******* 1030 [D loss: 0.298871, acc: 89.84%] [G loss: 2.530426]\n",
            "******* 1031 [D loss: 0.198934, acc: 95.31%] [G loss: 2.759647]\n",
            "******* 1032 [D loss: 0.140441, acc: 96.09%] [G loss: 3.704803]\n",
            "******* 1033 [D loss: 0.298894, acc: 87.50%] [G loss: 3.091616]\n",
            "******* 1034 [D loss: 0.155302, acc: 96.09%] [G loss: 3.147051]\n",
            "******* 1035 [D loss: 0.132020, acc: 96.09%] [G loss: 3.430569]\n",
            "******* 1036 [D loss: 0.163504, acc: 96.09%] [G loss: 3.268995]\n",
            "******* 1037 [D loss: 0.117364, acc: 97.66%] [G loss: 3.594321]\n",
            "******* 1038 [D loss: 0.187726, acc: 96.09%] [G loss: 3.426919]\n",
            "******* 1039 [D loss: 0.228222, acc: 95.31%] [G loss: 2.990403]\n",
            "******* 1040 [D loss: 0.200028, acc: 94.53%] [G loss: 3.338490]\n",
            "******* 1041 [D loss: 0.189272, acc: 96.09%] [G loss: 3.430996]\n",
            "******* 1042 [D loss: 0.243541, acc: 94.53%] [G loss: 2.902772]\n",
            "******* 1043 [D loss: 0.248464, acc: 92.97%] [G loss: 2.706222]\n",
            "******* 1044 [D loss: 0.208542, acc: 93.75%] [G loss: 3.392208]\n",
            "******* 1045 [D loss: 0.080369, acc: 98.44%] [G loss: 4.474381]\n",
            "******* 1046 [D loss: 0.320535, acc: 92.97%] [G loss: 2.951124]\n",
            "******* 1047 [D loss: 0.205213, acc: 93.75%] [G loss: 2.429908]\n",
            "******* 1048 [D loss: 0.129401, acc: 98.44%] [G loss: 3.564713]\n",
            "******* 1049 [D loss: 0.184992, acc: 94.53%] [G loss: 4.019809]\n",
            "******* 1050 [D loss: 0.159548, acc: 96.09%] [G loss: 3.505986]\n",
            "******* 1051 [D loss: 0.249950, acc: 91.41%] [G loss: 2.863525]\n",
            "******* 1052 [D loss: 0.129612, acc: 97.66%] [G loss: 3.335510]\n",
            "******* 1053 [D loss: 0.137223, acc: 96.09%] [G loss: 4.065405]\n",
            "******* 1054 [D loss: 0.102354, acc: 96.09%] [G loss: 3.352202]\n",
            "******* 1055 [D loss: 0.163943, acc: 96.09%] [G loss: 2.905569]\n",
            "******* 1056 [D loss: 0.159966, acc: 96.09%] [G loss: 3.342839]\n",
            "******* 1057 [D loss: 0.200939, acc: 92.97%] [G loss: 3.479759]\n",
            "******* 1058 [D loss: 0.244595, acc: 91.41%] [G loss: 2.776907]\n",
            "******* 1059 [D loss: 0.121803, acc: 99.22%] [G loss: 3.109966]\n",
            "******* 1060 [D loss: 0.200109, acc: 92.97%] [G loss: 3.424904]\n",
            "******* 1061 [D loss: 0.328299, acc: 93.75%] [G loss: 3.176876]\n",
            "******* 1062 [D loss: 0.141442, acc: 98.44%] [G loss: 3.542595]\n",
            "******* 1063 [D loss: 0.201187, acc: 95.31%] [G loss: 3.560433]\n",
            "******* 1064 [D loss: 0.281021, acc: 90.62%] [G loss: 2.874390]\n",
            "******* 1065 [D loss: 0.187971, acc: 94.53%] [G loss: 2.716138]\n",
            "******* 1066 [D loss: 0.138818, acc: 97.66%] [G loss: 3.802172]\n",
            "******* 1067 [D loss: 0.211458, acc: 91.41%] [G loss: 3.240894]\n",
            "******* 1068 [D loss: 0.178649, acc: 96.88%] [G loss: 2.926406]\n",
            "******* 1069 [D loss: 0.157341, acc: 94.53%] [G loss: 3.361703]\n",
            "******* 1070 [D loss: 0.120987, acc: 96.09%] [G loss: 3.879817]\n",
            "******* 1071 [D loss: 0.185837, acc: 93.75%] [G loss: 3.156781]\n",
            "******* 1072 [D loss: 0.176729, acc: 95.31%] [G loss: 2.753549]\n",
            "******* 1073 [D loss: 0.111799, acc: 99.22%] [G loss: 3.637163]\n",
            "******* 1074 [D loss: 0.161838, acc: 94.53%] [G loss: 3.758315]\n",
            "******* 1075 [D loss: 0.156469, acc: 92.97%] [G loss: 2.873163]\n",
            "******* 1076 [D loss: 0.136958, acc: 98.44%] [G loss: 3.555791]\n",
            "******* 1077 [D loss: 0.159670, acc: 96.88%] [G loss: 3.425415]\n",
            "******* 1078 [D loss: 0.371423, acc: 87.50%] [G loss: 2.715754]\n",
            "******* 1079 [D loss: 0.140110, acc: 95.31%] [G loss: 3.492486]\n",
            "******* 1080 [D loss: 0.227491, acc: 94.53%] [G loss: 3.882266]\n",
            "******* 1081 [D loss: 0.136898, acc: 96.88%] [G loss: 3.320303]\n",
            "******* 1082 [D loss: 0.208565, acc: 94.53%] [G loss: 3.047666]\n",
            "******* 1083 [D loss: 0.166206, acc: 97.66%] [G loss: 3.566581]\n",
            "******* 1084 [D loss: 0.118741, acc: 96.88%] [G loss: 4.387854]\n",
            "******* 1085 [D loss: 0.206069, acc: 93.75%] [G loss: 3.214275]\n",
            "******* 1086 [D loss: 0.182611, acc: 96.88%] [G loss: 2.578662]\n",
            "******* 1087 [D loss: 0.096415, acc: 98.44%] [G loss: 3.327829]\n",
            "******* 1088 [D loss: 0.143061, acc: 92.97%] [G loss: 3.628699]\n",
            "******* 1089 [D loss: 0.137562, acc: 95.31%] [G loss: 3.786162]\n",
            "******* 1090 [D loss: 0.230884, acc: 92.19%] [G loss: 2.617366]\n",
            "******* 1091 [D loss: 0.304098, acc: 89.06%] [G loss: 3.095508]\n",
            "******* 1092 [D loss: 0.160643, acc: 96.88%] [G loss: 3.953773]\n",
            "******* 1093 [D loss: 0.229479, acc: 92.97%] [G loss: 3.154457]\n",
            "******* 1094 [D loss: 0.344744, acc: 88.28%] [G loss: 2.447299]\n",
            "******* 1095 [D loss: 0.194375, acc: 92.19%] [G loss: 3.102206]\n",
            "******* 1096 [D loss: 0.158180, acc: 92.97%] [G loss: 4.202868]\n",
            "******* 1097 [D loss: 0.174626, acc: 94.53%] [G loss: 3.768579]\n",
            "******* 1098 [D loss: 0.188782, acc: 94.53%] [G loss: 2.999008]\n",
            "******* 1099 [D loss: 0.300397, acc: 89.06%] [G loss: 2.840927]\n",
            "******* 1100 [D loss: 0.101803, acc: 96.88%] [G loss: 3.912465]\n",
            "******* 1101 [D loss: 0.292600, acc: 89.06%] [G loss: 2.424615]\n",
            "******* 1102 [D loss: 0.211718, acc: 92.19%] [G loss: 2.947452]\n",
            "******* 1103 [D loss: 0.143430, acc: 96.09%] [G loss: 3.885487]\n",
            "******* 1104 [D loss: 0.190089, acc: 92.97%] [G loss: 3.274606]\n",
            "******* 1105 [D loss: 0.219142, acc: 92.97%] [G loss: 2.563837]\n",
            "******* 1106 [D loss: 0.154973, acc: 96.09%] [G loss: 3.168876]\n",
            "******* 1107 [D loss: 0.181691, acc: 93.75%] [G loss: 3.234886]\n",
            "******* 1108 [D loss: 0.166863, acc: 97.66%] [G loss: 3.151882]\n",
            "******* 1109 [D loss: 0.170701, acc: 95.31%] [G loss: 3.134687]\n",
            "******* 1110 [D loss: 0.160724, acc: 96.09%] [G loss: 2.717745]\n",
            "******* 1111 [D loss: 0.191137, acc: 96.09%] [G loss: 3.205963]\n",
            "******* 1112 [D loss: 0.113260, acc: 98.44%] [G loss: 3.887864]\n",
            "******* 1113 [D loss: 0.324664, acc: 89.84%] [G loss: 2.490272]\n",
            "******* 1114 [D loss: 0.442249, acc: 74.22%] [G loss: 2.833217]\n",
            "******* 1115 [D loss: 0.175029, acc: 95.31%] [G loss: 4.112003]\n",
            "******* 1116 [D loss: 0.210801, acc: 91.41%] [G loss: 3.544317]\n",
            "******* 1117 [D loss: 0.221204, acc: 94.53%] [G loss: 2.569005]\n",
            "******* 1118 [D loss: 0.255261, acc: 87.50%] [G loss: 3.074462]\n",
            "******* 1119 [D loss: 0.128267, acc: 95.31%] [G loss: 3.985254]\n",
            "******* 1120 [D loss: 0.241697, acc: 92.97%] [G loss: 3.359276]\n",
            "******* 1121 [D loss: 0.245955, acc: 89.06%] [G loss: 3.032969]\n",
            "******* 1122 [D loss: 0.244015, acc: 94.53%] [G loss: 3.306742]\n",
            "******* 1123 [D loss: 0.195974, acc: 93.75%] [G loss: 3.686034]\n",
            "******* 1124 [D loss: 0.235719, acc: 92.19%] [G loss: 3.113172]\n",
            "******* 1125 [D loss: 0.263227, acc: 91.41%] [G loss: 2.444833]\n",
            "******* 1126 [D loss: 0.308586, acc: 89.84%] [G loss: 3.003843]\n",
            "******* 1127 [D loss: 0.096082, acc: 98.44%] [G loss: 3.921835]\n",
            "******* 1128 [D loss: 0.167621, acc: 96.09%] [G loss: 4.344269]\n",
            "******* 1129 [D loss: 0.235461, acc: 91.41%] [G loss: 3.040551]\n",
            "******* 1130 [D loss: 0.244265, acc: 91.41%] [G loss: 2.566066]\n",
            "******* 1131 [D loss: 0.157443, acc: 95.31%] [G loss: 3.382576]\n",
            "******* 1132 [D loss: 0.203163, acc: 91.41%] [G loss: 3.431692]\n",
            "******* 1133 [D loss: 0.295619, acc: 87.50%] [G loss: 2.920337]\n",
            "******* 1134 [D loss: 0.211172, acc: 92.97%] [G loss: 3.190195]\n",
            "******* 1135 [D loss: 0.122016, acc: 98.44%] [G loss: 3.400404]\n",
            "******* 1136 [D loss: 0.266723, acc: 94.53%] [G loss: 3.074216]\n",
            "******* 1137 [D loss: 0.248842, acc: 94.53%] [G loss: 2.934779]\n",
            "******* 1138 [D loss: 0.281770, acc: 92.19%] [G loss: 2.924615]\n",
            "******* 1139 [D loss: 0.225797, acc: 90.62%] [G loss: 3.060965]\n",
            "******* 1140 [D loss: 0.293099, acc: 92.97%] [G loss: 3.487644]\n",
            "******* 1141 [D loss: 0.259796, acc: 92.19%] [G loss: 3.224492]\n",
            "******* 1142 [D loss: 0.240139, acc: 92.19%] [G loss: 3.079775]\n",
            "******* 1143 [D loss: 0.308196, acc: 89.84%] [G loss: 2.929250]\n",
            "******* 1144 [D loss: 0.193874, acc: 95.31%] [G loss: 3.271214]\n",
            "******* 1145 [D loss: 0.376448, acc: 85.94%] [G loss: 2.045913]\n",
            "******* 1146 [D loss: 0.278461, acc: 88.28%] [G loss: 2.641985]\n",
            "******* 1147 [D loss: 0.187034, acc: 94.53%] [G loss: 3.508715]\n",
            "******* 1148 [D loss: 0.325644, acc: 89.06%] [G loss: 3.052986]\n",
            "******* 1149 [D loss: 0.370143, acc: 86.72%] [G loss: 2.540769]\n",
            "******* 1150 [D loss: 0.204087, acc: 95.31%] [G loss: 3.235963]\n",
            "******* 1151 [D loss: 0.207257, acc: 92.97%] [G loss: 3.595441]\n",
            "******* 1152 [D loss: 0.329071, acc: 91.41%] [G loss: 2.905355]\n",
            "******* 1153 [D loss: 0.338851, acc: 87.50%] [G loss: 2.166473]\n",
            "******* 1154 [D loss: 0.179654, acc: 95.31%] [G loss: 3.443033]\n",
            "******* 1155 [D loss: 0.187344, acc: 94.53%] [G loss: 3.741373]\n",
            "******* 1156 [D loss: 0.211385, acc: 92.97%] [G loss: 3.153644]\n",
            "******* 1157 [D loss: 0.218294, acc: 92.97%] [G loss: 2.543452]\n",
            "******* 1158 [D loss: 0.227403, acc: 93.75%] [G loss: 3.038851]\n",
            "******* 1159 [D loss: 0.193690, acc: 96.09%] [G loss: 3.745389]\n",
            "******* 1160 [D loss: 0.165518, acc: 94.53%] [G loss: 3.302265]\n",
            "******* 1161 [D loss: 0.308136, acc: 89.84%] [G loss: 2.422841]\n",
            "******* 1162 [D loss: 0.311295, acc: 90.62%] [G loss: 3.057080]\n",
            "******* 1163 [D loss: 0.211122, acc: 93.75%] [G loss: 3.808241]\n",
            "******* 1164 [D loss: 0.377866, acc: 84.38%] [G loss: 2.486865]\n",
            "******* 1165 [D loss: 0.513134, acc: 75.78%] [G loss: 2.240569]\n",
            "******* 1166 [D loss: 0.251056, acc: 92.19%] [G loss: 3.703283]\n",
            "******* 1167 [D loss: 0.315178, acc: 89.84%] [G loss: 3.526079]\n",
            "******* 1168 [D loss: 0.292810, acc: 87.50%] [G loss: 2.469169]\n",
            "******* 1169 [D loss: 0.399903, acc: 82.81%] [G loss: 2.868794]\n",
            "******* 1170 [D loss: 0.342604, acc: 92.19%] [G loss: 3.610300]\n",
            "******* 1171 [D loss: 0.574937, acc: 82.81%] [G loss: 2.116962]\n",
            "******* 1172 [D loss: 0.406761, acc: 85.94%] [G loss: 2.670562]\n",
            "******* 1173 [D loss: 0.220135, acc: 91.41%] [G loss: 3.610232]\n",
            "******* 1174 [D loss: 0.399226, acc: 86.72%] [G loss: 2.966528]\n",
            "******* 1175 [D loss: 0.310303, acc: 89.06%] [G loss: 2.458972]\n",
            "******* 1176 [D loss: 0.323142, acc: 86.72%] [G loss: 2.478750]\n",
            "******* 1177 [D loss: 0.244613, acc: 92.19%] [G loss: 3.461186]\n",
            "******* 1178 [D loss: 0.352605, acc: 87.50%] [G loss: 2.975708]\n",
            "******* 1179 [D loss: 0.259889, acc: 89.84%] [G loss: 2.521097]\n",
            "******* 1180 [D loss: 0.291021, acc: 89.84%] [G loss: 2.457994]\n",
            "******* 1181 [D loss: 0.179767, acc: 94.53%] [G loss: 3.740970]\n",
            "******* 1182 [D loss: 0.268435, acc: 91.41%] [G loss: 3.014684]\n",
            "******* 1183 [D loss: 0.321521, acc: 86.72%] [G loss: 2.425814]\n",
            "******* 1184 [D loss: 0.337307, acc: 87.50%] [G loss: 2.727258]\n",
            "******* 1185 [D loss: 0.221134, acc: 91.41%] [G loss: 3.323225]\n",
            "******* 1186 [D loss: 0.340974, acc: 90.62%] [G loss: 2.893488]\n",
            "******* 1187 [D loss: 0.274063, acc: 89.06%] [G loss: 2.928255]\n",
            "******* 1188 [D loss: 0.200039, acc: 94.53%] [G loss: 3.193687]\n",
            "******* 1189 [D loss: 0.242802, acc: 90.62%] [G loss: 3.091936]\n",
            "******* 1190 [D loss: 0.184284, acc: 94.53%] [G loss: 3.314920]\n",
            "******* 1191 [D loss: 0.201575, acc: 93.75%] [G loss: 2.597685]\n",
            "******* 1192 [D loss: 0.224679, acc: 92.19%] [G loss: 2.650567]\n",
            "******* 1193 [D loss: 0.239716, acc: 91.41%] [G loss: 3.210773]\n",
            "******* 1194 [D loss: 0.288808, acc: 89.06%] [G loss: 2.684000]\n",
            "******* 1195 [D loss: 0.309124, acc: 89.84%] [G loss: 2.691047]\n",
            "******* 1196 [D loss: 0.255906, acc: 91.41%] [G loss: 3.086165]\n",
            "******* 1197 [D loss: 0.292276, acc: 92.97%] [G loss: 3.018744]\n",
            "******* 1198 [D loss: 0.241906, acc: 91.41%] [G loss: 3.117937]\n",
            "******* 1199 [D loss: 0.306959, acc: 87.50%] [G loss: 2.743053]\n",
            "******* 1200 [D loss: 0.408304, acc: 90.62%] [G loss: 2.154915]\n",
            "0.00000007\n",
            "saved\n",
            "******* 1201 [D loss: 0.335414, acc: 88.28%] [G loss: 2.401772]\n",
            "******* 1202 [D loss: 0.287295, acc: 91.41%] [G loss: 3.666660]\n",
            "******* 1203 [D loss: 0.292873, acc: 89.84%] [G loss: 3.145702]\n",
            "******* 1204 [D loss: 0.319155, acc: 89.84%] [G loss: 2.650832]\n",
            "******* 1205 [D loss: 0.209441, acc: 92.19%] [G loss: 3.265755]\n",
            "******* 1206 [D loss: 0.254420, acc: 93.75%] [G loss: 3.530769]\n",
            "******* 1207 [D loss: 0.283613, acc: 89.84%] [G loss: 2.671033]\n",
            "******* 1208 [D loss: 0.269911, acc: 91.41%] [G loss: 2.758371]\n",
            "******* 1209 [D loss: 0.261288, acc: 90.62%] [G loss: 2.810748]\n",
            "******* 1210 [D loss: 0.217386, acc: 94.53%] [G loss: 3.146320]\n",
            "******* 1211 [D loss: 0.225311, acc: 92.97%] [G loss: 3.384351]\n",
            "******* 1212 [D loss: 0.357900, acc: 92.19%] [G loss: 2.422848]\n",
            "******* 1213 [D loss: 0.345488, acc: 86.72%] [G loss: 2.971375]\n",
            "******* 1214 [D loss: 0.218241, acc: 90.62%] [G loss: 3.610211]\n",
            "******* 1215 [D loss: 0.323265, acc: 90.62%] [G loss: 2.854917]\n",
            "******* 1216 [D loss: 0.271365, acc: 90.62%] [G loss: 2.899987]\n",
            "******* 1217 [D loss: 0.303971, acc: 86.72%] [G loss: 2.818342]\n",
            "******* 1218 [D loss: 0.217748, acc: 95.31%] [G loss: 3.471959]\n",
            "******* 1219 [D loss: 0.304259, acc: 91.41%] [G loss: 2.915315]\n",
            "******* 1220 [D loss: 0.265418, acc: 89.06%] [G loss: 2.596129]\n",
            "******* 1221 [D loss: 0.267334, acc: 87.50%] [G loss: 3.193964]\n",
            "******* 1222 [D loss: 0.201672, acc: 94.53%] [G loss: 3.599150]\n",
            "******* 1223 [D loss: 0.263430, acc: 92.97%] [G loss: 2.597969]\n",
            "******* 1224 [D loss: 0.218176, acc: 94.53%] [G loss: 2.626398]\n",
            "******* 1225 [D loss: 0.200063, acc: 92.19%] [G loss: 3.424294]\n",
            "******* 1226 [D loss: 0.217293, acc: 88.28%] [G loss: 3.510184]\n",
            "******* 1227 [D loss: 0.488026, acc: 83.59%] [G loss: 2.332374]\n",
            "******* 1228 [D loss: 0.400454, acc: 84.38%] [G loss: 2.821138]\n",
            "******* 1229 [D loss: 0.163764, acc: 95.31%] [G loss: 3.803274]\n",
            "******* 1230 [D loss: 0.341817, acc: 87.50%] [G loss: 2.615233]\n",
            "******* 1231 [D loss: 0.369405, acc: 84.38%] [G loss: 2.764545]\n",
            "******* 1232 [D loss: 0.381114, acc: 87.50%] [G loss: 2.922322]\n",
            "******* 1233 [D loss: 0.235811, acc: 92.19%] [G loss: 3.038716]\n",
            "******* 1234 [D loss: 0.337103, acc: 89.84%] [G loss: 2.466275]\n",
            "******* 1235 [D loss: 0.457078, acc: 85.94%] [G loss: 2.257534]\n",
            "******* 1236 [D loss: 0.321494, acc: 91.41%] [G loss: 3.067827]\n",
            "******* 1237 [D loss: 0.189194, acc: 92.19%] [G loss: 3.952138]\n",
            "******* 1238 [D loss: 0.467760, acc: 83.59%] [G loss: 2.559779]\n",
            "******* 1239 [D loss: 0.456767, acc: 82.03%] [G loss: 2.215971]\n",
            "******* 1240 [D loss: 0.301735, acc: 87.50%] [G loss: 3.296458]\n",
            "******* 1241 [D loss: 0.410849, acc: 88.28%] [G loss: 3.373091]\n",
            "******* 1242 [D loss: 0.408395, acc: 87.50%] [G loss: 2.639509]\n",
            "******* 1243 [D loss: 0.298936, acc: 89.06%] [G loss: 2.829770]\n",
            "******* 1244 [D loss: 0.352706, acc: 84.38%] [G loss: 2.665043]\n",
            "******* 1245 [D loss: 0.458173, acc: 82.03%] [G loss: 2.451704]\n",
            "******* 1246 [D loss: 0.337673, acc: 89.84%] [G loss: 2.841185]\n",
            "******* 1247 [D loss: 0.372769, acc: 88.28%] [G loss: 2.737607]\n",
            "******* 1248 [D loss: 0.292463, acc: 90.62%] [G loss: 2.999478]\n",
            "******* 1249 [D loss: 0.270365, acc: 88.28%] [G loss: 2.609214]\n",
            "******* 1250 [D loss: 0.497204, acc: 82.81%] [G loss: 2.466986]\n",
            "******* 1251 [D loss: 0.221213, acc: 89.84%] [G loss: 3.490520]\n",
            "******* 1252 [D loss: 0.382902, acc: 89.06%] [G loss: 2.793056]\n",
            "******* 1253 [D loss: 0.533384, acc: 71.09%] [G loss: 2.240206]\n",
            "******* 1254 [D loss: 0.339478, acc: 88.28%] [G loss: 2.895165]\n",
            "******* 1255 [D loss: 0.301965, acc: 89.06%] [G loss: 3.296391]\n",
            "******* 1256 [D loss: 0.393652, acc: 87.50%] [G loss: 2.782912]\n",
            "******* 1257 [D loss: 0.416824, acc: 85.94%] [G loss: 2.737551]\n",
            "******* 1258 [D loss: 0.221664, acc: 94.53%] [G loss: 3.203265]\n",
            "******* 1259 [D loss: 0.243883, acc: 92.19%] [G loss: 3.198054]\n",
            "******* 1260 [D loss: 0.285332, acc: 92.19%] [G loss: 2.935579]\n",
            "******* 1261 [D loss: 0.305073, acc: 89.84%] [G loss: 2.613023]\n",
            "******* 1262 [D loss: 0.408409, acc: 82.03%] [G loss: 2.758942]\n",
            "******* 1263 [D loss: 0.212258, acc: 92.97%] [G loss: 3.455662]\n",
            "******* 1264 [D loss: 0.244093, acc: 89.84%] [G loss: 3.110898]\n",
            "******* 1265 [D loss: 0.333622, acc: 89.84%] [G loss: 2.579024]\n",
            "******* 1266 [D loss: 0.390527, acc: 80.47%] [G loss: 2.755033]\n",
            "******* 1267 [D loss: 0.235722, acc: 93.75%] [G loss: 3.449802]\n",
            "******* 1268 [D loss: 0.320655, acc: 89.84%] [G loss: 2.968579]\n",
            "******* 1269 [D loss: 0.414726, acc: 81.25%] [G loss: 2.519009]\n",
            "******* 1270 [D loss: 0.183696, acc: 94.53%] [G loss: 3.202839]\n",
            "******* 1271 [D loss: 0.317965, acc: 89.84%] [G loss: 3.347181]\n",
            "******* 1272 [D loss: 0.289248, acc: 92.19%] [G loss: 2.951706]\n",
            "******* 1273 [D loss: 0.204107, acc: 92.97%] [G loss: 2.909097]\n",
            "******* 1274 [D loss: 0.234070, acc: 95.31%] [G loss: 2.668518]\n",
            "******* 1275 [D loss: 0.296056, acc: 89.06%] [G loss: 2.742072]\n",
            "******* 1276 [D loss: 0.332135, acc: 88.28%] [G loss: 2.812721]\n",
            "******* 1277 [D loss: 0.358691, acc: 90.62%] [G loss: 2.812225]\n",
            "******* 1278 [D loss: 0.208405, acc: 94.53%] [G loss: 3.252352]\n",
            "******* 1279 [D loss: 0.421395, acc: 82.81%] [G loss: 2.550127]\n",
            "******* 1280 [D loss: 0.235578, acc: 93.75%] [G loss: 2.986554]\n",
            "******* 1281 [D loss: 0.278150, acc: 86.72%] [G loss: 2.715985]\n",
            "******* 1282 [D loss: 0.318028, acc: 88.28%] [G loss: 2.584456]\n",
            "******* 1283 [D loss: 0.137494, acc: 98.44%] [G loss: 3.621119]\n",
            "******* 1284 [D loss: 0.183450, acc: 93.75%] [G loss: 3.747927]\n",
            "******* 1285 [D loss: 0.375361, acc: 88.28%] [G loss: 2.372905]\n",
            "******* 1286 [D loss: 0.366797, acc: 85.94%] [G loss: 2.306284]\n",
            "******* 1287 [D loss: 0.142867, acc: 98.44%] [G loss: 3.603371]\n",
            "******* 1288 [D loss: 0.352775, acc: 89.84%] [G loss: 3.294699]\n",
            "******* 1289 [D loss: 0.196081, acc: 94.53%] [G loss: 2.633545]\n",
            "******* 1290 [D loss: 0.294751, acc: 89.06%] [G loss: 2.623769]\n",
            "******* 1291 [D loss: 0.254899, acc: 89.06%] [G loss: 3.613421]\n",
            "******* 1292 [D loss: 0.250281, acc: 89.06%] [G loss: 2.810008]\n",
            "******* 1293 [D loss: 0.399516, acc: 83.59%] [G loss: 2.180793]\n",
            "******* 1294 [D loss: 0.203011, acc: 95.31%] [G loss: 2.922001]\n",
            "******* 1295 [D loss: 0.234764, acc: 91.41%] [G loss: 3.480250]\n",
            "******* 1296 [D loss: 0.253837, acc: 93.75%] [G loss: 3.094308]\n",
            "******* 1297 [D loss: 0.292729, acc: 90.62%] [G loss: 2.484976]\n",
            "******* 1298 [D loss: 0.297295, acc: 88.28%] [G loss: 3.059995]\n",
            "******* 1299 [D loss: 0.149533, acc: 96.88%] [G loss: 3.611985]\n",
            "******* 1300 [D loss: 0.321531, acc: 86.72%] [G loss: 2.875844]\n",
            "******* 1301 [D loss: 0.320184, acc: 89.06%] [G loss: 2.712780]\n",
            "******* 1302 [D loss: 0.179208, acc: 96.09%] [G loss: 3.299727]\n",
            "******* 1303 [D loss: 0.172672, acc: 93.75%] [G loss: 3.526250]\n",
            "******* 1304 [D loss: 0.251944, acc: 91.41%] [G loss: 2.463789]\n",
            "******* 1305 [D loss: 0.344307, acc: 87.50%] [G loss: 2.311953]\n",
            "******* 1306 [D loss: 0.202251, acc: 96.09%] [G loss: 3.138488]\n",
            "******* 1307 [D loss: 0.290677, acc: 89.06%] [G loss: 2.801578]\n",
            "******* 1308 [D loss: 0.299937, acc: 85.94%] [G loss: 2.478271]\n",
            "******* 1309 [D loss: 0.324666, acc: 89.84%] [G loss: 2.624264]\n",
            "******* 1310 [D loss: 0.335329, acc: 89.84%] [G loss: 2.797889]\n",
            "******* 1311 [D loss: 0.215120, acc: 95.31%] [G loss: 2.757448]\n",
            "******* 1312 [D loss: 0.386205, acc: 89.06%] [G loss: 2.307011]\n",
            "******* 1313 [D loss: 0.280490, acc: 89.06%] [G loss: 2.341999]\n",
            "******* 1314 [D loss: 0.282149, acc: 89.84%] [G loss: 2.893086]\n",
            "******* 1315 [D loss: 0.362486, acc: 89.84%] [G loss: 3.110467]\n",
            "******* 1316 [D loss: 0.312003, acc: 89.06%] [G loss: 2.777437]\n",
            "******* 1317 [D loss: 0.360425, acc: 86.72%] [G loss: 2.626920]\n",
            "******* 1318 [D loss: 0.274584, acc: 91.41%] [G loss: 2.698196]\n",
            "******* 1319 [D loss: 0.221967, acc: 92.19%] [G loss: 3.127038]\n",
            "******* 1320 [D loss: 0.456342, acc: 81.25%] [G loss: 2.399927]\n",
            "******* 1321 [D loss: 0.402147, acc: 82.03%] [G loss: 2.191718]\n",
            "******* 1322 [D loss: 0.228422, acc: 90.62%] [G loss: 3.072322]\n",
            "******* 1323 [D loss: 0.231435, acc: 92.97%] [G loss: 3.841560]\n",
            "******* 1324 [D loss: 0.361672, acc: 89.84%] [G loss: 2.751515]\n",
            "******* 1325 [D loss: 0.313223, acc: 90.62%] [G loss: 2.082474]\n",
            "******* 1326 [D loss: 0.358311, acc: 85.16%] [G loss: 3.007175]\n",
            "******* 1327 [D loss: 0.313675, acc: 86.72%] [G loss: 3.133517]\n",
            "******* 1328 [D loss: 0.324095, acc: 89.84%] [G loss: 2.688221]\n",
            "******* 1329 [D loss: 0.245879, acc: 92.19%] [G loss: 2.923425]\n",
            "******* 1330 [D loss: 0.213012, acc: 94.53%] [G loss: 2.850143]\n",
            "******* 1331 [D loss: 0.482082, acc: 85.94%] [G loss: 2.347631]\n",
            "******* 1332 [D loss: 0.165451, acc: 98.44%] [G loss: 3.132700]\n",
            "******* 1333 [D loss: 0.414326, acc: 86.72%] [G loss: 2.640903]\n",
            "******* 1334 [D loss: 0.308096, acc: 89.84%] [G loss: 2.220634]\n",
            "******* 1335 [D loss: 0.457102, acc: 81.25%] [G loss: 2.403329]\n",
            "******* 1336 [D loss: 0.462726, acc: 82.03%] [G loss: 2.586554]\n",
            "******* 1337 [D loss: 0.430083, acc: 84.38%] [G loss: 2.528665]\n",
            "******* 1338 [D loss: 0.314229, acc: 91.41%] [G loss: 2.885662]\n",
            "******* 1339 [D loss: 0.310808, acc: 91.41%] [G loss: 2.668749]\n",
            "******* 1340 [D loss: 0.566189, acc: 76.56%] [G loss: 2.235015]\n",
            "******* 1341 [D loss: 0.382138, acc: 85.94%] [G loss: 3.016671]\n",
            "******* 1342 [D loss: 0.245843, acc: 92.97%] [G loss: 3.669060]\n",
            "******* 1343 [D loss: 0.416603, acc: 82.03%] [G loss: 2.780266]\n",
            "******* 1344 [D loss: 0.344242, acc: 85.94%] [G loss: 2.120105]\n",
            "******* 1345 [D loss: 0.424918, acc: 83.59%] [G loss: 2.701714]\n",
            "******* 1346 [D loss: 0.300452, acc: 89.06%] [G loss: 3.547768]\n",
            "******* 1347 [D loss: 0.429471, acc: 86.72%] [G loss: 2.972316]\n",
            "******* 1348 [D loss: 0.439891, acc: 76.56%] [G loss: 2.336981]\n",
            "******* 1349 [D loss: 0.392543, acc: 88.28%] [G loss: 3.021942]\n",
            "******* 1350 [D loss: 0.387443, acc: 89.84%] [G loss: 3.284103]\n",
            "******* 1351 [D loss: 0.334256, acc: 89.06%] [G loss: 2.421044]\n",
            "******* 1352 [D loss: 0.501462, acc: 73.44%] [G loss: 2.178990]\n",
            "******* 1353 [D loss: 0.294375, acc: 92.19%] [G loss: 3.147886]\n",
            "******* 1354 [D loss: 0.228472, acc: 93.75%] [G loss: 3.681667]\n",
            "******* 1355 [D loss: 0.376990, acc: 88.28%] [G loss: 2.449662]\n",
            "******* 1356 [D loss: 0.367219, acc: 82.03%] [G loss: 2.448641]\n",
            "******* 1357 [D loss: 0.444133, acc: 78.91%] [G loss: 2.507265]\n",
            "******* 1358 [D loss: 0.179515, acc: 94.53%] [G loss: 3.354617]\n",
            "******* 1359 [D loss: 0.543642, acc: 84.38%] [G loss: 2.526261]\n",
            "******* 1360 [D loss: 0.405495, acc: 87.50%] [G loss: 1.976679]\n",
            "******* 1361 [D loss: 0.490418, acc: 79.69%] [G loss: 2.504027]\n",
            "******* 1362 [D loss: 0.261764, acc: 89.06%] [G loss: 3.474607]\n",
            "******* 1363 [D loss: 0.276939, acc: 94.53%] [G loss: 3.672364]\n",
            "******* 1364 [D loss: 0.656743, acc: 78.91%] [G loss: 1.602468]\n",
            "******* 1365 [D loss: 0.638159, acc: 61.72%] [G loss: 1.884497]\n",
            "******* 1366 [D loss: 0.264503, acc: 92.19%] [G loss: 3.770705]\n",
            "******* 1367 [D loss: 0.234664, acc: 90.62%] [G loss: 4.303808]\n",
            "******* 1368 [D loss: 0.327275, acc: 89.06%] [G loss: 2.779198]\n",
            "******* 1369 [D loss: 0.417396, acc: 80.47%] [G loss: 1.860859]\n",
            "******* 1370 [D loss: 0.279609, acc: 90.62%] [G loss: 2.863477]\n",
            "******* 1371 [D loss: 0.200915, acc: 95.31%] [G loss: 4.397068]\n",
            "******* 1372 [D loss: 0.349048, acc: 85.94%] [G loss: 3.462861]\n",
            "******* 1373 [D loss: 0.409197, acc: 82.03%] [G loss: 1.917016]\n",
            "******* 1374 [D loss: 0.301499, acc: 85.94%] [G loss: 2.462682]\n",
            "******* 1375 [D loss: 0.224524, acc: 91.41%] [G loss: 3.872254]\n",
            "******* 1376 [D loss: 0.259764, acc: 88.28%] [G loss: 3.552063]\n",
            "******* 1377 [D loss: 0.288916, acc: 92.19%] [G loss: 2.583812]\n",
            "******* 1378 [D loss: 0.435289, acc: 84.38%] [G loss: 2.116154]\n",
            "******* 1379 [D loss: 0.225877, acc: 94.53%] [G loss: 3.323347]\n",
            "******* 1380 [D loss: 0.179768, acc: 93.75%] [G loss: 4.032441]\n",
            "******* 1381 [D loss: 0.221727, acc: 92.97%] [G loss: 3.587573]\n",
            "******* 1382 [D loss: 0.335643, acc: 92.97%] [G loss: 2.399684]\n",
            "******* 1383 [D loss: 0.364336, acc: 79.69%] [G loss: 2.186444]\n",
            "******* 1384 [D loss: 0.200736, acc: 95.31%] [G loss: 3.072804]\n",
            "******* 1385 [D loss: 0.345708, acc: 85.16%] [G loss: 3.046955]\n",
            "******* 1386 [D loss: 0.405061, acc: 83.59%] [G loss: 2.213162]\n",
            "******* 1387 [D loss: 0.400437, acc: 83.59%] [G loss: 2.341014]\n",
            "******* 1388 [D loss: 0.214656, acc: 91.41%] [G loss: 2.828446]\n",
            "******* 1389 [D loss: 0.377719, acc: 85.16%] [G loss: 2.440395]\n",
            "******* 1390 [D loss: 0.423063, acc: 82.81%] [G loss: 2.206423]\n",
            "******* 1391 [D loss: 0.330356, acc: 89.06%] [G loss: 2.596030]\n",
            "******* 1392 [D loss: 0.280578, acc: 89.84%] [G loss: 2.974047]\n",
            "******* 1393 [D loss: 0.380129, acc: 89.84%] [G loss: 2.493587]\n",
            "******* 1394 [D loss: 0.274415, acc: 90.62%] [G loss: 2.253520]\n",
            "******* 1395 [D loss: 0.307321, acc: 89.84%] [G loss: 2.520389]\n",
            "******* 1396 [D loss: 0.302341, acc: 88.28%] [G loss: 2.744386]\n",
            "******* 1397 [D loss: 0.279136, acc: 90.62%] [G loss: 2.744742]\n",
            "******* 1398 [D loss: 0.285272, acc: 89.84%] [G loss: 2.360613]\n",
            "******* 1399 [D loss: 0.432339, acc: 82.81%] [G loss: 2.024267]\n",
            "******* 1400 [D loss: 0.264819, acc: 92.97%] [G loss: 2.995337]\n",
            "0.00000008\n",
            "saved\n",
            "******* 1401 [D loss: 0.345272, acc: 90.62%] [G loss: 3.386709]\n",
            "******* 1402 [D loss: 0.402493, acc: 83.59%] [G loss: 2.484207]\n",
            "******* 1403 [D loss: 0.308397, acc: 85.16%] [G loss: 2.338009]\n",
            "******* 1404 [D loss: 0.386676, acc: 82.81%] [G loss: 2.561538]\n",
            "******* 1405 [D loss: 0.170823, acc: 95.31%] [G loss: 3.422910]\n",
            "******* 1406 [D loss: 0.341402, acc: 88.28%] [G loss: 2.760890]\n",
            "******* 1407 [D loss: 0.432930, acc: 78.91%] [G loss: 2.083134]\n",
            "******* 1408 [D loss: 0.313972, acc: 85.94%] [G loss: 2.816363]\n",
            "******* 1409 [D loss: 0.266755, acc: 91.41%] [G loss: 3.509715]\n",
            "******* 1410 [D loss: 0.433580, acc: 86.72%] [G loss: 2.333825]\n",
            "******* 1411 [D loss: 0.388241, acc: 84.38%] [G loss: 1.954222]\n",
            "******* 1412 [D loss: 0.362521, acc: 86.72%] [G loss: 2.796287]\n",
            "******* 1413 [D loss: 0.365460, acc: 86.72%] [G loss: 2.910429]\n",
            "******* 1414 [D loss: 0.462824, acc: 88.28%] [G loss: 2.256500]\n",
            "******* 1415 [D loss: 0.376246, acc: 87.50%] [G loss: 2.254989]\n",
            "******* 1416 [D loss: 0.320408, acc: 88.28%] [G loss: 2.817822]\n",
            "******* 1417 [D loss: 0.256859, acc: 92.97%] [G loss: 3.166408]\n",
            "******* 1418 [D loss: 0.483648, acc: 79.69%] [G loss: 2.162611]\n",
            "******* 1419 [D loss: 0.527835, acc: 73.44%] [G loss: 2.145052]\n",
            "******* 1420 [D loss: 0.374470, acc: 82.03%] [G loss: 3.181377]\n",
            "******* 1421 [D loss: 0.382374, acc: 86.72%] [G loss: 2.777524]\n",
            "******* 1422 [D loss: 0.551372, acc: 78.12%] [G loss: 1.835389]\n",
            "******* 1423 [D loss: 0.450017, acc: 75.78%] [G loss: 2.734538]\n",
            "******* 1424 [D loss: 0.185812, acc: 94.53%] [G loss: 4.097813]\n",
            "******* 1425 [D loss: 0.389678, acc: 84.38%] [G loss: 3.188888]\n",
            "******* 1426 [D loss: 0.431239, acc: 82.81%] [G loss: 1.693841]\n",
            "******* 1427 [D loss: 0.406944, acc: 79.69%] [G loss: 1.921148]\n",
            "******* 1428 [D loss: 0.166101, acc: 96.88%] [G loss: 3.733463]\n",
            "******* 1429 [D loss: 0.321101, acc: 90.62%] [G loss: 3.611952]\n",
            "******* 1430 [D loss: 0.666050, acc: 75.00%] [G loss: 1.800243]\n",
            "******* 1431 [D loss: 0.423011, acc: 78.91%] [G loss: 1.976052]\n",
            "******* 1432 [D loss: 0.263932, acc: 92.19%] [G loss: 3.595779]\n",
            "******* 1433 [D loss: 0.227876, acc: 92.97%] [G loss: 3.737552]\n",
            "******* 1434 [D loss: 0.215713, acc: 91.41%] [G loss: 2.797492]\n",
            "******* 1435 [D loss: 0.332625, acc: 88.28%] [G loss: 1.868851]\n",
            "******* 1436 [D loss: 0.264727, acc: 88.28%] [G loss: 2.417637]\n",
            "******* 1437 [D loss: 0.257777, acc: 90.62%] [G loss: 3.567937]\n",
            "******* 1438 [D loss: 0.319095, acc: 86.72%] [G loss: 2.853266]\n",
            "******* 1439 [D loss: 0.399612, acc: 85.16%] [G loss: 2.069900]\n",
            "******* 1440 [D loss: 0.318068, acc: 87.50%] [G loss: 2.330882]\n",
            "******* 1441 [D loss: 0.237199, acc: 92.97%] [G loss: 3.353970]\n",
            "******* 1442 [D loss: 0.255879, acc: 90.62%] [G loss: 3.073058]\n",
            "******* 1443 [D loss: 0.354872, acc: 89.06%] [G loss: 2.586982]\n",
            "******* 1444 [D loss: 0.334324, acc: 86.72%] [G loss: 2.611598]\n",
            "******* 1445 [D loss: 0.275820, acc: 88.28%] [G loss: 2.509285]\n",
            "******* 1446 [D loss: 0.244823, acc: 93.75%] [G loss: 2.740017]\n",
            "******* 1447 [D loss: 0.383765, acc: 84.38%] [G loss: 2.422579]\n",
            "******* 1448 [D loss: 0.394400, acc: 84.38%] [G loss: 2.265766]\n",
            "******* 1449 [D loss: 0.371601, acc: 86.72%] [G loss: 2.505612]\n",
            "******* 1450 [D loss: 0.184372, acc: 92.97%] [G loss: 3.176793]\n",
            "******* 1451 [D loss: 0.230716, acc: 92.97%] [G loss: 2.994710]\n",
            "******* 1452 [D loss: 0.310853, acc: 89.84%] [G loss: 2.603004]\n",
            "******* 1453 [D loss: 0.300915, acc: 87.50%] [G loss: 2.425402]\n",
            "******* 1454 [D loss: 0.250591, acc: 92.19%] [G loss: 3.211580]\n",
            "******* 1455 [D loss: 0.460194, acc: 85.16%] [G loss: 2.260386]\n",
            "******* 1456 [D loss: 0.335394, acc: 86.72%] [G loss: 2.388918]\n",
            "******* 1457 [D loss: 0.225285, acc: 92.19%] [G loss: 2.979100]\n",
            "******* 1458 [D loss: 0.265402, acc: 93.75%] [G loss: 2.880861]\n",
            "******* 1459 [D loss: 0.544453, acc: 85.94%] [G loss: 2.292322]\n",
            "******* 1460 [D loss: 0.363839, acc: 85.94%] [G loss: 2.443133]\n",
            "******* 1461 [D loss: 0.250440, acc: 90.62%] [G loss: 2.942150]\n",
            "******* 1462 [D loss: 0.300254, acc: 92.19%] [G loss: 2.884622]\n",
            "******* 1463 [D loss: 0.343184, acc: 87.50%] [G loss: 2.355439]\n",
            "******* 1464 [D loss: 0.351385, acc: 87.50%] [G loss: 2.481095]\n",
            "******* 1465 [D loss: 0.253896, acc: 95.31%] [G loss: 2.988540]\n",
            "******* 1466 [D loss: 0.388369, acc: 87.50%] [G loss: 2.807185]\n",
            "******* 1467 [D loss: 0.227439, acc: 92.97%] [G loss: 2.677435]\n",
            "******* 1468 [D loss: 0.312976, acc: 89.06%] [G loss: 2.517812]\n",
            "******* 1469 [D loss: 0.187546, acc: 96.09%] [G loss: 2.887069]\n",
            "******* 1470 [D loss: 0.312542, acc: 88.28%] [G loss: 2.518208]\n",
            "******* 1471 [D loss: 0.209627, acc: 92.97%] [G loss: 2.713537]\n",
            "******* 1472 [D loss: 0.385076, acc: 88.28%] [G loss: 2.506768]\n",
            "******* 1473 [D loss: 0.317242, acc: 87.50%] [G loss: 2.603156]\n",
            "******* 1474 [D loss: 0.260983, acc: 93.75%] [G loss: 3.197755]\n",
            "******* 1475 [D loss: 0.330573, acc: 89.06%] [G loss: 2.876866]\n",
            "******* 1476 [D loss: 0.393736, acc: 85.16%] [G loss: 2.293973]\n",
            "******* 1477 [D loss: 0.405384, acc: 78.12%] [G loss: 2.398220]\n",
            "******* 1478 [D loss: 0.255238, acc: 90.62%] [G loss: 2.717239]\n",
            "******* 1479 [D loss: 0.351166, acc: 90.62%] [G loss: 2.813128]\n",
            "******* 1480 [D loss: 0.346727, acc: 85.94%] [G loss: 2.664095]\n",
            "******* 1481 [D loss: 0.321071, acc: 89.06%] [G loss: 2.534124]\n",
            "******* 1482 [D loss: 0.332430, acc: 85.94%] [G loss: 2.496013]\n",
            "******* 1483 [D loss: 0.335289, acc: 89.06%] [G loss: 2.753685]\n",
            "******* 1484 [D loss: 0.387252, acc: 83.59%] [G loss: 2.253249]\n",
            "******* 1485 [D loss: 0.330224, acc: 89.06%] [G loss: 2.759712]\n",
            "******* 1486 [D loss: 0.361559, acc: 86.72%] [G loss: 2.206551]\n",
            "******* 1487 [D loss: 0.535325, acc: 79.69%] [G loss: 2.054862]\n",
            "******* 1488 [D loss: 0.574341, acc: 72.66%] [G loss: 2.080200]\n",
            "******* 1489 [D loss: 0.270919, acc: 90.62%] [G loss: 2.959373]\n",
            "******* 1490 [D loss: 0.495976, acc: 86.72%] [G loss: 2.893408]\n",
            "******* 1491 [D loss: 0.556653, acc: 79.69%] [G loss: 2.020177]\n",
            "******* 1492 [D loss: 0.265713, acc: 92.97%] [G loss: 2.470659]\n",
            "******* 1493 [D loss: 0.320767, acc: 88.28%] [G loss: 2.685186]\n",
            "******* 1494 [D loss: 0.331317, acc: 85.94%] [G loss: 2.646855]\n",
            "******* 1495 [D loss: 0.462344, acc: 79.69%] [G loss: 2.147972]\n",
            "******* 1496 [D loss: 0.594841, acc: 71.09%] [G loss: 2.367602]\n",
            "******* 1497 [D loss: 0.338186, acc: 89.84%] [G loss: 2.899129]\n",
            "******* 1498 [D loss: 0.399679, acc: 83.59%] [G loss: 2.540748]\n",
            "******* 1499 [D loss: 0.477195, acc: 80.47%] [G loss: 2.163064]\n",
            "******* 1500 [D loss: 0.297882, acc: 90.62%] [G loss: 2.645916]\n",
            "******* 1501 [D loss: 0.428152, acc: 79.69%] [G loss: 2.311150]\n",
            "******* 1502 [D loss: 0.343863, acc: 85.16%] [G loss: 2.467166]\n",
            "******* 1503 [D loss: 0.439866, acc: 86.72%] [G loss: 2.436683]\n",
            "******* 1504 [D loss: 0.403863, acc: 83.59%] [G loss: 2.331497]\n",
            "******* 1505 [D loss: 0.450112, acc: 81.25%] [G loss: 1.950999]\n",
            "******* 1506 [D loss: 0.303032, acc: 89.06%] [G loss: 2.821747]\n",
            "******* 1507 [D loss: 0.466405, acc: 85.16%] [G loss: 2.374249]\n",
            "******* 1508 [D loss: 0.356549, acc: 85.94%] [G loss: 2.052939]\n",
            "******* 1509 [D loss: 0.416156, acc: 82.03%] [G loss: 1.877098]\n",
            "******* 1510 [D loss: 0.337132, acc: 89.84%] [G loss: 2.326297]\n",
            "******* 1511 [D loss: 0.463886, acc: 81.25%] [G loss: 2.358298]\n",
            "******* 1512 [D loss: 0.568005, acc: 73.44%] [G loss: 2.110171]\n",
            "******* 1513 [D loss: 0.306001, acc: 87.50%] [G loss: 2.908522]\n",
            "******* 1514 [D loss: 0.603231, acc: 79.69%] [G loss: 2.221131]\n",
            "******* 1515 [D loss: 0.553759, acc: 69.53%] [G loss: 1.807112]\n",
            "******* 1516 [D loss: 0.390031, acc: 83.59%] [G loss: 2.499743]\n",
            "******* 1517 [D loss: 0.294785, acc: 91.41%] [G loss: 2.903693]\n",
            "******* 1518 [D loss: 0.510449, acc: 82.81%] [G loss: 2.225578]\n",
            "******* 1519 [D loss: 0.472026, acc: 78.12%] [G loss: 2.047447]\n",
            "******* 1520 [D loss: 0.290635, acc: 89.84%] [G loss: 2.536940]\n",
            "******* 1521 [D loss: 0.322349, acc: 92.19%] [G loss: 3.096893]\n",
            "******* 1522 [D loss: 0.501443, acc: 83.59%] [G loss: 2.377341]\n",
            "******* 1523 [D loss: 0.616501, acc: 73.44%] [G loss: 1.770299]\n",
            "******* 1524 [D loss: 0.342221, acc: 85.94%] [G loss: 2.666854]\n",
            "******* 1525 [D loss: 0.299987, acc: 89.84%] [G loss: 3.194872]\n",
            "******* 1526 [D loss: 0.455034, acc: 84.38%] [G loss: 2.324766]\n",
            "******* 1527 [D loss: 0.578852, acc: 76.56%] [G loss: 1.598217]\n",
            "******* 1528 [D loss: 0.356137, acc: 85.16%] [G loss: 2.221481]\n",
            "******* 1529 [D loss: 0.206516, acc: 92.97%] [G loss: 3.682619]\n",
            "******* 1530 [D loss: 0.444779, acc: 84.38%] [G loss: 2.614290]\n",
            "******* 1531 [D loss: 0.634418, acc: 71.88%] [G loss: 1.634709]\n",
            "******* 1532 [D loss: 0.431176, acc: 77.34%] [G loss: 2.243652]\n",
            "******* 1533 [D loss: 0.331970, acc: 86.72%] [G loss: 3.583143]\n",
            "******* 1534 [D loss: 0.540992, acc: 80.47%] [G loss: 2.669959]\n",
            "******* 1535 [D loss: 0.569969, acc: 76.56%] [G loss: 1.589109]\n",
            "******* 1536 [D loss: 0.507942, acc: 70.31%] [G loss: 1.839249]\n",
            "******* 1537 [D loss: 0.198857, acc: 95.31%] [G loss: 3.327381]\n",
            "******* 1538 [D loss: 0.304553, acc: 89.06%] [G loss: 3.133666]\n",
            "******* 1539 [D loss: 0.403996, acc: 85.94%] [G loss: 1.974096]\n",
            "******* 1540 [D loss: 0.408293, acc: 82.81%] [G loss: 1.722607]\n",
            "******* 1541 [D loss: 0.374969, acc: 87.50%] [G loss: 2.686807]\n",
            "******* 1542 [D loss: 0.286935, acc: 89.06%] [G loss: 3.159997]\n",
            "******* 1543 [D loss: 0.364333, acc: 85.16%] [G loss: 2.320643]\n",
            "******* 1544 [D loss: 0.522233, acc: 78.12%] [G loss: 1.743191]\n",
            "******* 1545 [D loss: 0.495870, acc: 78.12%] [G loss: 2.280003]\n",
            "******* 1546 [D loss: 0.250713, acc: 92.97%] [G loss: 3.109768]\n",
            "******* 1547 [D loss: 0.482459, acc: 83.59%] [G loss: 2.134662]\n",
            "******* 1548 [D loss: 0.367511, acc: 83.59%] [G loss: 1.755516]\n",
            "******* 1549 [D loss: 0.378918, acc: 79.69%] [G loss: 2.616637]\n",
            "******* 1550 [D loss: 0.247904, acc: 91.41%] [G loss: 3.195873]\n",
            "******* 1551 [D loss: 0.419473, acc: 83.59%] [G loss: 2.287193]\n",
            "******* 1552 [D loss: 0.551796, acc: 75.00%] [G loss: 1.629635]\n",
            "******* 1553 [D loss: 0.389196, acc: 81.25%] [G loss: 2.256505]\n",
            "******* 1554 [D loss: 0.260385, acc: 92.19%] [G loss: 3.370466]\n",
            "******* 1555 [D loss: 0.410307, acc: 83.59%] [G loss: 2.448885]\n",
            "******* 1556 [D loss: 0.464309, acc: 79.69%] [G loss: 1.808270]\n",
            "******* 1557 [D loss: 0.351522, acc: 85.94%] [G loss: 1.991138]\n",
            "******* 1558 [D loss: 0.325103, acc: 89.06%] [G loss: 2.689875]\n",
            "******* 1559 [D loss: 0.311762, acc: 88.28%] [G loss: 3.047680]\n",
            "******* 1560 [D loss: 0.451175, acc: 82.81%] [G loss: 2.225066]\n",
            "******* 1561 [D loss: 0.460163, acc: 80.47%] [G loss: 1.675384]\n",
            "******* 1562 [D loss: 0.438983, acc: 83.59%] [G loss: 2.024055]\n",
            "******* 1563 [D loss: 0.347217, acc: 84.38%] [G loss: 2.523367]\n",
            "******* 1564 [D loss: 0.325653, acc: 89.84%] [G loss: 3.087641]\n",
            "******* 1565 [D loss: 0.478827, acc: 82.03%] [G loss: 2.331113]\n",
            "******* 1566 [D loss: 0.462763, acc: 82.81%] [G loss: 1.601087]\n",
            "******* 1567 [D loss: 0.422079, acc: 78.91%] [G loss: 2.113054]\n",
            "******* 1568 [D loss: 0.343741, acc: 88.28%] [G loss: 2.859353]\n",
            "******* 1569 [D loss: 0.433982, acc: 83.59%] [G loss: 2.064298]\n",
            "******* 1570 [D loss: 0.495794, acc: 82.81%] [G loss: 1.575957]\n",
            "******* 1571 [D loss: 0.489895, acc: 79.69%] [G loss: 2.111515]\n",
            "******* 1572 [D loss: 0.488730, acc: 82.81%] [G loss: 2.625612]\n",
            "******* 1573 [D loss: 0.471951, acc: 80.47%] [G loss: 2.345551]\n",
            "******* 1574 [D loss: 0.735445, acc: 68.75%] [G loss: 1.625917]\n",
            "******* 1575 [D loss: 0.397152, acc: 85.94%] [G loss: 2.299877]\n",
            "******* 1576 [D loss: 0.352620, acc: 89.06%] [G loss: 2.815180]\n",
            "******* 1577 [D loss: 0.271241, acc: 92.19%] [G loss: 2.618287]\n",
            "******* 1578 [D loss: 0.386671, acc: 83.59%] [G loss: 1.965178]\n",
            "******* 1579 [D loss: 0.500569, acc: 79.69%] [G loss: 2.052833]\n",
            "******* 1580 [D loss: 0.520609, acc: 77.34%] [G loss: 2.525577]\n",
            "******* 1581 [D loss: 0.534387, acc: 79.69%] [G loss: 2.301028]\n",
            "******* 1582 [D loss: 0.537405, acc: 74.22%] [G loss: 1.842842]\n",
            "******* 1583 [D loss: 0.491554, acc: 76.56%] [G loss: 2.203016]\n",
            "******* 1584 [D loss: 0.640957, acc: 76.56%] [G loss: 2.822133]\n",
            "******* 1585 [D loss: 0.538399, acc: 79.69%] [G loss: 2.459942]\n",
            "******* 1586 [D loss: 0.368063, acc: 84.38%] [G loss: 2.149901]\n",
            "******* 1587 [D loss: 0.496335, acc: 70.31%] [G loss: 1.970654]\n",
            "******* 1588 [D loss: 0.552619, acc: 78.12%] [G loss: 2.267059]\n",
            "******* 1589 [D loss: 0.422086, acc: 83.59%] [G loss: 2.092985]\n",
            "******* 1590 [D loss: 0.597064, acc: 74.22%] [G loss: 1.865241]\n",
            "******* 1591 [D loss: 0.535887, acc: 70.31%] [G loss: 2.132979]\n",
            "******* 1592 [D loss: 0.424311, acc: 82.03%] [G loss: 2.239724]\n",
            "******* 1593 [D loss: 0.498623, acc: 82.81%] [G loss: 1.923944]\n",
            "******* 1594 [D loss: 0.537958, acc: 77.34%] [G loss: 1.902169]\n",
            "******* 1595 [D loss: 0.486291, acc: 75.00%] [G loss: 1.980477]\n",
            "******* 1596 [D loss: 0.484090, acc: 82.03%] [G loss: 2.044881]\n",
            "******* 1597 [D loss: 0.356647, acc: 83.59%] [G loss: 2.485435]\n",
            "******* 1598 [D loss: 0.480054, acc: 78.91%] [G loss: 2.220590]\n",
            "******* 1599 [D loss: 0.476730, acc: 79.69%] [G loss: 1.743053]\n",
            "******* 1600 [D loss: 0.310599, acc: 89.06%] [G loss: 2.403402]\n",
            "0.00000009\n",
            "saved\n",
            "******* 1601 [D loss: 0.463037, acc: 82.03%] [G loss: 2.354989]\n",
            "******* 1602 [D loss: 0.477441, acc: 83.59%] [G loss: 1.763294]\n",
            "******* 1603 [D loss: 0.481242, acc: 79.69%] [G loss: 1.843641]\n",
            "******* 1604 [D loss: 0.433323, acc: 85.16%] [G loss: 2.597203]\n",
            "******* 1605 [D loss: 0.401319, acc: 85.94%] [G loss: 2.299728]\n",
            "******* 1606 [D loss: 0.315552, acc: 90.62%] [G loss: 2.362587]\n",
            "******* 1607 [D loss: 0.405605, acc: 85.16%] [G loss: 2.546326]\n",
            "******* 1608 [D loss: 0.344567, acc: 88.28%] [G loss: 2.578632]\n",
            "******* 1609 [D loss: 0.365021, acc: 85.16%] [G loss: 2.453266]\n",
            "******* 1610 [D loss: 0.364198, acc: 85.94%] [G loss: 2.352752]\n",
            "******* 1611 [D loss: 0.658956, acc: 75.00%] [G loss: 2.264266]\n",
            "******* 1612 [D loss: 0.403331, acc: 85.94%] [G loss: 2.614727]\n",
            "******* 1613 [D loss: 0.365595, acc: 86.72%] [G loss: 3.145054]\n",
            "******* 1614 [D loss: 0.420383, acc: 87.50%] [G loss: 2.694028]\n",
            "******* 1615 [D loss: 0.530552, acc: 74.22%] [G loss: 1.608345]\n",
            "******* 1616 [D loss: 0.452676, acc: 78.12%] [G loss: 2.043578]\n",
            "******* 1617 [D loss: 0.393984, acc: 85.16%] [G loss: 2.586386]\n",
            "******* 1618 [D loss: 0.259676, acc: 89.84%] [G loss: 2.836107]\n",
            "******* 1619 [D loss: 0.432430, acc: 82.81%] [G loss: 2.194412]\n",
            "******* 1620 [D loss: 0.455532, acc: 77.34%] [G loss: 2.055726]\n",
            "******* 1621 [D loss: 0.342578, acc: 82.81%] [G loss: 2.497482]\n",
            "******* 1622 [D loss: 0.204406, acc: 93.75%] [G loss: 3.231054]\n",
            "******* 1623 [D loss: 0.373994, acc: 88.28%] [G loss: 2.728149]\n",
            "******* 1624 [D loss: 0.464885, acc: 83.59%] [G loss: 1.894136]\n",
            "******* 1625 [D loss: 0.364999, acc: 83.59%] [G loss: 2.041235]\n",
            "******* 1626 [D loss: 0.351677, acc: 85.16%] [G loss: 2.577892]\n",
            "******* 1627 [D loss: 0.241240, acc: 91.41%] [G loss: 2.746408]\n",
            "******* 1628 [D loss: 0.343819, acc: 89.06%] [G loss: 2.481897]\n",
            "******* 1629 [D loss: 0.297124, acc: 91.41%] [G loss: 2.259742]\n",
            "******* 1630 [D loss: 0.383241, acc: 86.72%] [G loss: 2.306611]\n",
            "******* 1631 [D loss: 0.200379, acc: 92.19%] [G loss: 2.907761]\n",
            "******* 1632 [D loss: 0.418591, acc: 85.16%] [G loss: 2.329232]\n",
            "******* 1633 [D loss: 0.360381, acc: 87.50%] [G loss: 2.140229]\n",
            "******* 1634 [D loss: 0.304274, acc: 86.72%] [G loss: 2.261522]\n",
            "******* 1635 [D loss: 0.386162, acc: 86.72%] [G loss: 2.977561]\n",
            "******* 1636 [D loss: 0.348863, acc: 88.28%] [G loss: 3.176796]\n",
            "******* 1637 [D loss: 0.380059, acc: 85.16%] [G loss: 2.236955]\n",
            "******* 1638 [D loss: 0.472147, acc: 78.91%] [G loss: 1.965742]\n",
            "******* 1639 [D loss: 0.426466, acc: 79.69%] [G loss: 2.200581]\n",
            "******* 1640 [D loss: 0.459962, acc: 82.81%] [G loss: 2.325460]\n",
            "******* 1641 [D loss: 0.447191, acc: 83.59%] [G loss: 2.566423]\n",
            "******* 1642 [D loss: 0.259958, acc: 92.19%] [G loss: 2.733266]\n",
            "******* 1643 [D loss: 0.380293, acc: 85.94%] [G loss: 2.292893]\n",
            "******* 1644 [D loss: 0.426805, acc: 85.94%] [G loss: 2.160348]\n",
            "******* 1645 [D loss: 0.393265, acc: 87.50%] [G loss: 2.459177]\n",
            "******* 1646 [D loss: 0.304042, acc: 89.84%] [G loss: 3.160644]\n",
            "******* 1647 [D loss: 0.403831, acc: 85.94%] [G loss: 2.493648]\n",
            "******* 1648 [D loss: 0.292920, acc: 89.84%] [G loss: 1.925170]\n",
            "******* 1649 [D loss: 0.407369, acc: 82.81%] [G loss: 2.010018]\n",
            "******* 1650 [D loss: 0.366551, acc: 85.16%] [G loss: 2.991910]\n",
            "******* 1651 [D loss: 0.280763, acc: 89.84%] [G loss: 3.107756]\n",
            "******* 1652 [D loss: 0.390696, acc: 85.94%] [G loss: 2.499568]\n",
            "******* 1653 [D loss: 0.395725, acc: 85.16%] [G loss: 1.907742]\n",
            "******* 1654 [D loss: 0.339598, acc: 87.50%] [G loss: 2.531117]\n",
            "******* 1655 [D loss: 0.291498, acc: 89.06%] [G loss: 3.193692]\n",
            "******* 1656 [D loss: 0.435263, acc: 85.16%] [G loss: 2.388849]\n",
            "******* 1657 [D loss: 0.484298, acc: 78.12%] [G loss: 1.702164]\n",
            "******* 1658 [D loss: 0.517362, acc: 74.22%] [G loss: 2.133999]\n",
            "******* 1659 [D loss: 0.305682, acc: 89.06%] [G loss: 3.409066]\n",
            "******* 1660 [D loss: 0.360387, acc: 84.38%] [G loss: 3.190723]\n",
            "******* 1661 [D loss: 0.490285, acc: 81.25%] [G loss: 1.812923]\n",
            "******* 1662 [D loss: 0.440253, acc: 78.91%] [G loss: 1.714678]\n",
            "******* 1663 [D loss: 0.359689, acc: 86.72%] [G loss: 2.374097]\n",
            "******* 1664 [D loss: 0.198913, acc: 93.75%] [G loss: 3.394835]\n",
            "******* 1665 [D loss: 0.330691, acc: 89.06%] [G loss: 2.633948]\n",
            "******* 1666 [D loss: 0.516927, acc: 75.78%] [G loss: 1.622484]\n",
            "******* 1667 [D loss: 0.348805, acc: 86.72%] [G loss: 2.329861]\n",
            "******* 1668 [D loss: 0.338221, acc: 89.06%] [G loss: 2.865930]\n",
            "******* 1669 [D loss: 0.234070, acc: 93.75%] [G loss: 2.925078]\n",
            "******* 1670 [D loss: 0.410901, acc: 83.59%] [G loss: 2.242254]\n",
            "******* 1671 [D loss: 0.440552, acc: 83.59%] [G loss: 1.908127]\n",
            "******* 1672 [D loss: 0.411538, acc: 84.38%] [G loss: 2.418695]\n",
            "******* 1673 [D loss: 0.324377, acc: 92.19%] [G loss: 2.843403]\n",
            "******* 1674 [D loss: 0.407551, acc: 86.72%] [G loss: 2.200139]\n",
            "******* 1675 [D loss: 0.408841, acc: 85.16%] [G loss: 2.148960]\n",
            "******* 1676 [D loss: 0.379080, acc: 86.72%] [G loss: 2.567050]\n",
            "******* 1677 [D loss: 0.455793, acc: 85.94%] [G loss: 2.659908]\n",
            "******* 1678 [D loss: 0.285730, acc: 88.28%] [G loss: 2.470014]\n",
            "******* 1679 [D loss: 0.423780, acc: 83.59%] [G loss: 2.134708]\n",
            "******* 1680 [D loss: 0.368224, acc: 86.72%] [G loss: 1.880311]\n",
            "******* 1681 [D loss: 0.356116, acc: 86.72%] [G loss: 2.285030]\n",
            "******* 1682 [D loss: 0.383430, acc: 84.38%] [G loss: 2.338609]\n",
            "******* 1683 [D loss: 0.414942, acc: 84.38%] [G loss: 2.438249]\n",
            "******* 1684 [D loss: 0.425798, acc: 84.38%] [G loss: 2.182326]\n",
            "******* 1685 [D loss: 0.453907, acc: 75.00%] [G loss: 2.000607]\n",
            "******* 1686 [D loss: 0.434444, acc: 83.59%] [G loss: 2.084118]\n",
            "******* 1687 [D loss: 0.381356, acc: 83.59%] [G loss: 2.437582]\n",
            "******* 1688 [D loss: 0.303540, acc: 89.06%] [G loss: 2.618808]\n",
            "******* 1689 [D loss: 0.368737, acc: 82.81%] [G loss: 2.267244]\n",
            "******* 1690 [D loss: 0.426019, acc: 82.81%] [G loss: 2.048028]\n",
            "******* 1691 [D loss: 0.446002, acc: 84.38%] [G loss: 2.091889]\n",
            "******* 1692 [D loss: 0.480466, acc: 81.25%] [G loss: 2.179038]\n",
            "******* 1693 [D loss: 0.599021, acc: 77.34%] [G loss: 1.980902]\n",
            "******* 1694 [D loss: 0.494043, acc: 79.69%] [G loss: 2.166103]\n",
            "******* 1695 [D loss: 0.377241, acc: 85.94%] [G loss: 2.591823]\n",
            "******* 1696 [D loss: 0.690134, acc: 71.09%] [G loss: 2.016016]\n",
            "******* 1697 [D loss: 0.530734, acc: 74.22%] [G loss: 1.774297]\n",
            "******* 1698 [D loss: 0.552506, acc: 76.56%] [G loss: 2.448407]\n",
            "******* 1699 [D loss: 0.487900, acc: 81.25%] [G loss: 2.598399]\n",
            "******* 1700 [D loss: 0.583632, acc: 77.34%] [G loss: 2.183156]\n",
            "******* 1701 [D loss: 0.542466, acc: 77.34%] [G loss: 1.839157]\n",
            "******* 1702 [D loss: 0.407223, acc: 85.94%] [G loss: 2.346599]\n",
            "******* 1703 [D loss: 0.402945, acc: 85.94%] [G loss: 2.699433]\n",
            "******* 1704 [D loss: 0.333861, acc: 85.16%] [G loss: 2.662214]\n",
            "******* 1705 [D loss: 0.635322, acc: 77.34%] [G loss: 1.670640]\n",
            "******* 1706 [D loss: 0.526691, acc: 71.09%] [G loss: 1.712621]\n",
            "******* 1707 [D loss: 0.334067, acc: 85.94%] [G loss: 2.599866]\n",
            "******* 1708 [D loss: 0.337189, acc: 88.28%] [G loss: 3.024888]\n",
            "******* 1709 [D loss: 0.587742, acc: 77.34%] [G loss: 2.194946]\n",
            "******* 1710 [D loss: 0.475832, acc: 81.25%] [G loss: 1.849199]\n",
            "******* 1711 [D loss: 0.485407, acc: 78.91%] [G loss: 2.139022]\n",
            "******* 1712 [D loss: 0.435838, acc: 78.12%] [G loss: 2.357951]\n",
            "******* 1713 [D loss: 0.400696, acc: 81.25%] [G loss: 2.421738]\n",
            "******* 1714 [D loss: 0.479609, acc: 80.47%] [G loss: 2.197264]\n",
            "******* 1715 [D loss: 0.487097, acc: 75.78%] [G loss: 2.075716]\n",
            "******* 1716 [D loss: 0.320775, acc: 91.41%] [G loss: 2.606285]\n",
            "******* 1717 [D loss: 0.501794, acc: 79.69%] [G loss: 2.310285]\n",
            "******* 1718 [D loss: 0.506786, acc: 78.91%] [G loss: 1.871285]\n",
            "******* 1719 [D loss: 0.470653, acc: 74.22%] [G loss: 2.279697]\n",
            "******* 1720 [D loss: 0.390526, acc: 89.06%] [G loss: 2.621381]\n",
            "******* 1721 [D loss: 0.378917, acc: 88.28%] [G loss: 2.283775]\n",
            "******* 1722 [D loss: 0.514873, acc: 78.91%] [G loss: 2.074308]\n",
            "******* 1723 [D loss: 0.357402, acc: 84.38%] [G loss: 2.303336]\n",
            "******* 1724 [D loss: 0.449803, acc: 84.38%] [G loss: 2.590163]\n",
            "******* 1725 [D loss: 0.473279, acc: 80.47%] [G loss: 2.205349]\n",
            "******* 1726 [D loss: 0.344154, acc: 87.50%] [G loss: 2.090421]\n",
            "******* 1727 [D loss: 0.415797, acc: 84.38%] [G loss: 1.990522]\n",
            "******* 1728 [D loss: 0.348552, acc: 87.50%] [G loss: 2.456187]\n",
            "******* 1729 [D loss: 0.311886, acc: 92.97%] [G loss: 2.798892]\n",
            "******* 1730 [D loss: 0.483330, acc: 83.59%] [G loss: 1.948526]\n",
            "******* 1731 [D loss: 0.438299, acc: 78.12%] [G loss: 1.529700]\n",
            "******* 1732 [D loss: 0.574973, acc: 75.00%] [G loss: 1.808132]\n",
            "******* 1733 [D loss: 0.284334, acc: 91.41%] [G loss: 2.793072]\n",
            "******* 1734 [D loss: 0.325274, acc: 89.84%] [G loss: 2.775269]\n",
            "******* 1735 [D loss: 0.485252, acc: 79.69%] [G loss: 1.913118]\n",
            "******* 1736 [D loss: 0.688295, acc: 69.53%] [G loss: 1.269064]\n",
            "******* 1737 [D loss: 0.477323, acc: 74.22%] [G loss: 2.067137]\n",
            "******* 1738 [D loss: 0.442239, acc: 82.03%] [G loss: 2.696542]\n",
            "******* 1739 [D loss: 0.457034, acc: 85.94%] [G loss: 2.568563]\n",
            "******* 1740 [D loss: 0.471648, acc: 82.81%] [G loss: 1.876825]\n",
            "******* 1741 [D loss: 0.459744, acc: 78.12%] [G loss: 1.952128]\n",
            "******* 1742 [D loss: 0.348690, acc: 86.72%] [G loss: 2.484608]\n",
            "******* 1743 [D loss: 0.374128, acc: 89.84%] [G loss: 2.788413]\n",
            "******* 1744 [D loss: 0.625651, acc: 74.22%] [G loss: 1.937602]\n",
            "******* 1745 [D loss: 0.437755, acc: 80.47%] [G loss: 1.684937]\n",
            "******* 1746 [D loss: 0.353998, acc: 82.81%] [G loss: 2.304030]\n",
            "******* 1747 [D loss: 0.315124, acc: 90.62%] [G loss: 2.581941]\n",
            "******* 1748 [D loss: 0.280717, acc: 92.97%] [G loss: 2.662561]\n",
            "******* 1749 [D loss: 0.425811, acc: 85.16%] [G loss: 2.164974]\n",
            "******* 1750 [D loss: 0.410782, acc: 82.03%] [G loss: 2.259158]\n",
            "******* 1751 [D loss: 0.263378, acc: 93.75%] [G loss: 2.395234]\n",
            "******* 1752 [D loss: 0.470846, acc: 78.91%] [G loss: 2.140382]\n",
            "******* 1753 [D loss: 0.281042, acc: 91.41%] [G loss: 2.432449]\n",
            "******* 1754 [D loss: 0.372297, acc: 82.81%] [G loss: 2.433220]\n",
            "******* 1755 [D loss: 0.387635, acc: 83.59%] [G loss: 2.224833]\n",
            "******* 1756 [D loss: 0.319849, acc: 86.72%] [G loss: 2.684999]\n",
            "******* 1757 [D loss: 0.319026, acc: 87.50%] [G loss: 2.794963]\n",
            "******* 1758 [D loss: 0.425363, acc: 86.72%] [G loss: 2.484147]\n",
            "******* 1759 [D loss: 0.477342, acc: 77.34%] [G loss: 2.045406]\n",
            "******* 1760 [D loss: 0.488241, acc: 77.34%] [G loss: 2.044300]\n",
            "******* 1761 [D loss: 0.455146, acc: 79.69%] [G loss: 2.294995]\n",
            "******* 1762 [D loss: 0.568592, acc: 76.56%] [G loss: 2.130639]\n",
            "******* 1763 [D loss: 0.346220, acc: 88.28%] [G loss: 2.685638]\n",
            "******* 1764 [D loss: 0.345698, acc: 86.72%] [G loss: 2.745328]\n",
            "******* 1765 [D loss: 0.447126, acc: 87.50%] [G loss: 2.212380]\n",
            "******* 1766 [D loss: 0.505838, acc: 78.91%] [G loss: 1.857283]\n",
            "******* 1767 [D loss: 0.335046, acc: 85.16%] [G loss: 2.294741]\n",
            "******* 1768 [D loss: 0.257179, acc: 91.41%] [G loss: 2.929526]\n",
            "******* 1769 [D loss: 0.424137, acc: 82.81%] [G loss: 2.413492]\n",
            "******* 1770 [D loss: 0.754706, acc: 67.19%] [G loss: 1.628965]\n",
            "******* 1771 [D loss: 0.333723, acc: 88.28%] [G loss: 1.983517]\n",
            "******* 1772 [D loss: 0.357348, acc: 89.84%] [G loss: 2.583605]\n",
            "******* 1773 [D loss: 0.400426, acc: 83.59%] [G loss: 2.704500]\n",
            "******* 1774 [D loss: 0.552528, acc: 78.91%] [G loss: 2.158427]\n",
            "******* 1775 [D loss: 0.608775, acc: 71.88%] [G loss: 1.719105]\n",
            "******* 1776 [D loss: 0.438905, acc: 75.00%] [G loss: 1.941394]\n",
            "******* 1777 [D loss: 0.367584, acc: 85.16%] [G loss: 2.370941]\n",
            "******* 1778 [D loss: 0.440723, acc: 82.03%] [G loss: 2.388413]\n",
            "******* 1779 [D loss: 0.365893, acc: 82.81%] [G loss: 2.350372]\n",
            "******* 1780 [D loss: 0.487353, acc: 82.81%] [G loss: 1.989186]\n",
            "******* 1781 [D loss: 0.459812, acc: 82.03%] [G loss: 1.743431]\n",
            "******* 1782 [D loss: 0.407397, acc: 83.59%] [G loss: 1.810092]\n",
            "******* 1783 [D loss: 0.417897, acc: 84.38%] [G loss: 2.389327]\n",
            "******* 1784 [D loss: 0.347084, acc: 84.38%] [G loss: 2.558117]\n",
            "******* 1785 [D loss: 0.381674, acc: 88.28%] [G loss: 1.936128]\n",
            "******* 1786 [D loss: 0.415666, acc: 83.59%] [G loss: 1.546449]\n",
            "******* 1787 [D loss: 0.377225, acc: 84.38%] [G loss: 1.885384]\n",
            "******* 1788 [D loss: 0.413480, acc: 88.28%] [G loss: 2.700387]\n",
            "******* 1789 [D loss: 0.468169, acc: 78.12%] [G loss: 2.323142]\n",
            "******* 1790 [D loss: 0.569649, acc: 76.56%] [G loss: 1.728572]\n",
            "******* 1791 [D loss: 0.365163, acc: 85.94%] [G loss: 2.218022]\n",
            "******* 1792 [D loss: 0.277296, acc: 91.41%] [G loss: 2.683216]\n",
            "******* 1793 [D loss: 0.405357, acc: 89.84%] [G loss: 2.371624]\n",
            "******* 1794 [D loss: 0.316575, acc: 88.28%] [G loss: 2.291739]\n",
            "******* 1795 [D loss: 0.421475, acc: 82.03%] [G loss: 1.962109]\n",
            "******* 1796 [D loss: 0.378550, acc: 83.59%] [G loss: 2.091100]\n",
            "******* 1797 [D loss: 0.397470, acc: 83.59%] [G loss: 2.392446]\n",
            "******* 1798 [D loss: 0.405495, acc: 81.25%] [G loss: 2.221376]\n",
            "******* 1799 [D loss: 0.536599, acc: 76.56%] [G loss: 1.969763]\n",
            "******* 1800 [D loss: 0.405928, acc: 85.16%] [G loss: 2.186078]\n",
            "0.00000010\n",
            "saved\n",
            "******* 1801 [D loss: 0.321439, acc: 87.50%] [G loss: 2.639332]\n",
            "******* 1802 [D loss: 0.584438, acc: 76.56%] [G loss: 2.054533]\n",
            "******* 1803 [D loss: 0.379526, acc: 86.72%] [G loss: 1.888386]\n",
            "******* 1804 [D loss: 0.279805, acc: 92.19%] [G loss: 2.222722]\n",
            "******* 1805 [D loss: 0.284145, acc: 87.50%] [G loss: 2.357846]\n",
            "******* 1806 [D loss: 0.251294, acc: 91.41%] [G loss: 2.751654]\n",
            "******* 1807 [D loss: 0.405576, acc: 85.16%] [G loss: 2.240860]\n",
            "******* 1808 [D loss: 0.250544, acc: 90.62%] [G loss: 2.215827]\n",
            "******* 1809 [D loss: 0.388308, acc: 90.62%] [G loss: 2.035265]\n",
            "******* 1810 [D loss: 0.325837, acc: 92.19%] [G loss: 2.450747]\n",
            "******* 1811 [D loss: 0.287023, acc: 88.28%] [G loss: 2.443260]\n",
            "******* 1812 [D loss: 0.320096, acc: 85.94%] [G loss: 2.344869]\n",
            "******* 1813 [D loss: 0.374608, acc: 84.38%] [G loss: 2.364690]\n",
            "******* 1814 [D loss: 0.283076, acc: 91.41%] [G loss: 2.606649]\n",
            "******* 1815 [D loss: 0.263125, acc: 92.97%] [G loss: 2.758503]\n",
            "******* 1816 [D loss: 0.391309, acc: 84.38%] [G loss: 2.376177]\n",
            "******* 1817 [D loss: 0.524602, acc: 80.47%] [G loss: 1.883744]\n",
            "******* 1818 [D loss: 0.288145, acc: 89.84%] [G loss: 2.284534]\n",
            "******* 1819 [D loss: 0.521987, acc: 82.03%] [G loss: 2.341011]\n",
            "******* 1820 [D loss: 0.284633, acc: 90.62%] [G loss: 2.443948]\n",
            "******* 1821 [D loss: 0.375502, acc: 85.16%] [G loss: 2.466001]\n",
            "******* 1822 [D loss: 0.279139, acc: 89.84%] [G loss: 2.294639]\n",
            "******* 1823 [D loss: 0.361823, acc: 83.59%] [G loss: 2.073153]\n",
            "******* 1824 [D loss: 0.303603, acc: 85.94%] [G loss: 2.593343]\n",
            "******* 1825 [D loss: 0.412118, acc: 84.38%] [G loss: 2.688674]\n",
            "******* 1826 [D loss: 0.451506, acc: 85.16%] [G loss: 2.406988]\n",
            "******* 1827 [D loss: 0.341646, acc: 85.94%] [G loss: 1.785290]\n",
            "******* 1828 [D loss: 0.423129, acc: 79.69%] [G loss: 2.307345]\n",
            "******* 1829 [D loss: 0.315390, acc: 89.84%] [G loss: 2.397192]\n",
            "******* 1830 [D loss: 0.401046, acc: 82.03%] [G loss: 1.978295]\n",
            "******* 1831 [D loss: 0.379247, acc: 87.50%] [G loss: 1.997312]\n",
            "******* 1832 [D loss: 0.421203, acc: 79.69%] [G loss: 2.130065]\n",
            "******* 1833 [D loss: 0.320405, acc: 90.62%] [G loss: 2.256525]\n",
            "******* 1834 [D loss: 0.281599, acc: 88.28%] [G loss: 2.430214]\n",
            "******* 1835 [D loss: 0.476118, acc: 81.25%] [G loss: 1.975016]\n",
            "******* 1836 [D loss: 0.405603, acc: 85.94%] [G loss: 2.292567]\n",
            "******* 1837 [D loss: 0.322825, acc: 92.19%] [G loss: 2.851896]\n",
            "******* 1838 [D loss: 0.391094, acc: 84.38%] [G loss: 2.370279]\n",
            "******* 1839 [D loss: 0.374218, acc: 82.81%] [G loss: 1.983614]\n",
            "******* 1840 [D loss: 0.514922, acc: 76.56%] [G loss: 2.065801]\n",
            "******* 1841 [D loss: 0.373349, acc: 82.03%] [G loss: 2.890847]\n",
            "******* 1842 [D loss: 0.424028, acc: 84.38%] [G loss: 2.788686]\n",
            "******* 1843 [D loss: 0.546696, acc: 80.47%] [G loss: 1.881786]\n",
            "******* 1844 [D loss: 0.526373, acc: 70.31%] [G loss: 1.764818]\n",
            "******* 1845 [D loss: 0.371626, acc: 85.94%] [G loss: 2.196485]\n",
            "******* 1846 [D loss: 0.597777, acc: 78.12%] [G loss: 2.092145]\n",
            "******* 1847 [D loss: 0.453844, acc: 85.16%] [G loss: 2.283678]\n",
            "******* 1848 [D loss: 0.466491, acc: 85.94%] [G loss: 2.168834]\n",
            "******* 1849 [D loss: 0.319969, acc: 87.50%] [G loss: 2.260695]\n",
            "******* 1850 [D loss: 0.468739, acc: 78.12%] [G loss: 1.848204]\n",
            "******* 1851 [D loss: 0.523927, acc: 71.88%] [G loss: 1.984505]\n",
            "******* 1852 [D loss: 0.363074, acc: 87.50%] [G loss: 2.496350]\n",
            "******* 1853 [D loss: 0.376353, acc: 88.28%] [G loss: 2.295346]\n",
            "******* 1854 [D loss: 0.242250, acc: 91.41%] [G loss: 2.239551]\n",
            "******* 1855 [D loss: 0.409816, acc: 83.59%] [G loss: 1.862817]\n",
            "******* 1856 [D loss: 0.446633, acc: 78.91%] [G loss: 1.808502]\n",
            "******* 1857 [D loss: 0.388648, acc: 85.16%] [G loss: 2.073645]\n",
            "******* 1858 [D loss: 0.318496, acc: 85.16%] [G loss: 2.299844]\n",
            "******* 1859 [D loss: 0.437194, acc: 85.94%] [G loss: 2.284982]\n",
            "******* 1860 [D loss: 0.326620, acc: 88.28%] [G loss: 2.359999]\n",
            "******* 1861 [D loss: 0.420104, acc: 79.69%] [G loss: 2.416561]\n",
            "******* 1862 [D loss: 0.348007, acc: 87.50%] [G loss: 2.199353]\n",
            "******* 1863 [D loss: 0.419750, acc: 84.38%] [G loss: 2.022755]\n",
            "******* 1864 [D loss: 0.403855, acc: 81.25%] [G loss: 2.217058]\n",
            "******* 1865 [D loss: 0.406399, acc: 86.72%] [G loss: 2.104708]\n",
            "******* 1866 [D loss: 0.286866, acc: 91.41%] [G loss: 2.370039]\n",
            "******* 1867 [D loss: 0.357165, acc: 88.28%] [G loss: 2.476820]\n",
            "******* 1868 [D loss: 0.625500, acc: 73.44%] [G loss: 1.984877]\n",
            "******* 1869 [D loss: 0.513049, acc: 75.00%] [G loss: 1.988631]\n",
            "******* 1870 [D loss: 0.431655, acc: 79.69%] [G loss: 2.399348]\n",
            "******* 1871 [D loss: 0.474461, acc: 81.25%] [G loss: 2.267705]\n",
            "******* 1872 [D loss: 0.594331, acc: 68.75%] [G loss: 1.706808]\n",
            "******* 1873 [D loss: 0.340483, acc: 86.72%] [G loss: 2.488286]\n",
            "******* 1874 [D loss: 0.443125, acc: 82.03%] [G loss: 2.853973]\n",
            "******* 1875 [D loss: 0.405787, acc: 81.25%] [G loss: 2.222757]\n",
            "******* 1876 [D loss: 0.409465, acc: 83.59%] [G loss: 1.913116]\n",
            "******* 1877 [D loss: 0.469453, acc: 79.69%] [G loss: 1.895855]\n",
            "******* 1878 [D loss: 0.471058, acc: 80.47%] [G loss: 2.431506]\n",
            "******* 1879 [D loss: 0.575881, acc: 79.69%] [G loss: 2.015924]\n",
            "******* 1880 [D loss: 0.447296, acc: 81.25%] [G loss: 1.942466]\n",
            "******* 1881 [D loss: 0.320728, acc: 88.28%] [G loss: 2.203532]\n",
            "******* 1882 [D loss: 0.397877, acc: 85.94%] [G loss: 2.213991]\n",
            "******* 1883 [D loss: 0.415096, acc: 82.03%] [G loss: 2.046188]\n",
            "******* 1884 [D loss: 0.354711, acc: 86.72%] [G loss: 2.148125]\n",
            "******* 1885 [D loss: 0.415060, acc: 83.59%] [G loss: 2.184607]\n",
            "******* 1886 [D loss: 0.432102, acc: 82.81%] [G loss: 2.263762]\n",
            "******* 1887 [D loss: 0.406329, acc: 82.81%] [G loss: 2.030004]\n",
            "******* 1888 [D loss: 0.534287, acc: 77.34%] [G loss: 1.881930]\n",
            "******* 1889 [D loss: 0.367999, acc: 85.16%] [G loss: 2.304723]\n",
            "******* 1890 [D loss: 0.395670, acc: 89.06%] [G loss: 2.582036]\n",
            "******* 1891 [D loss: 0.385580, acc: 84.38%] [G loss: 2.149759]\n",
            "******* 1892 [D loss: 0.304471, acc: 85.94%] [G loss: 1.964923]\n",
            "******* 1893 [D loss: 0.488209, acc: 78.12%] [G loss: 2.034399]\n",
            "******* 1894 [D loss: 0.281444, acc: 92.19%] [G loss: 2.562394]\n",
            "******* 1895 [D loss: 0.417478, acc: 80.47%] [G loss: 2.321378]\n",
            "******* 1896 [D loss: 0.454637, acc: 79.69%] [G loss: 2.118559]\n",
            "******* 1897 [D loss: 0.383271, acc: 85.94%] [G loss: 2.213585]\n",
            "******* 1898 [D loss: 0.353687, acc: 85.94%] [G loss: 2.245607]\n",
            "******* 1899 [D loss: 0.434509, acc: 82.03%] [G loss: 1.887199]\n",
            "******* 1900 [D loss: 0.496770, acc: 76.56%] [G loss: 2.151919]\n",
            "******* 1901 [D loss: 0.265003, acc: 92.19%] [G loss: 2.648302]\n",
            "******* 1902 [D loss: 0.345249, acc: 86.72%] [G loss: 2.605038]\n",
            "******* 1903 [D loss: 0.471377, acc: 82.03%] [G loss: 1.835201]\n",
            "******* 1904 [D loss: 0.385663, acc: 85.94%] [G loss: 1.872618]\n",
            "******* 1905 [D loss: 0.371640, acc: 86.72%] [G loss: 2.146485]\n",
            "******* 1906 [D loss: 0.422587, acc: 85.16%] [G loss: 2.399627]\n",
            "******* 1907 [D loss: 0.356327, acc: 85.94%] [G loss: 2.442330]\n",
            "******* 1908 [D loss: 0.293513, acc: 89.84%] [G loss: 2.605297]\n",
            "******* 1909 [D loss: 0.401807, acc: 84.38%] [G loss: 2.315371]\n",
            "******* 1910 [D loss: 0.277131, acc: 88.28%] [G loss: 2.088266]\n",
            "******* 1911 [D loss: 0.364339, acc: 82.81%] [G loss: 2.063631]\n",
            "******* 1912 [D loss: 0.361071, acc: 83.59%] [G loss: 2.213036]\n",
            "******* 1913 [D loss: 0.473151, acc: 84.38%] [G loss: 2.086794]\n",
            "******* 1914 [D loss: 0.331423, acc: 88.28%] [G loss: 2.207959]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-08179edae6ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-08179edae6ce>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs, batch_size, save_interval)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m#Train discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0md_loss_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0md_loss_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfakes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0md_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_loss_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_loss_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[1;32m   1898\u001b[0m                                                     class_weight)\n\u001b[1;32m   1899\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1900\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1902\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    940\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3130\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3131\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3133\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1959\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1960\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1962\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    601\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    604\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 59\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "po-jSQoN1Azl"
      },
      "source": [
        "### **8) Making GIF**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPShgQpg1EMy"
      },
      "source": [
        "anim_file = 'dcgan.gif'\n",
        "\n",
        "with imageio.get_writer(anim_file, mode='I') as writer:\n",
        "  filenames = glob.glob('generated_images/*.png')\n",
        "  filenames = sorted(filenames)\n",
        "  for filename in filenames:\n",
        "    image = imageio.imread(filename)\n",
        "    writer.append_data(image)\n",
        "  image = imageio.imread(filename)\n",
        "  writer.append_data(image)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wh37uv1torG5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}